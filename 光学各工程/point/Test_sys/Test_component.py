from email import header
import shutil 
import imp
# from msilib import MSIMODIFY_VALIDATE
import os
import re
# from tkinter import N
from unicodedata import decimal
from collections import deque
import cv2
from cv2 import normalize
import yaml
import copy
from kornia import tensor_to_image
import torch
import torch.nn as nn
#from e2cnn import nn as enn
import logging
import importlib
import numpy as np
import pandas as pd
from pathlib import Path
from imageio import imread
from PIL import Image, ImageDraw
from skimage.measure import ransac
from skimage.transform import AffineTransform
import torchvision.transforms as transforms
from torchvision import utils as vutils
from scipy.linalg import hadamard
# from zmq import device
from models.hardnet_model import *
import csv
import random

from get_trace import get_main 

from models.modules import sample_descriptor

from Model_component import draw_match_pair_hanming, getPtsFromHeatmapByNMS, inv_warp_image, compute_valid_mask, warp_points
from Model_component import filter_points, draw_keypoints_pair, draw_keypoints_pair_tradition, draw_match_pair_degree_match
from Model_component import get_point_pair_repeat, get_point_pair_repeat_double, getPtsFromHeatmap, flattenDetection, getPtsFromHeatmapByCoordinates
from other_tools import WHT
from models.hardnet_model import HardNet_fast_twice_half3_MO_MOE_short, HardNet_fast_twice_short, HardNet_fast_twice_big_short, HardNet_fast_half_short, HardNet_fast_twice_half3_MO_short
from models.ALNet import *

def get_module(name):  
    mod = importlib.import_module('Test_component')
    print(name)
    return getattr(mod, name)

def load_as_float(path):
    return imread(path).astype(np.float32) / 255

'''真实重复率测试（DOG数据）'''
class Test_Real_Repeat_DOG(object):
    def __init__(self, img_path, info_path, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        df = pd.read_csv(info_path, header=0, encoding="gb2312")
        image1_list = df['enroll_val'].to_list()
        image1_list = [str(i).rjust(3, '0') for i in image1_list]    # 获得名称并补齐 '1'-> '001'
        image2_list = df['path'].to_list()
        image2_list = [i[7:] for i in image2_list]

        for i in range(len(image2_list)):
            suffix = '.csv'
            path_share = '/'.join(image2_list[i].split('/')[:-1])
            image1_list[i] = path_share + '/' + image1_list[i] + suffix

        H0 = df['trans1'].to_list()
        H1 = df['trans2'].to_list()
        H2 = df['trans3'].to_list()
        H3 = df['trans4'].to_list()
        H4 = df['trans5'].to_list()
        H5 = df['trans6'].to_list()
        trans = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]    # combined into list

        image1_name = ['_'.join(k.split('/')[-3:])[:-4] for k in image1_list]
        image2_name = ['_'.join(k.split('/')[-3:])[:-4] for k in image2_list]

        # ------ images points ------
        points1_list = []
        points2_list = []
        for i in range(len(image1_list)):
            f = image1_list[i].replace('DOG_01', 'DOG_01_Pts')
            f = f.replace('csv', 'npy')
            points1_list.append(f)

            f = image2_list[i].replace('DOG_01', 'DOG_01_Pts')
            f = f.replace('csv', 'npy')
            points2_list.append(f)

        # ------ dic ------
        files1 = {
            'imgA': image2_list,    
            'imgB': image1_list,       # 模板
            'imgA_name': image2_name,
            'imgB_name': image1_name,
            'trans': trans
        }
        files2 = {
            'pointsA': points2_list,
            'pointsB': points1_list
        }   # 传统坐标数据

        sequence_set = []
        for (A, N_A, B, N_B, M_H, P_A, P_B) in zip(
            files1['imgA'], files1['imgA_name'],
            files1['imgB'], files1['imgB_name'],files1['trans'],
            files2['pointsA'], files2['pointsB']):   # zip返回元组

            sample = {
                'imgA': A, 
                'imgA_name': N_A, 
                'imgB': B, 
                'imgB_name': N_B,
                'm_all': M_H,
                'pointA': P_A,
                'pointB': P_B
                }   # 无标签时，只有img数据
            sequence_set.append(sample)

        self.samples = sequence_set
        # self.iterations = len(self.samples)c
        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        sample = self.samples[index]
        # sample2 = self.samples2[index]  # img & traditional points
        imgA_path = sample['imgA']
        imgB_path = sample['imgB']
        homo = np.array(sample['m_all'])
        homo = torch.tensor(homo.reshape(2, 3), dtype=torch.float32)
        homo = 2 * homo / 512.
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo, vec_one), 0)

        if 1:
            # img_o_A = load_as_float(sample1['imgA'])
            # img_o_B = load_as_float(sample1['imgB'])
            img_o_A = pd.read_csv(imgA_path, header=None).to_numpy()
            img_o_B = pd.read_csv(imgB_path, header=None).to_numpy()
            img_o_A = img_o_A.astype(np.float32)
            img_o_B = img_o_B.astype(np.float32)
            img_o_A = (img_o_A + 3500) / 7000.
            img_o_B = (img_o_B + 3500) / 7000.
            img_o_A[:3, :], img_o_B[:3, :] = 0., 0.
            img_o_A[:, :3], img_o_B[:, :3] = 0., 0.
            img_o_A[(img_o_A.shape[0] - 3):, :], img_o_B[(img_o_B.shape[0] - 3):, :] = 0., 0.
            img_o_A[:, (img_o_A.shape[1] - 3):], img_o_B[:, (img_o_B.shape[1] - 3):] = 0., 0.
            # img_o_A, img_o_B = img_o_A / 255., img_o_B / 255.
        else:
            img_o_A = np.load(sample['imgA'])
            img_o_B = np.load(sample['imgB'])
        # img_o = (img_o + 32768) / 65535.
            img_o_A = (img_o_A + 3500) / 7000.
            img_o_B = (img_o_B + 3500) / 7000.
            img_o_A[:3, :], img_o_B[:3, :] = 0., 0.
            img_o_A[:, :3], img_o_B[:, :3] = 0., 0.
            img_o_A[(img_o_A.shape[0] - 3):, :], img_o_B[(img_o_B.shape[0] - 3):, :] = 0., 0.
            img_o_A[:, (img_o_A.shape[1] - 3):], img_o_B[:, (img_o_B.shape[1] - 3):] = 0., 0.

        img_A_ori = self.transforms(img_o_A)
        img_B_ori = self.transforms(img_o_B)

        '''传统增强扩边处理-逆：36->32'''
        img_A = img_A_ori[0, :, 2:-2].unsqueeze(0)   # [1, 160, 32]
        img_B = img_B_ori[0, :, 2:-2].unsqueeze(0)

        H, W = img_A.shape[1], img_A.shape[2]

        img_A = torch.tensor(img_A, dtype=torch.float32).view(-1, H, W)
        img_B = torch.tensor(img_B, dtype=torch.float32).view(-1, H, W)
        imgA_warped = inv_warp_image(img_A.squeeze(), homo, mode='bilinear').unsqueeze(0)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        # idxA = self.files2['img_6159'].index(imgA_path) # 获取同手指图在点坐标列表中的index
        # idxB = self.files2['img_6159'].index(imgB_path)
        # ptsA = np.array(self.files2['pts'][idxA], dtype=np.float32).reshape(-1, 2)    #(x, y)
        # ptsB = np.array(self.files2['pts'][idxB], dtype=np.float32).reshape(-1, 2)

        ptsA = np.load(sample['pointA'])[:, [1,0]]
        ptsB = np.load(sample['pointB'])[:, [1,0]]
        ptsA = torch.tensor(ptsA).type(torch.FloatTensor)
        ptsB = torch.tensor(ptsB).type(torch.FloatTensor)

        input  = {}
        input.update({
            'imgA': img_A,  
            'imgB': img_B,  # 模板
            'imgA_warped': imgA_warped, # 非模板旋转
            'imgA_warped_mask': imgA_warped_mask,
            'imgA_name': sample['imgA_name'],
            'imgB_name': sample['imgB_name'],
            'H_AB': homo,
            'ptsA': ptsA,
            'ptsB': ptsB
            })

        return input

    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        tra_repeat_out = 0.
        content = []

        for idx in range(len(self.samples)):

            sample = self.get_data(idx)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['H_AB']
            # imgA, imgB= (
            #     imgA.to(self.device),
            #     imgB.to(self.device),
            # )
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['imgA_name']
            nameB = sample['imgB_name']
            nameAB = str(nameA) + '_' + str(nameB)
            logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # pass through network
            heatmap = FPDT.run_heatmap(imgA.to(self.device)).detach().cpu()
            heatmap_B = FPDT.run_heatmap(imgB.to(self.device)).detach().cpu()

            # heatmap_B = heatmap_B * imgA_warped_mask    # A-->B 后进行mask，只显示匹配区域
            pts_A = getPtsFromHeatmap(heatmap.squeeze(), self.conf_thresh, self.nms)
            pts_B = getPtsFromHeatmap(heatmap_B.squeeze(), self.conf_thresh, self.nms)  # pts_B 只包含匹配区域的预测点
            '''截断150'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :150]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :150]

            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,160,32]
            # from utils.utils import warp_points
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)
            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()
            print("total points: ", pts_A.shape)

            '''剔除DOG图黑边范围内的点'''
            mask_a = (pts_A[:, 0] >= 1) * (pts_A[:, 0] <= 30) * (pts_A[:, 1] >= 3) * (pts_A[:, 1] <= 132)
            mask_b = (pts_B[:, 0] >= 1) * (pts_B[:, 0] <= 30) * (pts_B[:, 1] >= 3) * (pts_B[:, 1] <= 132)
            pts_A = pts_A[mask_a]
            pts_B = pts_B[mask_b]
            mask = (warped_pnts[:, 0] >= 1) * (warped_pnts[:, 0] <= 30) * (warped_pnts[:, 1] >= 3) * (warped_pnts[:, 1] <= 132)
            warped_pnts = warped_pnts[mask]

            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            '''传统扩边-逆:36->32'''
            tra_ptsA, tra_ptsB = sample['ptsA'].squeeze(), sample['ptsB'].squeeze()
            mask_pts = (tra_ptsA[:, 0] >= 2) * (tra_ptsA[:, 0] <= 33)
            tra_ptsA = tra_ptsA[mask_pts]
            tra_ptsA[:, 0 ] -= 2    # 36->32  cut 2 pixel
            mask_pts = (tra_ptsB[:, 0] >= 2) * (tra_ptsB[:, 0] <= 33)
            tra_ptsB = tra_ptsB[mask_pts]
            tra_ptsB[:, 0 ] -= 2
            
            tra_warped_pts = warp_points(tra_ptsA, mat_H)
            tra_warped_pts, _ = filter_points(tra_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            '''剔除DOG图黑边范围内的点'''
            mask_a = (tra_ptsA[:, 0] >= 1) * (tra_ptsA[:, 0] <= 30) * (tra_ptsA[:, 1] >= 3) * (tra_ptsA[:, 1] <= 132)
            mask_b = (tra_ptsB[:, 0] >= 1) * (tra_ptsB[:, 0] <= 30) * (tra_ptsB[:, 1] >= 3) * (tra_ptsB[:, 1] <= 132)
            tra_ptsA = tra_ptsA[mask_a]
            tra_ptsB = tra_ptsB[mask_b]
            mask = (tra_warped_pts[:, 0] >= 1) * (tra_warped_pts[:, 0] <= 30) * (tra_warped_pts[:, 1] >= 3) * (tra_warped_pts[:, 1] <= 132)
            tra_warped_pts = tra_warped_pts[mask]

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": tra_ptsA})
            pred.update({'tra_pts_B': tra_ptsB})   # (x, y, 1)

            ## output images for visualization labels
            if self.output_images:
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy())
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, tra_warped_pts.numpy())
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    img_2D_warped = sample['imgA_warped'].numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=3, fy=3)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2])
                repeat_ratio = len(set(match_idx.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out += repeat_ratio
                print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                tra_ptsB = torch.tensor(tra_ptsB, dtype=torch.float32)    # sift点
                tra_match_idx, mask_idx = get_point_pair_repeat(tra_warped_pts.squeeze(), tra_ptsB[:, :2])
                repeat_ratio_tra = len(set(tra_match_idx.numpy().tolist())) / (len(tra_warped_pts.squeeze()) + eps)

                tra_repeat_out += repeat_ratio_tra
                print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            tra_numA, tra_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator, denominator = len(set(match_idx.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            tra_numerator, tra_denominator = len(set(tra_match_idx.numpy().tolist())), len(tra_warped_pts.squeeze())
            content.append([nameA, nameB, repeat_ratio, numerator, denominator, numA, numB, repeat_ratio_tra, tra_numerator, tra_denominator, tra_numA, tra_numB])

            count += 1
            if count == 50000:
                break
        df = pd.DataFrame(content, 
                        columns=[
                            'nameA',
                            'nameB',
                            'net repeat ratio',
                            'numerator',
                            'denominator',
                            'numA',
                            'numB',
                            'traditional repeat ratio',
                            'tra_numerator',
                            'tra_denominator',
                            'tra_numA',
                            'tra_numB'
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out / count, tra_repeat_out / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!! Or check the 'check_exist'!!!")

        pass

class Test_enhance(object):
    def __init__(self, img_path, info_path, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ load imagefiles ------
        names = []
        image_paths = []
        sift_paths = []

        e = [str(p) for p in Path(img_path, "pic").iterdir()]
        f = [p.replace("pic", "csv") for p in e]
        f = [p.replace(".bmp", ".csv") for p in f]
        n = [(p.split('/')[-1])[:-4] for p in f]
        image_paths.extend(e)
        sift_paths.extend(f)
        names.extend(n)
        
        sequence_set = []
        for (image, sift, N) in zip(image_paths, sift_paths, names):   # zip返回元组

            sample = {
                'img': image, 
                'sift': sift, 
                'name': N,
                }   # 无标签时，只有img数据
            sequence_set.append(sample)

        self.samples = sequence_set
        # self.iterations = len(self.samples)c
        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        sample = self.samples[index]
        # sample2 = self.samples2[index]  # img & traditional points
        imgA_path = sample['img']
        imgB_path = sample['sift']
        homo = np.array(sample['m_all'])
        homo = torch.tensor(homo.reshape(2, 3), dtype=torch.float32)
        homo = 2 * homo / 512.
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo, vec_one), 0)

        if 1:
            # img_o_A = load_as_float(sample1['imgA'])
            # img_o_B = load_as_float(sample1['imgB'])
            img_o_A = pd.read_csv(imgA_path, header=None).to_numpy()
            img_o_B = pd.read_csv(imgB_path, header=None).to_numpy()
            img_o_A = img_o_A.astype(np.float32)
            img_o_B = img_o_B.astype(np.float32)
            img_o_A = (img_o_A + 3500) / 7000.
            img_o_B = (img_o_B + 3500) / 7000.
            img_o_A[:3, :], img_o_B[:3, :] = 0., 0.
            img_o_A[:, :3], img_o_B[:, :3] = 0., 0.
            img_o_A[(img_o_A.shape[0] - 3):, :], img_o_B[(img_o_B.shape[0] - 3):, :] = 0., 0.
            img_o_A[:, (img_o_A.shape[1] - 3):], img_o_B[:, (img_o_B.shape[1] - 3):] = 0., 0.
            # img_o_A, img_o_B = img_o_A / 255., img_o_B / 255.
        else:
            img_o_A = np.load(sample['imgA'])
            img_o_B = np.load(sample['imgB'])
        # img_o = (img_o + 32768) / 65535.
            img_o_A = (img_o_A + 3500) / 7000.
            img_o_B = (img_o_B + 3500) / 7000.
            img_o_A[:3, :], img_o_B[:3, :] = 0., 0.
            img_o_A[:, :3], img_o_B[:, :3] = 0., 0.
            img_o_A[(img_o_A.shape[0] - 3):, :], img_o_B[(img_o_B.shape[0] - 3):, :] = 0., 0.
            img_o_A[:, (img_o_A.shape[1] - 3):], img_o_B[:, (img_o_B.shape[1] - 3):] = 0., 0.

        img_A_ori = self.transforms(img_o_A)
        img_B_ori = self.transforms(img_o_B)

        '''传统增强扩边处理-逆：36->32'''
        img_A = img_A_ori[0, :, 2:-2].unsqueeze(0)   # [1, 160, 32]
        img_B = img_B_ori[0, :, 2:-2].unsqueeze(0)

        H, W = img_A.shape[1], img_A.shape[2]

        img_A = torch.tensor(img_A, dtype=torch.float32).view(-1, H, W)
        img_B = torch.tensor(img_B, dtype=torch.float32).view(-1, H, W)
        imgA_warped = inv_warp_image(img_A.squeeze(), homo, mode='bilinear').unsqueeze(0)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        # idxA = self.files2['img_6159'].index(imgA_path) # 获取同手指图在点坐标列表中的index
        # idxB = self.files2['img_6159'].index(imgB_path)
        # ptsA = np.array(self.files2['pts'][idxA], dtype=np.float32).reshape(-1, 2)    #(x, y)
        # ptsB = np.array(self.files2['pts'][idxB], dtype=np.float32).reshape(-1, 2)

        ptsA = np.load(sample['pointA'])[:, [1,0]]
        ptsB = np.load(sample['pointB'])[:, [1,0]]
        ptsA = torch.tensor(ptsA).type(torch.FloatTensor)
        ptsB = torch.tensor(ptsB).type(torch.FloatTensor)

        input  = {}
        input.update({
            'imgA': img_A,  
            'imgB': img_B,  # 模板
            'imgA_warped': imgA_warped, # 非模板旋转
            'imgA_warped_mask': imgA_warped_mask,
            'imgA_name': sample['imgA_name'],
            'imgB_name': sample['imgB_name'],
            'H_AB': homo,
            'ptsA': ptsA,
            'ptsB': ptsB
            })

        return input

    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        tra_repeat_out = 0.
        content = []

        for idx in range(len(self.samples)):

            sample = self.get_data(idx)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['H_AB']
            # imgA, imgB= (
            #     imgA.to(self.device),
            #     imgB.to(self.device),
            # )
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['imgA_name']
            nameB = sample['imgB_name']
            nameAB = str(nameA) + '_' + str(nameB)
            logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # pass through network
            heatmap = FPDT.run_heatmap(imgA.to(self.device)).detach().cpu()
            heatmap_B = FPDT.run_heatmap(imgB.to(self.device)).detach().cpu()

            # heatmap_B = heatmap_B * imgA_warped_mask    # A-->B 后进行mask，只显示匹配区域
            pts_A = getPtsFromHeatmap(heatmap.squeeze(), self.conf_thresh, self.nms)
            pts_B = getPtsFromHeatmap(heatmap_B.squeeze(), self.conf_thresh, self.nms)  # pts_B 只包含匹配区域的预测点
            '''截断150'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :150]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :150]

            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,160,32]
            # from utils.utils import warp_points
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)
            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()
            print("total points: ", pts_A.shape)

            '''剔除DOG图黑边范围内的点'''
            mask_a = (pts_A[:, 0] >= 1) * (pts_A[:, 0] <= 30) * (pts_A[:, 1] >= 3) * (pts_A[:, 1] <= 132)
            mask_b = (pts_B[:, 0] >= 1) * (pts_B[:, 0] <= 30) * (pts_B[:, 1] >= 3) * (pts_B[:, 1] <= 132)
            pts_A = pts_A[mask_a]
            pts_B = pts_B[mask_b]
            mask = (warped_pnts[:, 0] >= 1) * (warped_pnts[:, 0] <= 30) * (warped_pnts[:, 1] >= 3) * (warped_pnts[:, 1] <= 132)
            warped_pnts = warped_pnts[mask]

            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            '''传统扩边-逆:36->32'''
            tra_ptsA, tra_ptsB = sample['ptsA'].squeeze(), sample['ptsB'].squeeze()
            mask_pts = (tra_ptsA[:, 0] >= 2) * (tra_ptsA[:, 0] <= 33)
            tra_ptsA = tra_ptsA[mask_pts]
            tra_ptsA[:, 0 ] -= 2    # 36->32  cut 2 pixel
            mask_pts = (tra_ptsB[:, 0] >= 2) * (tra_ptsB[:, 0] <= 33)
            tra_ptsB = tra_ptsB[mask_pts]
            tra_ptsB[:, 0 ] -= 2
            
            tra_warped_pts = warp_points(tra_ptsA, mat_H)
            tra_warped_pts, _ = filter_points(tra_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            '''剔除DOG图黑边范围内的点'''
            mask_a = (tra_ptsA[:, 0] >= 1) * (tra_ptsA[:, 0] <= 30) * (tra_ptsA[:, 1] >= 3) * (tra_ptsA[:, 1] <= 132)
            mask_b = (tra_ptsB[:, 0] >= 1) * (tra_ptsB[:, 0] <= 30) * (tra_ptsB[:, 1] >= 3) * (tra_ptsB[:, 1] <= 132)
            tra_ptsA = tra_ptsA[mask_a]
            tra_ptsB = tra_ptsB[mask_b]
            mask = (tra_warped_pts[:, 0] >= 1) * (tra_warped_pts[:, 0] <= 30) * (tra_warped_pts[:, 1] >= 3) * (tra_warped_pts[:, 1] <= 132)
            tra_warped_pts = tra_warped_pts[mask]

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": tra_ptsA})
            pred.update({'tra_pts_B': tra_ptsB})   # (x, y, 1)

            ## output images for visualization labels
            if self.output_images:
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy())
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, tra_warped_pts.numpy())
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    img_2D_warped = sample['imgA_warped'].numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=3, fy=3)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2])
                repeat_ratio = len(set(match_idx.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out += repeat_ratio
                print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                tra_ptsB = torch.tensor(tra_ptsB, dtype=torch.float32)    # sift点
                tra_match_idx, mask_idx = get_point_pair_repeat(tra_warped_pts.squeeze(), tra_ptsB[:, :2])
                repeat_ratio_tra = len(set(tra_match_idx.numpy().tolist())) / (len(tra_warped_pts.squeeze()) + eps)

                tra_repeat_out += repeat_ratio_tra
                print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            tra_numA, tra_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator, denominator = len(set(match_idx.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            tra_numerator, tra_denominator = len(set(tra_match_idx.numpy().tolist())), len(tra_warped_pts.squeeze())
            content.append([nameA, nameB, repeat_ratio, numerator, denominator, numA, numB, repeat_ratio_tra, tra_numerator, tra_denominator, tra_numA, tra_numB])

            count += 1
            if count == 50000:
                break
        df = pd.DataFrame(content, 
                        columns=[
                            'nameA',
                            'nameB',
                            'net repeat ratio',
                            'numerator',
                            'denominator',
                            'numA',
                            'numB',
                            'traditional repeat ratio',
                            'tra_numerator',
                            'tra_denominator',
                            'tra_numA',
                            'tra_numB'
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out / count, tra_repeat_out / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!! Or check the 'check_exist'!!!")

        pass

'''真实重复率测试（增强数据）'''
class Test_Real_Repeat_Enhance(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------
        match_path = Path('/hdd/file-input/qint/data/6159_cd_p11s400_enhance_image/log.1217.pick.txt')
        point_path = Path('/hdd/file-input/qint/data/6159_cd_p11s400_enhance_image/log2.feature.point.txt')

        # ------ matched images read ------
        df = pd.read_csv(
                    match_path,
                    header=None,
                    encoding = "gb2312",
                    names=['enroll', 'verify', 'path', 
                        'H0', 'H1', 'H2', 'H3', 'H4', 'H5',
                        'score', 'up']
                    )    # 'gb2312'
        image1_list = df['enroll'].to_list()
        image1_list = [i[8:].rjust(3, '0') for i in image1_list]    # 获得名称并补齐 '1'-> '001'
        image1_list = [i + '_enhance' for i in image1_list]
        image2_list = df['path'].to_list()
        image2_list = [i[7:] for i in image2_list]
        H0 = df['H0'].to_list()
        H0 = [(int)(h[6:]) for h in H0]
        H1 = df['H1'].to_list()
        H2 = df['H2'].to_list()
        H3 = df['H3'].to_list()
        H4 = df['H4'].to_list()
        H5 = df['H5'].to_list()

        for i in range(len(image2_list)):
            suffix = '.bmp'
            path_share = '/'.join(image2_list[i].split('/')[:-1])
            image1_list[i] = path_share + '/' + image1_list[i] + suffix
        trans = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]    # combined into list
        # trans = np.array(trans) # (N, 6)

        image1_name = ['_'.join(k.split('/')[-3:])[:-4] for k in image1_list]
        image2_name = ['_'.join(k.split('/')[-3:])[:-4] for k in image2_list]

        # ------ images points read ------
        coor = []
        coor_group = []
        img_6159_path = []
        df_p = pd.read_csv(point_path, header=None, encoding="gb2312", sep='\t')    # 'gb2312'

        for line in zip(df_p[0]):  # line: [index, .]
            if line[0][1:5] == '/hdd':
                '''换成npy格式(DOG数据)'''
                # tmp = line[0]
                # tmp = tmp.replace('bmp', 'npy')
                # tmp = tmp.replace('6159_cd_p11s400', '6159_cd_p11s400_DOG')
                # img_6159_path.append(tmp[1:])

                img_6159_path.append(line[0][1:])

            if line[0][1] == 'x':
                ls = re.split('[,:]', line[0])
                coor.append(ls[1])    # 提取x,y
                coor.append(ls[3])

            if (line[0][1] == '/') and (len(coor) != 0):
                coor_group.append(coor)
                coor = []

        # ------ dic ------
        files1 = {
            'imgA': image2_list,    
            'imgB': image1_list,       # 模板
            'imgA_name': image2_name,
            'imgB_name': image1_name,
            'trans': trans
        }
        self.files2 = {
            'img_6159': img_6159_path,
            'pts': coor_group
        }
    
        sequence_set1 = []
        sequence_set2 = []
        for (A, N_A, B, N_B, M_H) in zip(
            files1['imgA'], files1['imgA_name'],
            files1['imgB'], files1['imgB_name'],files1['trans']):   # zip返回元组

            '''换成npy格式(DOG数据)'''
            # A = A.replace('bmp', 'npy')
            # A = A.replace('6159_cd_p11s400', '6159_cd_p11s400_DOG')
            # B = B.replace('bmp', 'npy')
            # B = B.replace('6159_cd_p11s400', '6159_cd_p11s400_DOG')

            '''小位移'''
            if (int(N_A[8:11]) >= 200) or (int(N_B[8:11]) >= 200):
                continue
            '''旋转'''
            # if (int(N_A[8:11]) > 200 and int(N_B[8:11]) > 200) or (int(N_A[8:11]) < 200 and int(N_B[8:11]) < 200):
            #     continue

            sample = {
                'imgA': A, 
                'imgA_name': N_A, 
                'imgB': B, 
                'imgB_name': N_B,
                'm_all': M_H
                }   # 无标签时，只有img数据
            sequence_set1.append(sample)
        # for (I_6159, pts) in zip(files2['img_6159'], files2['pts']):   # zip返回元组
        #     sample = {
        #         'img_6159': I_6159,
        #         'pts': pts
        #         }   # 无标签时，只有img数据
        #     sequence_set1.append(sample)

        self.samples1 = sequence_set1
        # self.samples2 = sequence_set2
        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        sample = self.samples1[index]
        # sample2 = self.samples2[index]  # img & traditional points
        imgA_path = sample['imgA']
        imgB_path = sample['imgB']
        homo = np.array(sample['m_all'])
        homo = torch.tensor(homo.reshape(2, 3), dtype=torch.float32)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo, vec_one), 0)

        if 1:
            img_o_A = load_as_float(sample['imgA'])
            img_o_B = load_as_float(sample['imgB'])
        else:
            img_o_A = np.load(sample['imgA'])
            img_o_B = np.load(sample['imgB'])
        # img_o = (img_o + 32768) / 65535.
            img_o_A = (img_o_A + 3500) / 7000.
            img_o_B = (img_o_B + 3500) / 7000.
            img_o_A[:3, :], img_o_B[:3, :] = 0., 0.
            img_o_A[:, :3], img_o_B[:, :3] = 0., 0.
            img_o_A[(img_o_A.shape[0] - 3):, :], img_o_B[(img_o_B.shape[0] - 3):, :] = 0., 0.
            img_o_A[:, (img_o_A.shape[1] - 3):], img_o_B[:, (img_o_B.shape[1] - 3):] = 0., 0.

        img_A_ori = self.transforms(img_o_A)
        img_B_ori = self.transforms(img_o_B)

        '''传统增强扩边处理-逆：36->32'''
        img_A = img_A_ori[0, :, 2:-2].unsqueeze(0)   # [1, 160, 32]
        img_B = img_B_ori[0, :, 2:-2].unsqueeze(0)

        H, W = img_A.shape[1], img_A.shape[2]

        img_A = img_A.view(-1, H, W)
        img_B = img_B.view(-1, H, W)
        imgA_warped = inv_warp_image(img_A.squeeze(), homo, mode='bilinear').unsqueeze(0)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        idxA = self.files2['img_6159'].index(imgA_path) # 获取同手指图在点坐标列表中的index
        idxB = self.files2['img_6159'].index(imgB_path)
        ptsA = np.array(self.files2['pts'][idxA], dtype=np.float32).reshape(-1, 2)    #(x, y)
        ptsB = np.array(self.files2['pts'][idxB], dtype=np.float32).reshape(-1, 2)
        ptsA = torch.tensor(ptsA).type(torch.FloatTensor)
        ptsB = torch.tensor(ptsB).type(torch.FloatTensor)

        input  = {}
        input.update({
            'imgA': img_A,  
            'imgB': img_B,  # 模板
            'imgA_warped': imgA_warped, # 非模板旋转
            'imgA_warped_mask': imgA_warped_mask,
            'imgA_name': sample['imgA_name'],
            'imgB_name': sample['imgB_name'],
            'H_AB': homo,
            'ptsA': ptsA,
            'ptsB': ptsB
            })

        return input

    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        tra_repeat_out = 0.
        content = []

        total_count = len(self.samples1)
        for idx in range(len(self.samples1)):
            print("progress:{0}%".format(round((count + 1) * 100 / 50000)), end="\r")
            # print("it: ", count)
            sample = self.get_data(idx)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['H_AB']
            # imgA, imgB= (
            #     imgA.to(self.device),
            #     imgB.to(self.device),
            # )
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['imgA_name']
            nameB = sample['imgB_name']
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # pass through network
            heatmap, _ = FPDT.run_heatmap(imgA.to(self.device))
            heatmap_B, _ = FPDT.run_heatmap(imgB.to(self.device))

            # heatmap_B = heatmap_B * imgA_warped_mask    # A-->B 后进行mask，只显示匹配区域
            pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False)
            pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False)  # pts_B 只包含匹配区域的预测点
            '''截断150'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]

            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,160,32]
            # from utils.utils import warp_points
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)
            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()
            # print("total net points: ", pts_A.shape)

            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            '''传统扩边-逆:36->32'''
            tra_ptsA, tra_ptsB = sample['ptsA'].squeeze(), sample['ptsB'].squeeze()     # tensor
            mask_pts = (tra_ptsA[:, 0] >= 2) * (tra_ptsA[:, 0] <= 33)
            tra_ptsA = tra_ptsA[mask_pts]
            tra_ptsA[:, 0 ] -= 2    # 36->32  cut 2 pixel
            mask_pts = (tra_ptsB[:, 0] >= 2) * (tra_ptsB[:, 0] <= 33)
            tra_ptsB = tra_ptsB[mask_pts]
            tra_ptsB[:, 0 ] -= 2

            if self.top_k:
                if tra_ptsA.shape[0] > self.top_k:
                    tra_ptsA = tra_ptsA[:self.top_k, :]
                if tra_ptsB.shape[0] > self.top_k:
                    tra_ptsB = tra_ptsB[:self.top_k, :]
            
            tra_warped_pts = warp_points(tra_ptsA, mat_H)
            tra_warped_pts, _ = filter_points(tra_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": tra_ptsA})
            pred.update({'tra_pts_B': tra_ptsB})   # (x, y, 1)

            ## output images for visualization labels
            if self.output_images:
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy(), radius=1, s=1)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, tra_warped_pts.numpy(), radius=1, s=1)
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    img_2D_warped = sample['imgA_warped'].numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                # pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2])
                repeat_ratio = len(set(match_idx.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out += repeat_ratio
                # print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                # tra_ptsB = torch.tensor(tra_ptsB).type(torch.FloatTensor)   # sift点
                tra_match_idx, mask_idx = get_point_pair_repeat(tra_warped_pts.squeeze(), tra_ptsB[:, :2])
                repeat_ratio_tra = len(set(tra_match_idx.numpy().tolist())) / (len(tra_warped_pts.squeeze()) + eps)

                tra_repeat_out += repeat_ratio_tra
                # print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            tra_numA, tra_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator, denominator = len(set(match_idx.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            tra_numerator, tra_denominator = len(set(tra_match_idx.numpy().tolist())), len(tra_warped_pts.squeeze())
            content.append([nameA, nameB, repeat_ratio, numerator, denominator, numA, numB, repeat_ratio_tra, tra_numerator, tra_denominator, tra_numA, tra_numB])

            count += 1
            if count == 50000:
                break
        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'net repeat ratio',
                    'numerator',
                    'denominator',
                    'numA',
                    'numB',
                    'traditional repeat ratio',
                    'tra_numerator',
                    'tra_denominator',
                    'tra_numA',
                    'tra_numB'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out / count, tra_repeat_out / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass

# 带mask
class Test_Real_Repeat_Enhance_V2(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = 130 # config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = True # True False # config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.repeat_dis     = 1
        self.is_expand      = True
        self.bord           = 0 if not self.is_expand else 2

        self.test_dkd       = False
        self.is9800         = False # False
        self.is91000        = True
        self.partial_filter = True
        self.use_netori     = True
        self.ori_diff_thr   = 30 * 3.1415926 / 180
        self.plot_img_num   = 100 # 200000

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------
        # 6193_DK7_merge_test1_9800 6193_DK7_merge_test2_9800 6193-DK7-140-8-wet2_clear_test_9800 6193_DK7_XA_rot_test_9800 6193-DK4-130-purple-suppress-SNR28_9800
        sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1_9800/SIFT_Trans/SIFT_transSucc.csv')
        if not self.is9800:
            if self.is91000:
                self.sift_succ_dict = self.Decode_Succ_Fail_Csv_pnt(sift_succ_path, 'pnt_desc_data')
            else:
                self.sift_succ_dict = self.Decode_Succ_Fail_Csv_pnt(sift_succ_path, 'pnt_desc_data_X')
        else:
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set


    def Decode_Succ_Fail_Csv_pnt(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        if self.is91000: 
            img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_pnt_data'  # img_pnt_data img_pnt_data_9704
        else:
            img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_pnt_data_9704'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_kpt.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_kpt.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]
        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 122 x 36

        if self.isDilation:
            import torch.nn.functional as F
            if not self.is9800:
                if self.is_expand:
                    pad_size = (2, 2, 3, 3)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 30 -> 128 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                else:
                    pad_size = (2, 2, 0, 0)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 122 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                    # imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
                    # imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
            else:
                imgA = img_o_A[0, :, 6:-6].unsqueeze(0)      # 128 x 52 -> 128 x 40
                imgB = img_o_B[0, :, 6:-6].unsqueeze(0) 
                img_o_A = img_o_A[0, 3:-3, 8:-8].unsqueeze(0)   # 128 x 52 -> 122 x 36
                img_o_B = img_o_B[0, 3:-3, 8:-8].unsqueeze(0)
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]       # 122 X 36

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = inv_warp_image(img_o_B.squeeze(), torch.inverse(homo))
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_debase(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        import torch.nn.functional as F
        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(str(samples[index]['img_vf']).replace('_9800', '').replace('img_pnt_data', 'img_ori_data').replace('_kpt', ''))
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        img_o_A = F.pad(img_o_A, (2, 2, 2, 2) , "constant", 0)    # 118 x 32 -> 122 x 36 

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(str(samples[index]['img_en']).replace('_9800', '').replace('img_pnt_data', 'img_ori_data').replace('_kpt', ''))   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 122 x 36
        img_o_B = F.pad(img_o_B, (2, 2, 2, 2) , "constant", 0)    # 118 x 32 -> 122 x 36 

        if self.isDilation:
            if not self.is9800:
                if self.is_expand:
                    pad_size = (2, 2, 3, 3)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                else:
                    pad_size = (2, 2, 0, 0)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 122 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                    # imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
                    # imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
            else:
                imgA = img_o_A[0, :, 6:-6].unsqueeze(0)      # 128 x 52 -> 128 x 40
                imgB = img_o_B[0, :, 6:-6].unsqueeze(0) 
                img_o_A = img_o_A[0, 3:-3, 8:-8].unsqueeze(0)   # 128 x 52 -> 122 x 36
                img_o_B = img_o_B[0, 3:-3, 8:-8].unsqueeze(0)
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]       # 122 X 36

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = inv_warp_image(img_o_B.squeeze(), torch.inverse(homo))
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_netori(self, intensity, pnmap, pts_ori):
        H_o, W_o = intensity.shape[0], intensity.shape[1]      # 122x36 
        points_o = torch.tensor(pts_ori.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
        if self.use_netori and intensity is not None and pnmap is not None:
            points_o_normalized = points_o / points_o.new_tensor([W_o - 1, H_o - 1]).to(points_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

            ori_kp_predict_all = F.grid_sample(intensity.view(-1, H_o, W_o).unsqueeze(0),
                                    points_o_normalized.float().view(1, 1, -1, 2),
                                    mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
        
            pn_kp_predict_all = F.grid_sample(pnmap.view(-1, H_o, W_o).unsqueeze(0),
                                    points_o_normalized.float().view(1, 1, -1, 2),
                                    mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
            # 正负号mask
            pn_predict = (pn_kp_predict_all > 0.5).float().squeeze()   

            # print(pn_predict.shape, ori_kp_predict_all[:, 0])
            # 强度loss
            pai_coef = 3.1415926
            net_ori = ori_kp_predict_all[:, 0] * (2 * pn_predict - 1) * pai_coef / 2   # 正负90度

            return net_ori

    def test_process(self, FPDT):
        count = 0
        repeat_out1, repeat_out2 = 0., 0.
        sift_totalrepeat_out1, sift_totalrepeat_out2 = 0., 0.
        content = []

        total_count = len(self.sift_succ_dict)
        print(total_count)
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            # sample = self.Extract_Csv(idx, self.sift_succ_dict)
            sample = self.Extract_Succ_Csv_rot91_siftq_t2s(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            
            imgA_warped_mask = sample['imgA_warped_mask']

            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            if self.test_dkd:
                pts_A = FPDT.run_pts_alike(imgA.to(self.device))   # 120 X 40
                pts_B = FPDT.run_pts_alike(imgB.to(self.device))
            else:
                # pass through network
                if not self.is9800:
                    if self.is_expand:
                        # print(imgA[:, :, 3, :])
                        heatmap, _ = FPDT.run_heatmap_alike_9800(imgA.to(self.device))   # 128 X 40

                        heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB.to(self.device))
                        
                        # print(torch.sum(heatmap))
                        # print(heatmap[:, 5, :])
                        # exit()

                        # 有角度图
                        intensityA, intensityB = None, None
                        pnmapA, pnmapB = None, None
                        if self.use_netori and heatmap.shape[0] == 3 and heatmap_B.shape[0] == 3:
                            intensityA = heatmap[1, 3:-3, 2:-2]
                            intensityB = heatmap_B[1, 3:-3, 2:-2]
                            pnmapA = heatmap[2, 3:-3, 2:-2]
                            pnmapB = heatmap_B[2, 3:-3, 2:-2]

                        heatmap = heatmap[0, 3:-3, 2:-2]           # 128 x 40 -> 122 x 36
                        # print(torch.sum(torch.tensor(heatmap)))
                        heatmap_B = heatmap_B[0, 3:-3, 2:-2]   
                        if self.partial_filter:
                            heatmap = heatmap * imgA_partial_mask.to(heatmap.device)  
                            heatmap_B = heatmap_B * imgB_partial_mask.to(heatmap_B.device)                
                    else:
                        heatmap, _ = FPDT.run_heatmap_alike(imgA.to(self.device))   # 120 X 40
                        heatmap_B, _ = FPDT.run_heatmap_alike(imgB.to(self.device))
                else:
                    heatmap, _ = FPDT.run_heatmap_alike_9800(imgA.to(self.device))   # 128 X 40
                    heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB.to(self.device))
                # heatmap_B = heatmap_B * imgA_warped_mask    # A-->B 后进行mask，只显示匹配区域
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 120 x 40
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点

                # print(pts_A, pts_A.shape)  
                # exit()          
            '''后续计算均还原到原图136*36上'''
            if self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 122 x 36
                if not self.is9800:
                    if not self.is_expand:
                        mask_ptsA = (pts_A[0, :] >= 4) * (pts_A[0, :] <= 35) * (pts_A[1, :] >= 1) * (pts_A[1, :] <= 118)        # 122 x 36 中心 118 x 32
                        pts_A = pts_A[:, mask_ptsA]
                        pts_A[0, :] -= 2    # 40 -> 36
                        pts_A[1, :] += 1    # 120 -> 122
                        mask_ptsB = (pts_B[0, :] >= 4) * (pts_B[0, :] <= 35) * (pts_B[1, :] >= 1) * (pts_B[1, :] <= 118)        # 122 x 36 中心 118 x 32
                        pts_B = pts_B[:, mask_ptsB]
                        pts_B[0, :] -= 2
                        pts_B[1, :] += 1
                else:
                    mask_ptsA = (pts_A[0, :] >= 4) * (pts_A[0, :] <= 35) * (pts_A[1, :] >= 5) * (pts_A[1, :] <= 122)        # 128 x 40 中心 118 x 32
                    pts_A = pts_A[:, mask_ptsA]
                    pts_A[0, :] -= 2    # 40 -> 36
                    pts_A[1, :] -= 3    # 128 -> 122
                    mask_ptsB = (pts_B[0, :] >= 4) * (pts_B[0, :] <= 35) * (pts_B[1, :] >= 5) * (pts_B[1, :] <= 122)        # 128 x 40 中心 118 x 32
                    pts_B = pts_B[:, mask_ptsB]
                    pts_B[0, :] -= 2
                    pts_B[1, :] -= 3
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                pts_A[0, :] += 2
                pts_B[0, :] += 2
            
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)

            # if self.partial_filter:
            #     pntA_msk = imgA_partial_mask[pts_A[1, :].long(), pts_A[0, :].long()] > 0.5
            #     pntB_msk = imgB_partial_mask[pts_B[1, :].long(), pts_B[0, :].long()] > 0.5
            #     pts_A = pts_A[:, pntA_msk]
            #     pts_B = pts_B[:, pntB_msk]
            
            '''截断'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]
            
            if self.use_netori and intensityA is not None and intensityB is not None:
               net_oriA = self.get_netori(intensityA, pnmapA, pts_A)
               net_oriB = self.get_netori(intensityB, pnmapB, pts_B)
               
            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,122,36]

            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()
            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            pai_coef = 3.1415926
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()     # tensor
            sift_oriA, sift_oriB = sample['ori_vf'] * pai_coef / 180, sample['ori_en'] * pai_coef / 180

            # # 与C代码测试分数一致性
            # print((((net_oriA + pai_coef / 2).float() * 4096).int()).int())
            # print(torch.sum(((pts_A[:, 0])).int()))
            # print(torch.sum(((pts_A[:, 1])).int()))
            # print(torch.sum(((pts_A[:, -1]) * (-3500)).int()))
            # print(torch.sum(((net_oriA + pai_coef / 2) * 4096).int()))
            # print((torch.sum(((pts_A[:, 0])).int())) + (torch.sum(((pts_A[:, 1])).int())) + (torch.sum(((pts_A[:, -1]) * (-3500)).int())) + (torch.sum(((net_oriA + pai_coef / 2) * 4096).int())))
            # exit()

            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]

            if self.use_netori and intensityA is not None and intensityB is not None:
               siftnet_oriA = self.get_netori(intensityA, pnmapA, sift_ptsA.transpose(0, 1))
               siftnet_oriB = self.get_netori(intensityB, pnmapB, sift_ptsB.transpose(0, 1))
            
            sift_warped_pts = warp_points(sift_ptsA, mat_H)
            sift_warped_pts, _ = filter_points(sift_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": sift_ptsA})
            pred.update({'tra_pts_B': sift_ptsB})   # (x, y, 1)

            ori_diffmaxA = torch.max(torch.abs(siftnet_oriA - sift_oriA.to(self.device)))
            ori_diffmaxB = torch.max(torch.abs(siftnet_oriB - sift_oriB.to(self.device)))

            ## output images for visualization labels
            if self.output_images and count <= self.plot_img_num and (ori_diffmaxA > self.ori_diff_thr or ori_diffmaxB > self.ori_diff_thr): # 100
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy(), radius=3, s=3)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, sift_warped_pts.numpy(), radius=3, s=3)
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    if self.use_netori:
                        sift_pred, net_pred, img_pair = {}, {}, {}
                        sift_pred.update({
                            "pts": sift_ptsA.detach().cpu().numpy(), 
                            "pts_H": sift_ptsB.detach().cpu().numpy(),
                            "lab": None, #pts_A_lable,
                            "lab_H": None, #pts_B_lable,
                            "pts_TH": None, # warped_pts_A.detach().cpu().numpy(),
                            "pts_repeatA": None, #pos_repeatA.detach().cpu().numpy(),
                            "pts_repeatB": None, #pos_repeatB.detach().cpu().numpy(),
                            "pts_nncandA": None, #pos_nncandA.detach().cpu().numpy(),
                            "pts_nncandB": None, #pos_nncandB.detach().cpu().numpy(),
                            "pts_nnA": None, # pos_nnA.detach().cpu().numpy(),
                            "pts_nnB": None, # pos_nnB.detach().cpu().numpy(),
                            'pts_degree': siftnet_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree': siftnet_oriB.squeeze().detach().cpu().numpy(), 
                            'pts_degree_label': sift_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree_label': sift_oriB.squeeze().detach().cpu().numpy(),
                            })
                        
                        net_pred.update({
                            "pts": pts_A[:, :2].detach().cpu().numpy(), 
                            "pts_H": pts_B[:, :2].detach().cpu().numpy(),
                            "lab": None, #pts_A_lable,
                            "lab_H": None, #pts_B_lable,
                            "pts_TH": None, # warped_pts_A.detach().cpu().numpy(),
                            "pts_repeatA": None, #pos_repeatA.detach().cpu().numpy(),
                            "pts_repeatB": None, #pos_repeatB.detach().cpu().numpy(),
                            "pts_nncandA": None, #pos_nncandA.detach().cpu().numpy(),
                            "pts_nncandB": None, #pos_nncandB.detach().cpu().numpy(),
                            "pts_nnA": None, # pos_nnA.detach().cpu().numpy(),
                            "pts_nnB": None, # pos_nnB.detach().cpu().numpy(),
                            'pts_degree': net_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree': net_oriB.squeeze().detach().cpu().numpy(), 
                            'pts_degree_label': net_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree_label': net_oriB.squeeze().detach().cpu().numpy(),
                            })
                        '''NET'''
                        # img_2D_A = imgA.numpy().squeeze()[3:-3, 8:-8]       # 122 x 36
                        # img_2D_B = imgB.numpy().squeeze()[3:-3, 8:-8]
                        img_pair.update({'img': img_2D_A, 'img_H': img_2D_B})
                        # img_base_pair.update({'img': imgA_base.numpy().squeeze(), 'img_H': imgB_base.numpy().squeeze()})
                        img_siftp_angle_match = draw_match_pair_degree_match(img_pair, sift_pred, color=(255, 0, 0), radius=3, s=3, Htrans=mat_H)
                        fsift = os.path.join(self.output_dir, nameAB + '_siftp_match.bmp')
                        cv2.imwrite(str(fsift), img_siftp_angle_match)

                        img_netp_angle_match = draw_match_pair_degree_match(img_pair, net_pred, color=(255, 0, 0), radius=3, s=3, Htrans=mat_H)
                        fnet = os.path.join(self.output_dir, nameAB + '_netp_match.bmp')
                        cv2.imwrite(str(fnet), img_netp_angle_match)
                        # exit()
                    else:
                        # 画sift配准图
                        img_2D_warped = sample['imgA_o_warped'].numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                # pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                # match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2], correspond=self.repeat_dis)
                match_idx1, mask_idx1, match_idx2, mask_idx2 = get_point_pair_repeat_double(warped_pnts.squeeze(), pts_B[:, :2], correspond=self.repeat_dis)
                repeat_ratio1 = len(set(match_idx1.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out1 += repeat_ratio1
                repeat_ratio2 = len(set(match_idx2.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out2 += repeat_ratio2
                # print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                # tra_ptsB = torch.tensor(tra_ptsB).type(torch.FloatTensor)   # sift点
                tra_match_idx1, mask_idx1, tra_match_idx2, mask_idx2 = get_point_pair_repeat_double(sift_warped_pts.squeeze(), sift_ptsB[:, :2], correspond=self.repeat_dis)
                sift_repeat_ratio1 = len(set(tra_match_idx1.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                sift_totalrepeat_out1 += sift_repeat_ratio1

                sift_repeat_ratio2 = len(set(tra_match_idx2.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                sift_totalrepeat_out2 += sift_repeat_ratio2
                # print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            sift_numA, sift_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator1, numerator2, denominator = len(set(match_idx1.numpy().tolist())), len(set(match_idx2.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            sift_numerator1, sift_numerator2, sift_denominator = len(set(tra_match_idx1.numpy().tolist())), len(set(tra_match_idx2.numpy().tolist())), len(sift_warped_pts.squeeze())
            '''Trans angle'''
            mat_H = sample['homo']
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi  # [0, 360]

            content.append([nameA, nameB, trans_angle, repeat_ratio1, numerator1, repeat_ratio2, numerator2, denominator, numA, numB, sift_repeat_ratio1, sift_numerator1, sift_repeat_ratio2, sift_numerator2, sift_denominator, sift_numA, sift_numB])



        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net repeat ratio1',
                    'net_numerator1',
                    'net repeat ratio2',
                    'net_numerator2',
                    'net_denominator',
                    'net_numA',
                    'net_numB',
                    'sift repeat ratio1',
                    'sift_numerator1',
                    'sift repeat ratio2',
                    'sift_numerator2',
                    'sift_denominator',
                    'sift_numA',
                    'sift_numB'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("dis<1: net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out1 / count, sift_totalrepeat_out1 / count))      # 均值
            print("dis<2: net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out2 / count, sift_totalrepeat_out2 / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass

# 带mask
class Test_Real_Repeat_Enhance_6195_V2(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = 130 # config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = True # True False # config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.repeat_dis     = 1
        self.is_expand      = True
        self.bord           = 0 if not self.is_expand else 2

        self.test_dkd       = False
        self.is9800         = False # False
        self.is91000        = True
        self.is_imitate_95  = False
        self.partial_filter = True
        self.use_netori     = True
        self.ori_diff_thr   = 30 * 3.1415926 / 180
        self.plot_img_num   = 30000 # 200000  100

        self.enhance_version = '615'     # '20577' '91' '612' '615'

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------
        # 6193_DK7_merge_test1_9800 6193_DK7_merge_test2_9800 6193-DK7-140-8-wet2_clear_test_9800 6193_DK7_XA_rot_test_9800 6193-DK4-130-purple-suppress-SNR28_9800
        sift_succ_path = Path('/home/linwc/match/data/6195Test/6195_DK7140_wet_' + self.enhance_version + '/SIFT_Trans/SIFT_transSucc.csv')
        if not self.is9800:
            if self.is91000:
                self.sift_succ_dict = self.Decode_Succ_Fail_Csv_pnt(sift_succ_path, 'pnt_desc_data')
            else:
                self.sift_succ_dict = self.Decode_Succ_Fail_Csv_pnt(sift_succ_path, 'pnt_desc_data_X')
        else:
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set


    def Decode_Succ_Fail_Csv_pnt(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        if self.is91000: 
            img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_pnt_data'  # img_pnt_data img_pnt_data_9704
        else:
            img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_pnt_data_9704'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_kpt.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_kpt.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]
        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 122 x 36

        if self.isDilation:
            import torch.nn.functional as F
            if not self.is9800:
                if self.is_expand:
                    pad_size = (5, 5, 3, 3)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 30 -> 128 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                else:
                    pad_size = (2, 2, 0, 0)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 122 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                    # imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
                    # imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
            else:
                imgA = img_o_A[0, :, 6:-6].unsqueeze(0)      # 128 x 52 -> 128 x 40
                imgB = img_o_B[0, :, 6:-6].unsqueeze(0) 
                img_o_A = img_o_A[0, 3:-3, 8:-8].unsqueeze(0)   # 128 x 52 -> 122 x 36
                img_o_B = img_o_B[0, 3:-3, 8:-8].unsqueeze(0)
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]       # 122 X 36

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

            if self.is_imitate_95:
                homo_modify = self.homography_centorcrop(homo, 0, 3) 
            else:
                homo_modify = copy.deepcopy(homo)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
            homo_modify = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = inv_warp_image(img_o_B.squeeze(), torch.inverse(homo))
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed,  'homo_modify': torch.inverse(homo_modify)})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_debase(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        import torch.nn.functional as F
        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(str(samples[index]['img_vf']).replace('_9800', '').replace('img_pnt_data', 'img_ori_data').replace('_kpt', ''))
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        img_o_A = F.pad(img_o_A, (2, 2, 2, 2) , "constant", 0)    # 118 x 32 -> 122 x 36 

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(str(samples[index]['img_en']).replace('_9800', '').replace('img_pnt_data', 'img_ori_data').replace('_kpt', ''))   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 122 x 36
        img_o_B = F.pad(img_o_B, (2, 2, 2, 2) , "constant", 0)    # 118 x 32 -> 122 x 36 

        if self.isDilation:
            if not self.is9800:
                if self.is_expand:
                    pad_size = (2, 2, 3, 3)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                else:
                    pad_size = (2, 2, 0, 0)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 122 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                    # imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
                    # imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
            else:
                imgA = img_o_A[0, :, 6:-6].unsqueeze(0)      # 128 x 52 -> 128 x 40
                imgB = img_o_B[0, :, 6:-6].unsqueeze(0) 
                img_o_A = img_o_A[0, 3:-3, 8:-8].unsqueeze(0)   # 128 x 52 -> 122 x 36
                img_o_B = img_o_B[0, 3:-3, 8:-8].unsqueeze(0)
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]       # 122 X 36

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_pnt_data', 'img_mask_data').replace('_kpt', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = inv_warp_image(img_o_B.squeeze(), torch.inverse(homo))
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_netori(self, intensity, pnmap, pts_ori):
        H_o, W_o = intensity.shape[0], intensity.shape[1]      # 122x36 
        points_o = torch.tensor(pts_ori.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
        if self.use_netori and intensity is not None and pnmap is not None:
            points_o_normalized = points_o / points_o.new_tensor([W_o - 1, H_o - 1]).to(points_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

            ori_kp_predict_all = F.grid_sample(intensity.view(-1, H_o, W_o).unsqueeze(0),
                                    points_o_normalized.float().view(1, 1, -1, 2),
                                    mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
        
            pn_kp_predict_all = F.grid_sample(pnmap.view(-1, H_o, W_o).unsqueeze(0),
                                    points_o_normalized.float().view(1, 1, -1, 2),
                                    mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
            # 正负号mask
            pn_predict = (pn_kp_predict_all > 0.5).float().squeeze()   

            # print(pn_predict.shape, ori_kp_predict_all[:, 0])
            # 强度loss
            pai_coef = 3.1415926
            net_ori = ori_kp_predict_all[:, 0] * (2 * pn_predict - 1) * pai_coef / 2   # 正负90度

            return net_ori

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    def test_process(self, FPDT):
        count = 0
        repeat_out1, repeat_out2 = 0., 0.
        sift_totalrepeat_out1, sift_totalrepeat_out2 = 0., 0.
        content = []

        total_count = len(self.sift_succ_dict)
        print(total_count)
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            # sample = self.Extract_Csv(idx, self.sift_succ_dict)
            sample = self.Extract_Succ_Csv_rot91_siftq_t2s(idx, self.sift_succ_dict)
            imgA, imgB, mat_H, mat_H_mod = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo'], sample['homo_modify']
            
            imgA_warped_mask = sample['imgA_warped_mask']

            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            if self.test_dkd:
                pts_A = FPDT.run_pts_alike(imgA.to(self.device))   # 120 X 40
                pts_B = FPDT.run_pts_alike(imgB.to(self.device))
            else:
                # pass through network
                if not self.is9800:
                    if self.is_expand:
                        # print(imgA[:, :, 3, :])
                        heatmap, _ = FPDT.run_heatmap_alike_9800(imgA.to(self.device))   # 128 X 40

                        heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB.to(self.device))
                        
                        # print(torch.sum(heatmap))
                        # print(heatmap[:, 5, :])
                        # exit()

                        # 有角度图
                        intensityA, intensityB = None, None
                        pnmapA, pnmapB = None, None
                        if self.use_netori and heatmap.shape[0] == 3 and heatmap_B.shape[0] == 3:
                            intensityA = heatmap[1, 3:-3, 5:-5]
                            intensityB = heatmap_B[1, 3:-3, 5:-5]
                            pnmapA = heatmap[2, 3:-3, 5:-5]
                            pnmapB = heatmap_B[2, 3:-3, 5:-5]

                        heatmap = heatmap[0, 3:-3, 5:-5]           # 128 x 40 -> 122 x 36
                        # print(torch.sum(torch.tensor(heatmap)))
                        heatmap_B = heatmap_B[0, 3:-3, 5:-5]   
                        if self.partial_filter:
                            heatmap = heatmap * imgA_partial_mask.to(heatmap.device)  
                            heatmap_B = heatmap_B * imgB_partial_mask.to(heatmap_B.device)                
                    else:
                        heatmap, _ = FPDT.run_heatmap_alike(imgA.to(self.device))   # 120 X 40
                        heatmap_B, _ = FPDT.run_heatmap_alike(imgB.to(self.device))
                else:
                    heatmap, _ = FPDT.run_heatmap_alike_9800(imgA.to(self.device))   # 128 X 40
                    heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB.to(self.device))
                # heatmap_B = heatmap_B * imgA_warped_mask    # A-->B 后进行mask，只显示匹配区域
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 120 x 40
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点

                # print(pts_A, pts_A.shape)  
                # exit()          
            '''后续计算均还原到原图136*36上'''
            if self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 122 x 36
                if not self.is9800:
                    if not self.is_expand:
                        mask_ptsA = (pts_A[0, :] >= 4) * (pts_A[0, :] <= 35) * (pts_A[1, :] >= 1) * (pts_A[1, :] <= 118)        # 122 x 36 中心 118 x 32
                        pts_A = pts_A[:, mask_ptsA]
                        pts_A[0, :] -= 2    # 40 -> 36
                        pts_A[1, :] += 1    # 120 -> 122
                        mask_ptsB = (pts_B[0, :] >= 4) * (pts_B[0, :] <= 35) * (pts_B[1, :] >= 1) * (pts_B[1, :] <= 118)        # 122 x 36 中心 118 x 32
                        pts_B = pts_B[:, mask_ptsB]
                        pts_B[0, :] -= 2
                        pts_B[1, :] += 1
                else:
                    mask_ptsA = (pts_A[0, :] >= 4) * (pts_A[0, :] <= 35) * (pts_A[1, :] >= 5) * (pts_A[1, :] <= 122)        # 128 x 40 中心 118 x 32
                    pts_A = pts_A[:, mask_ptsA]
                    pts_A[0, :] -= 2    # 40 -> 36
                    pts_A[1, :] -= 3    # 128 -> 122
                    mask_ptsB = (pts_B[0, :] >= 4) * (pts_B[0, :] <= 35) * (pts_B[1, :] >= 5) * (pts_B[1, :] <= 122)        # 128 x 40 中心 118 x 32
                    pts_B = pts_B[:, mask_ptsB]
                    pts_B[0, :] -= 2
                    pts_B[1, :] -= 3
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                pts_A[0, :] += 2
                pts_B[0, :] += 2
            
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)

            # if self.partial_filter:
            #     pntA_msk = imgA_partial_mask[pts_A[1, :].long(), pts_A[0, :].long()] > 0.5
            #     pntB_msk = imgB_partial_mask[pts_B[1, :].long(), pts_B[0, :].long()] > 0.5
            #     pts_A = pts_A[:, pntA_msk]
            #     pts_B = pts_B[:, pntB_msk]
            
            '''截断'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]
            
            if self.use_netori and intensityA is not None and intensityB is not None:
               net_oriA = self.get_netori(intensityA, pnmapA, pts_A)
               net_oriB = self.get_netori(intensityB, pnmapB, pts_B)
               
            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,122,30]

            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H_mod)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()
            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            pai_coef = 3.1415926
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()     # tensor
            sift_oriA, sift_oriB = sample['ori_vf'] * pai_coef / 180, sample['ori_en'] * pai_coef / 180

            if self.is_imitate_95:
                # 122 x 36 -> 118 x 26
                remove_border_r_w = 5   # 3
                remove_border_r_h = 2   # 0
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r_w, remove_border_r_h, W + 6, H)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r_w, remove_border_r_h, W + 6, H)
                sift_ptsA[:, 0] -= 3
                sift_ptsB[:, 0] -= 3
                sift_oriA = sift_oriA[borderA]
                sift_oriB = sift_oriB[borderB]

            # # 与C代码测试分数一致性
            # print((((net_oriA + pai_coef / 2).float() * 4096).int()).int())
            # print(torch.sum(((pts_A[:, 0])).int()))
            # print(torch.sum(((pts_A[:, 1])).int()))
            # print(torch.sum(((pts_A[:, -1]) * (-3500)).int()))
            # print(torch.sum(((net_oriA + pai_coef / 2) * 4096).int()))
            # print((torch.sum(((pts_A[:, 0])).int())) + (torch.sum(((pts_A[:, 1])).int())) + (torch.sum(((pts_A[:, -1]) * (-3500)).int())) + (torch.sum(((net_oriA + pai_coef / 2) * 4096).int())))
            # exit()

            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]

            if self.use_netori and intensityA is not None and intensityB is not None:
               siftnet_oriA = self.get_netori(intensityA, pnmapA, sift_ptsA.transpose(0, 1))
               siftnet_oriB = self.get_netori(intensityB, pnmapB, sift_ptsB.transpose(0, 1))
            
            sift_warped_pts = warp_points(sift_ptsA, mat_H_mod)
            sift_warped_pts, _ = filter_points(sift_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": sift_ptsA})
            pred.update({'tra_pts_B': sift_ptsB})   # (x, y, 1)

            ori_diffmaxA = torch.max(torch.abs(siftnet_oriA - sift_oriA.to(self.device)))
            ori_diffmaxB = torch.max(torch.abs(siftnet_oriB - sift_oriB.to(self.device)))

            ## output images for visualization labels
            if self.output_images and count <= self.plot_img_num and (ori_diffmaxA > self.ori_diff_thr or ori_diffmaxB > self.ori_diff_thr): # 100
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy(), radius=3, s=3)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, sift_warped_pts.numpy(), radius=3, s=3)
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    if self.use_netori:
                        sift_pred, net_pred, img_pair = {}, {}, {}
                        sift_pred.update({
                            "pts": sift_ptsA.detach().cpu().numpy(), 
                            "pts_H": sift_ptsB.detach().cpu().numpy(),
                            "lab": None, #pts_A_lable,
                            "lab_H": None, #pts_B_lable,
                            "pts_TH": None, # warped_pts_A.detach().cpu().numpy(),
                            "pts_repeatA": None, #pos_repeatA.detach().cpu().numpy(),
                            "pts_repeatB": None, #pos_repeatB.detach().cpu().numpy(),
                            "pts_nncandA": None, #pos_nncandA.detach().cpu().numpy(),
                            "pts_nncandB": None, #pos_nncandB.detach().cpu().numpy(),
                            "pts_nnA": None, # pos_nnA.detach().cpu().numpy(),
                            "pts_nnB": None, # pos_nnB.detach().cpu().numpy(),
                            'pts_degree': siftnet_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree': siftnet_oriB.squeeze().detach().cpu().numpy(), 
                            'pts_degree_label': sift_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree_label': sift_oriB.squeeze().detach().cpu().numpy(),
                            })
                        
                        net_pred.update({
                            "pts": pts_A[:, :2].detach().cpu().numpy(), 
                            "pts_H": pts_B[:, :2].detach().cpu().numpy(),
                            "lab": None, #pts_A_lable,
                            "lab_H": None, #pts_B_lable,
                            "pts_TH": None, # warped_pts_A.detach().cpu().numpy(),
                            "pts_repeatA": None, #pos_repeatA.detach().cpu().numpy(),
                            "pts_repeatB": None, #pos_repeatB.detach().cpu().numpy(),
                            "pts_nncandA": None, #pos_nncandA.detach().cpu().numpy(),
                            "pts_nncandB": None, #pos_nncandB.detach().cpu().numpy(),
                            "pts_nnA": None, # pos_nnA.detach().cpu().numpy(),
                            "pts_nnB": None, # pos_nnB.detach().cpu().numpy(),
                            'pts_degree': net_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree': net_oriB.squeeze().detach().cpu().numpy(), 
                            'pts_degree_label': net_oriA.squeeze().detach().cpu().numpy(),
                            'pts_H_degree_label': net_oriB.squeeze().detach().cpu().numpy(),
                            })
                        '''NET'''
                        # img_2D_A = imgA.numpy().squeeze()[3:-3, 8:-8]       # 122 x 36
                        # img_2D_B = imgB.numpy().squeeze()[3:-3, 8:-8]
                        img_pair.update({'img': img_2D_A, 'img_H': img_2D_B})
                        # img_base_pair.update({'img': imgA_base.numpy().squeeze(), 'img_H': imgB_base.numpy().squeeze()})
                        img_siftp_angle_match = draw_match_pair_degree_match(img_pair, sift_pred, color=(255, 0, 0), radius=3, s=3, Htrans=mat_H_mod)
                        fsift = os.path.join(self.output_dir, nameAB + '_siftp_match.bmp')
                        cv2.imwrite(str(fsift), img_siftp_angle_match)

                        img_netp_angle_match = draw_match_pair_degree_match(img_pair, net_pred, color=(255, 0, 0), radius=3, s=3, Htrans=mat_H_mod)
                        fnet = os.path.join(self.output_dir, nameAB + '_netp_match.bmp')
                        cv2.imwrite(str(fnet), img_netp_angle_match)
                        # exit()
                    else:
                        # 画sift配准图
                        img_2D_warped = sample['imgA_o_warped'].numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                # pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                # match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2], correspond=self.repeat_dis)
                match_idx1, mask_idx1, match_idx2, mask_idx2 = get_point_pair_repeat_double(warped_pnts.squeeze(), pts_B[:, :2], correspond=self.repeat_dis)
                repeat_ratio1 = len(set(match_idx1.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out1 += repeat_ratio1
                repeat_ratio2 = len(set(match_idx2.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out2 += repeat_ratio2
                # print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                # tra_ptsB = torch.tensor(tra_ptsB).type(torch.FloatTensor)   # sift点
                try:
                    tra_match_idx1, mask_idx1, tra_match_idx2, mask_idx2 = get_point_pair_repeat_double(sift_warped_pts.squeeze(), sift_ptsB[:, :2], correspond=self.repeat_dis)
                    sift_repeat_ratio1 = len(set(tra_match_idx1.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                    sift_totalrepeat_out1 += sift_repeat_ratio1

                    sift_repeat_ratio2 = len(set(tra_match_idx2.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                    sift_totalrepeat_out2 += sift_repeat_ratio2
                except:
                     tra_match_idx1, mask_idx1, tra_match_idx2, mask_idx2 = torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])
                     sift_repeat_ratio1 = 0
                     sift_repeat_ratio2 = 0

                # print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            sift_numA, sift_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator1, numerator2, denominator = len(set(match_idx1.numpy().tolist())), len(set(match_idx2.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            sift_numerator1, sift_numerator2, sift_denominator = len(set(tra_match_idx1.numpy().tolist())), len(set(tra_match_idx2.numpy().tolist())), len(sift_warped_pts.squeeze())
            '''Trans angle'''
            mat_H = sample['homo']
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi  # [0, 360]

            content.append([nameA, nameB, trans_angle, repeat_ratio1, numerator1, repeat_ratio2, numerator2, denominator, numA, numB, sift_repeat_ratio1, sift_numerator1, sift_repeat_ratio2, sift_numerator2, sift_denominator, sift_numA, sift_numB])



        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net repeat ratio1',
                    'net_numerator1',
                    'net repeat ratio2',
                    'net_numerator2',
                    'net_denominator',
                    'net_numA',
                    'net_numB',
                    'sift repeat ratio1',
                    'sift_numerator1',
                    'sift repeat ratio2',
                    'sift_numerator2',
                    'sift_denominator',
                    'sift_numA',
                    'sift_numB'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("dis<1: net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out1 / count, sift_totalrepeat_out1 / count))      # 均值
            print("dis<2: net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out2 / count, sift_totalrepeat_out2 / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass


class Test_Real_Repeat_Enhance_V3(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = 130 # config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = True # True False # config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 1 # config['nms']
        self.repeat_dis     = 1
        self.is_expand      = True
        self.bord           = 0 if not self.is_expand else 2

        self.test_dkd       = False
        self.is9800         = False # False

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------
        # 6193_DK7_merge_test1_9800 6193_DK7_merge_test2_9800 6193-DK7-140-8-wet2_clear_test_9800 6193_DK7_XA_rot_test_9800
        sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1_9800/SIFT_Trans/SIFT_transSucc.csv')
        if not self.is9800:
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_pnt(sift_succ_path, 'pnt_desc_data_X')
        else:
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set


    def Decode_Succ_Fail_Csv_pnt(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_pnt_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_kpt.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_kpt.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]
        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 122 x 36

        if self.isDilation:
            import torch.nn.functional as F
            if not self.is9800:
                if self.is_expand:
                    pad_size = (2, 2, 3, 3)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                else:
                    pad_size = (2, 2, 0, 0)     
                    imgA = F.pad(img_o_A, pad_size, "constant", 0)    # 122 x 36 -> 122 x 40 
                    imgB = F.pad(img_o_B, pad_size, "constant", 0)
                    # imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
                    # imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
            else:
                imgA = img_o_A[0, :, 6:-6].unsqueeze(0)      # 128 x 52 -> 128 x 40
                imgB = img_o_B[0, :, 6:-6].unsqueeze(0) 
                img_o_A = img_o_A[0, 3:-3, 8:-8].unsqueeze(0)   # 128 x 52 -> 122 x 36
                img_o_B = img_o_B[0, 3:-3, 8:-8].unsqueeze(0)
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]       # 122 X 36

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = inv_warp_image(img_o_B.squeeze(), torch.inverse(homo))
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def test_process(self, FPDT):
        count = 0
        repeat_out1, repeat_out2 = 0., 0.
        sift_totalrepeat_out1, sift_totalrepeat_out2 = 0., 0.
        content = []

        total_count = len(self.sift_succ_dict)
        print(total_count)
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            # sample = self.Extract_Csv(idx, self.sift_succ_dict)
            sample = self.Extract_Succ_Csv_rot91_siftq_t2s(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']

            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            if self.test_dkd:
                pts_A = FPDT.run_pts_alike(imgA.to(self.device))   # 120 X 40
                pts_B = FPDT.run_pts_alike(imgB.to(self.device))
            else:
                # pass through network
                if not self.is9800:
                    if self.is_expand:
                        heatmap, _ = FPDT.run_heatmap_alike_9800(imgA.to(self.device))   # 128 X 40
                        heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB.to(self.device))
                        # print(heatmap.shape)
                        heatmap = heatmap[:,3:-3, 2:-2]           # 128 x 40 -> 122 x 36
                        heatmap_B = heatmap_B[:, 3:-3, 2:-2]                        
                    else:
                        heatmap, _ = FPDT.run_heatmap_alike(imgA.to(self.device))   # 120 X 40
                        heatmap_B, _ = FPDT.run_heatmap_alike(imgB.to(self.device))
                else:
                    heatmap, _ = FPDT.run_heatmap_alike_9800(imgA.to(self.device))   # 128 X 40
                    heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB.to(self.device))
                # heatmap_B = heatmap_B * imgA_warped_mask    # A-->B 后进行mask，只显示匹配区域
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 120 x 40
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点
                            
            '''后续计算均还原到原图136*36上'''
            if self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 122 x 36
                if not self.is9800:
                    if not self.is_expand:
                        mask_ptsA = (pts_A[0, :] >= 4) * (pts_A[0, :] <= 35) * (pts_A[1, :] >= 1) * (pts_A[1, :] <= 118)        # 122 x 36 中心 118 x 32
                        pts_A = pts_A[:, mask_ptsA]
                        pts_A[0, :] -= 2    # 40 -> 36
                        pts_A[1, :] += 1    # 120 -> 122
                        mask_ptsB = (pts_B[0, :] >= 4) * (pts_B[0, :] <= 35) * (pts_B[1, :] >= 1) * (pts_B[1, :] <= 118)        # 122 x 36 中心 118 x 32
                        pts_B = pts_B[:, mask_ptsB]
                        pts_B[0, :] -= 2
                        pts_B[1, :] += 1
                else:
                    mask_ptsA = (pts_A[0, :] >= 4) * (pts_A[0, :] <= 35) * (pts_A[1, :] >= 5) * (pts_A[1, :] <= 122)        # 128 x 40 中心 118 x 32
                    pts_A = pts_A[:, mask_ptsA]
                    pts_A[0, :] -= 2    # 40 -> 36
                    pts_A[1, :] -= 3    # 128 -> 122
                    mask_ptsB = (pts_B[0, :] >= 4) * (pts_B[0, :] <= 35) * (pts_B[1, :] >= 5) * (pts_B[1, :] <= 122)        # 128 x 40 中心 118 x 32
                    pts_B = pts_B[:, mask_ptsB]
                    pts_B[0, :] -= 2
                    pts_B[1, :] -= 3
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                pts_A[0, :] += 2
                pts_B[0, :] += 2

            '''截断'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]

            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,122,36]
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)
            warped_pnts_ori = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts_ori.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()

            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()     # tensor
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]

            sift_warped_pts_ori = warp_points(sift_ptsA, mat_H)
            sift_warped_pts, _ = filter_points(sift_warped_pts_ori.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": sift_ptsA})
            pred.update({'tra_pts_B': sift_ptsB})   # (x, y, 1)

            ## output images for visualization labels
            if self.output_images and count <= 100:
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy(), radius=3, s=3)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, sift_warped_pts.numpy(), radius=3, s=3)
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    img_2D_warped = sample['imgA_o_warped'].numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                # pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                # match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2], correspond=self.repeat_dis)
                match_idx1, mask_idx1, match_idx2, mask_idx2 = get_point_pair_repeat_double(warped_pnts_ori.squeeze(), pts_B[:, :2], correspond=self.repeat_dis)
                repeat_ratio1 = len(set(match_idx1.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out1 += repeat_ratio1
                repeat_ratio2 = len(set(match_idx2.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out2 += repeat_ratio2
                # print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                # tra_ptsB = torch.tensor(tra_ptsB).type(torch.FloatTensor)   # sift点
                tra_match_idx1, mask_idx1, tra_match_idx2, mask_idx2 = get_point_pair_repeat_double(sift_warped_pts_ori.squeeze(), sift_ptsB[:, :2], correspond=self.repeat_dis)
                sift_repeat_ratio1 = len(set(tra_match_idx1.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                sift_totalrepeat_out1 += sift_repeat_ratio1

                sift_repeat_ratio2 = len(set(tra_match_idx2.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                sift_totalrepeat_out2 += sift_repeat_ratio2
                # print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            sift_numA, sift_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator1, numerator2, denominator = len(set(match_idx1.numpy().tolist())), len(set(match_idx2.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            sift_numerator1, sift_numerator2, sift_denominator = len(set(tra_match_idx1.numpy().tolist())), len(set(tra_match_idx2.numpy().tolist())), len(sift_warped_pts.squeeze())
            '''Trans angle'''
            mat_H = sample['homo']
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi  # [0, 360]

            content.append([nameA, nameB, trans_angle, repeat_ratio1, numerator1, repeat_ratio2, numerator2, denominator, numA, numB, sift_repeat_ratio1, sift_numerator1, sift_repeat_ratio2, sift_numerator2, sift_denominator, sift_numA, sift_numB])



        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net repeat ratio1',
                    'net_numerator1',
                    'net repeat ratio2',
                    'net_numerator2',
                    'net_denominator',
                    'net_numA',
                    'net_numB',
                    'sift repeat ratio1',
                    'sift_numerator1',
                    'sift repeat ratio2',
                    'sift_numerator2',
                    'sift_denominator',
                    'sift_numA',
                    'sift_numB'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("dis<1: net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out1 / count, sift_totalrepeat_out1 / count))      # 均值
            print("dis<2: net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out2 / count, sift_totalrepeat_out2 / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass


class Test_Real_Repeat_Enhance_Ip_V2(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'

        self.w_size         = config['w_size']
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------
        sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
        self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        sift_totalrepeat_out = 0.
        content = []

        total_count = len(self.sift_succ_dict)
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']

            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # pass through network
            heatmap, _ = FPDT.run_heatmap(imgA.to(self.device))
            heatmap_B, _ = FPDT.run_heatmap(imgB.to(self.device))
            # heatmap_B = heatmap_B * imgA_warped_mask    # A-->B 后进行mask，只显示匹配区域
            # ip_layer

            pts_A = getPtsFromHeatmapByNMS(heatmap, self.conf_thresh, self.nms).transpose(0, 1)
            pts_B = getPtsFromHeatmapByNMS(heatmap_B, self.conf_thresh, self.nms).transpose(0, 1)

            # pts_A = getPtsFromHeatmapByCoordinates(heatmap, self.conf_thresh, self.w_size, bord=0)
            # pts_B = getPtsFromHeatmapByCoordinates(heatmap_B, self.conf_thresh, self.w_size, bord=0)
            # pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False)
            # pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False)  # pts_B 只包含匹配区域的预测点
                        
            '''后续计算均还原到原图136*36上'''
            if self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                mask_pts = (pts_A[0, :] >= 2) * (pts_A[0, :] <= 37)
                pts_A = pts_A[:, mask_pts]
                pts_A[0, :] -= 2
                mask_pts = (pts_B[0, :] >= 2) * (pts_B[0, :] <= 37)
                pts_B = pts_B[:, mask_pts]
                pts_B[0, :] -= 2
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                pts_A[0, :] += 2
                pts_B[0, :] += 2

            '''截断'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]

            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,136,36]
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)
            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()

            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()     # tensor
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]

            sift_warped_pts = warp_points(sift_ptsA, mat_H)
            sift_warped_pts, _ = filter_points(sift_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": sift_ptsA})
            pred.update({'tra_pts_B': sift_ptsB})   # (x, y, 1)

            ## output images for visualization labels
            if self.output_images:
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy(), radius=3, s=3)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, sift_warped_pts.numpy(), radius=3, s=3)
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    img_2D_warped = sample['imgA_o_warped'].numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                # pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2], correspond=self.nms)
                repeat_ratio = len(set(match_idx.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out += repeat_ratio
                # print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                # tra_ptsB = torch.tensor(tra_ptsB).type(torch.FloatTensor)   # sift点
                tra_match_idx, mask_idx = get_point_pair_repeat(sift_warped_pts.squeeze(), sift_ptsB[:, :2], correspond=self.nms)
                sift_repeat_ratio = len(set(tra_match_idx.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                sift_totalrepeat_out += sift_repeat_ratio
                # print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            sift_numA, sift_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator, denominator = len(set(match_idx.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            sift_numerator, sift_denominator = len(set(tra_match_idx.numpy().tolist())), len(sift_warped_pts.squeeze())
            content.append([nameA, nameB, repeat_ratio, numerator, denominator, numA, numB, sift_repeat_ratio, sift_numerator, sift_denominator, sift_numA, sift_numB])


        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'net repeat ratio',
                    'net_numerator',
                    'net_denominator',
                    'net_numA',
                    'net_numB',
                    'sift repeat ratio',
                    'sift_numerator',
                    'sift_denominator',
                    'sift_numA',
                    'sift_numB'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out / count, sift_totalrepeat_out / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass

class Test_Real_Repeat_Enhance_Alike_V2(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.isrot          = True
        self.isrot91        = True
        self.remove_border_w = 2
        self.remove_border_h = 14
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_enhance_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front

    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        return pts, desc_front, desc_back

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        # imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = np.logical_or(pts[0, :] < bordW, pts[0, :] >= (W - bordW))
        toremoveH = np.logical_or(pts[1, :] < bordH, pts[1, :] >= (H - bordH))
        toremove = np.logical_or(toremoveW, toremoveH)
        pts = pts[:, ~toremove]
        return pts, ~toremove
    
    def remove_border_torch(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W - bord))
    #     toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H - bord))
    #     toremove = np.logical_or(toremoveW, toremoveH)
    #     pts = pts[:, ~toremove]
    #     return pts, ~toremove

    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        sift_totalrepeat_out = 0.
        content = []
        rdis = 2

        total_count = len(self.sift_succ_dict)
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:               
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']

            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # pass through network
            # # Get Points by DKD
            # H, W = imgA.shape[2], imgA.shape[3] 
            # pts_A_norm, _, _ = FPDT.run_pts_dkd(imgA.to(self.device))
            # pts_B_norm, _, _ = FPDT.run_pts_dkd(imgB.to(self.device))
            # pts_A = (pts_A_norm + 1) / 2 * pts_A_norm.new_tensor([[W - 1, H - 1]]).to(self.device)
            # pts_B = (pts_B_norm + 1) / 2 * pts_B_norm.new_tensor([[W - 1, H - 1]]).to(self.device)
            # pts_A = pts_A.transpose(0, 1)
            # pts_B = pts_B.transpose(0, 1)

            heatmap, _ = FPDT.run_heatmap(imgA.to(self.device))
            heatmap_B, _ = FPDT.run_heatmap(imgB.to(self.device))
            pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False)
            pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False)  # pts_B 只包含匹配区域的预测点
            remove_border_r = 2            
            '''后续计算均还原到原图136*36上'''
            if self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                mask_pts = (pts_A[0, :] >= 2) * (pts_A[0, :] <= 37)
                pts_A = pts_A[:, mask_pts]
                pts_A[0, :] -= 2
                mask_pts = (pts_B[0, :] >= 2) * (pts_B[0, :] <= 37)
                pts_B = pts_B[:, mask_pts]
                pts_B[0, :] -= 2
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                pts_A[0, :] += 2
                pts_B[0, :] += 2

            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,136,36]
            if remove_border_r > 0:
                if self.isrot and not self.isrot91:
                    pts_A[1, :] += 12
                    pts_B[1, :] += 12
                    pts_A, _ = self.remove_border(pts_A, self.remove_border_w, self.remove_border_h, W, H)
                    pts_B, _ = self.remove_border(pts_B, self.remove_border_w, self.remove_border_h, W, H)
                else:
                    pts_A, _ = self.remove_border(pts_A, remove_border_r, remove_border_r, W, H)
                    pts_B, _ = self.remove_border(pts_B, remove_border_r, remove_border_r, W, H)            

            '''截断'''
            if self.top_k_net:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]

            # A,B点集
            # H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,136,36]
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)

            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()

            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()     # tensor

            if self.isrot and not self.isrot91:
                sift_ptsA, _ = self.remove_border_torch(sift_ptsA, self.remove_border_w, self.remove_border_h, W, H)
                sift_ptsB, _ = self.remove_border_torch(sift_ptsB, self.remove_border_w, self.remove_border_h, W, H)
            
            if self.isrot91:
                sift_ptsA, _ = self.remove_border_torch(sift_ptsA, remove_border_r, remove_border_r, W, H)
                sift_ptsB, _ = self.remove_border_torch(sift_ptsB, remove_border_r, remove_border_r, W, H)

            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]

            sift_warped_pts = warp_points(sift_ptsA, mat_H)
            sift_warped_pts, _ = filter_points(sift_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": sift_ptsA})
            pred.update({'tra_pts_B': sift_ptsB})   # (x, y, 1)

            ## output images for visualization labels
            if self.output_images:
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy(), radius=3, s=3)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, sift_warped_pts.numpy(), radius=3, s=3)
                f = self.output_dir / (nameAB + '_tradition.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = False
                if require_image:
                    img_2D_warped = sample['imgA_o_warped'].numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_match.bmp'), image_merge)

                # '''inverse img'''
                # inv_img = img_2D.copy()
                # # for i in range(H):
                # #     for j in range(W):
                # #         inv_img[i][j] = 1 - img_2D[i][j]    # 模板反色
                # # A, B
                # b1, b2 = inv_img, img_2D_warped
                # g1, g2 = inv_img, np.zeros_like(img_2D)
                # r1, r2 = inv_img, np.ones_like(img_2D)
                # img1, img2 = cv2.merge([b1, g1, r1]), cv2.merge([b2, g2, r2])
                # image_merge2 = (img1 * 0.5 + img2 * 0.5) * 255
                # cv2.imwrite(os.path.join(save_output, nameAB + '_merge2.bmp'), image_merge2)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                # pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2], correspond=rdis)
                repeat_ratio = len(set(match_idx.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out += repeat_ratio
                # print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                # tra_ptsB = torch.tensor(tra_ptsB).type(torch.FloatTensor)   # sift点
                tra_match_idx, mask_idx = get_point_pair_repeat(sift_warped_pts.squeeze(), sift_ptsB[:, :2], correspond=rdis)
                sift_repeat_ratio = len(set(tra_match_idx.numpy().tolist())) / (len(sift_warped_pts.squeeze()) + eps)

                sift_totalrepeat_out += sift_repeat_ratio
                # print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            sift_numA, sift_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator, denominator = len(set(match_idx.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            sift_numerator, sift_denominator = len(set(tra_match_idx.numpy().tolist())), len(sift_warped_pts.squeeze())
            content.append([nameA, nameB, repeat_ratio, numerator, denominator, numA, numB, sift_repeat_ratio, sift_numerator, sift_denominator, sift_numA, sift_numB])


        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'net repeat ratio',
                    'net_numerator',
                    'net_denominator',
                    'net_numA',
                    'net_numB',
                    'sift repeat ratio',
                    'sift_numerator',
                    'sift_denominator',
                    'sift_numA',
                    'sift_numB'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out / count, sift_totalrepeat_out / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass

'''输出原生SIFT标签'''
class Output_Sift(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']
        self.batch         = 5  # 进行5次光学变换提点

        self.output_dir     = Path(config['output_dir']) / 'output'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ load imagefiles ------
        names = []
        image_paths = []
        path_part = []

        for p in Path(img_path).iterdir():
            for s in p.rglob('*.bmp'):  
                # filename = '_'.join(str(s).split('/')[-5:])
                image_paths.append(str(s))
                names.append(str(s.name)[:-4])
                path_part.append('/'.join(str(s).split('/')[-3:-1]))    # 'train/pic'
        
        sequence_set = []
        for (image, N, P) in zip(image_paths, names, path_part):   # zip返回元组
            sample = {
                'img': image, 
                'name': N,
                'path_part': P
                }   # 无标签时，只有img数据
            sequence_set.append(sample)

        self.samples = sequence_set
        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        sample = self.samples[index]

        # img = imread(sample['img'])
        # img = img[:, 2:-2]
        # img = np.uint8(img)
        img_ori = load_as_float(sample['img'])
        img_ori = self.transforms(img_ori)
        
        # '''传统增强扩边处理-逆：36->32'''
        # img_ori = img_ori[0, :, 2:-2].unsqueeze(0)   # [1, 136, 32]
        img_ori = img_ori.unsqueeze(0)   # [1, 136, 32]
        H, W = img_ori.shape[1], img_ori.shape[2]

        from Model_component import imgPhotometric
        for n in range(self.batch):
            img = imgPhotometric(img_ori.squeeze().numpy(), **self.config)  # 进行batch次光学变换
            img = img.squeeze()
            if n == 0:
                img_batch = img[np.newaxis, np.newaxis, :, :]
            else:
                img_batch = np.concatenate((img_batch, img[np.newaxis, np.newaxis, :, :]), axis=0)
        img_batch = np.uint8(img_batch * 255)
        input  = {}
        input.update({
            'img_ori': img_ori,
            'img_batch': img_batch,  
            'name': sample['name'],
            'path_part': sample['path_part'],
            })
        return input

    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        tra_repeat_out = 0.
        content = []

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((idx + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img_batch, img_ori, name = sample["img_batch"], sample["img_ori"], sample['name']

            out_dir = self.output_dir / Path(sample['path_part'])
            os.makedirs(out_dir, exist_ok=True)
            # logging.info(f"name: {name}")

            pts_batch_list = []
            batch = img_batch.shape[0]
            for n in range(batch):  # 相同图做不同光学变换，提sift点
                img = img_batch[n, 0, :, :]
                '''sift提点'''
                sift = cv2.SIFT_create(contrastThreshold=1e-5, edgeThreshold=15, sigma=1.0)
                kp, des = sift.detectAndCompute(img, None)

                '''输出每张的sift点图'''
                img = cv2.drawKeypoints(img, kp, img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
                f = out_dir / (name + "_" + str(n) + ".bmp")
                cv2.imwrite(str(f), img)

                '''汇总sift点'''
                pts_sift = np.array([p.pt for p in kp])
                pts_sift = torch.tensor(pts_sift).float()
                pts_batch_list.extend(pts_sift)

            '''点去重'''
            temp_set = set(tuple(s.numpy().tolist()) for s in pts_batch_list)
            new_batch_list = [list(t) for t in temp_set]
            pts_batch = np.array(new_batch_list).round()
            pts_batch = pts_batch[:200, :]

            '''汇总sift点图'''
            img_ori = img_ori * 255
            img = np.repeat(cv2.resize(img_ori.numpy().squeeze(), None, fx=3, fy=3)[..., np.newaxis], 3, -1)
            for c in np.stack(pts_batch):
                cv2.circle(img, tuple((3 * c[:2]).astype(int)), radius=3, color=(0, 255, 0), thickness=-1)
            f = out_dir / (name + ".bmp")
            cv2.imwrite(str(f), img)

            '''save csv'''
            # pts_x, pts_y = pts_batch[:, 0].tolist(), pts_batch[:, 1].tolist()
            content = pts_batch.tolist()
            df = pd.DataFrame(content, columns=['x', 'y'])
            df.to_csv(os.path.join(out_dir, (name + '.csv')))
        
        pass

'''网络重复率/精度测试（增强数据）（T:/qint/data/toQt_Jy/）'''
class Test_Net_AccuracyandRepeat_Enhance(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.newLabels      = config['newLabels']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.homo_max_angle = config['augmentation']['homo']['params']['max_angle']
        self.homo_n_angles  = config['augmentation']['homo']['params']['n_angles']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        gallery_yml = {}
        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_rot_p20s80",
            # "6159_S1_p20s80",
            # "03_大数据库",
            "6159_cd_p11s400",
            "6159_limit_p20s80",
            "6159_p20s80",
        ]
        gallery_yml.update({"gallery": gallery})
        # ------ matched images ------
        names = []
        image_paths = []
        points_list = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            path = Path(img_path, select, 'val', 'pic')
            for p in Path(path).iterdir():
                # for s in p.rglob('*.npy'):
                if str(p)[-4:] == '.bmp':
                    filename = '_'.join(str(p).split('/')[-1:])
                    filename = select + '_' + filename
                    names.append(filename[:-4])
                    image_paths.append(str(p))
                    f = str(p).replace('pic/', 'csv/')
                    f = str(f).replace('.bmp', '.csv')
                    f = str(f).replace('enhance', 'point')
                    points_list.append(str(f))

        gallery2 = [
            # "2022_4_01_NewData/6159_normal0412_p20s80",
            # "2022_4_01_NewData/6159_normal2_0412_p20s80",
            # "2022_4_01_NewData/6159_p10s300",
            # "2022_4_01_NewData/6159_S1_p20s80",
            # "2022_4_01_NewData/6159_S2_p20s80",
            # "2022_4_01_NewData/6159_rot_p10s300",
            # "2022_4_01_NewData/6159_rot_p20s80",
        ]
        gallery_yml.update({"gallery2": gallery2})

        for select in gallery2:
            logging.info("add image {}.".format(select))
            path = Path(img_path, select, 'images', 'val')
            for p in Path(path).iterdir():
                # for s in p.rglob('*.npy'):
                if str(p)[-4:] == '.bmp':
                    filename = '_'.join(str(p).split('/')[-1:])
                    filename = select + '_' + filename
                    names.append(filename[:-4])
                    image_paths.append(str(p))
                    f = str(p).replace('images/', 'points/')
                    f = str(f).replace('.bmp', '.csv')
                    points_list.append(str(f))
        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names, 'points': points_list}
    
        sequence_set = []
        for (img, name, point) in zip(files['image_paths'], files['names'], files['points']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name, 'point': point}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        with open(os.path.join(config['output_dir'], "gallery.yml"), "w") as f:
            yaml.dump(gallery_yml, f, default_flow_style=False)

        pass

    def get_data(self, index):
        from Model_component import sample_homography_cv
        from Model_component import imgPhotometric

        img_o_A = load_as_float(self.samples[index]['image'])

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = img_aug.view(-1, H, W)

        '''warped image'''
        homography = sample_homography_cv(H, W, max_angle=self.homo_max_angle, n_angles=self.homo_n_angles)
        homography = torch.tensor(homography, dtype=torch.float32)
        
        warped_img = imgPhotometric(img_aug.squeeze().numpy(), **self.config)
        warped_img = inv_warp_image(torch.tensor(warped_img.squeeze()).type(torch.FloatTensor), homography, mode='bilinear').unsqueeze(0)
        valid_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homography)

        input  = {}
        input.update({'image': img_aug})
        input.update({'warped_image': warped_img})
        input.update({'valid_mask': valid_mask})
        input.update({'homography': homography})
        input.update({'point': self.samples[index]['point']})
        input.update({'name': self.samples[index]['name']})

        return input

    def test_process(self, FPDT):

        count = 0
        precision_out = 0.
        recall_out = 0.
        repeat_out = 0.
        content = []

        total_count = len(self.samples)
        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")

            sample = self.get_data(idx)
            img = sample["image"].unsqueeze(0)
            img_H, mask_2D = sample["warped_image"].unsqueeze(0), sample["valid_mask"]
            # img = img.transpose(0, 1)
            # mask_2D = mask_2D.transpose(0, 1)

            img, img_H, mask_2D = img.to(self.device), img_H.to(self.device), mask_2D.to(self.device)
            # sample = test_set[i]
            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            heatmap, _ = FPDT.run_heatmap(img)
            heatmap_H, _ = FPDT.run_heatmap(img_H)
            outputs = heatmap
            outputs_H = heatmap_H * mask_2D
            pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze().numpy(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)
            pts_H = getPtsFromHeatmap(outputs_H.detach().cpu().squeeze().numpy(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)

            '''截断150'''
            if self.top_k:
                if pts.shape[1] > self.top_k:
                    pts = pts[:, :self.top_k]
                if pts_H.shape[1] > self.top_k:
                    pts_H = pts_H[:, :self.top_k]
                
            H, W = img.shape[2], img.shape[3]   # img:[1,1,160,32]
            # from utils.utils import warp_points
            pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)
            pts_H = torch.tensor(pts_H).type(torch.FloatTensor).transpose(1, 0)
           
            '''Tradition labels--精度/召回'''
            '''传统扩边-逆:36->32'''
            '''136*36 -(cut)> 136*32'''
            if not self.newLabels:
                df = pd.read_csv(sample["point"], encoding = "gb2312", names=['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4'])
                pnts_x = df['x'].to_list()
                pnts_y = df['y'].to_list()
                pnts_x = np.array(pnts_x[1:]).astype(np.float32)
                pnts_y = np.array(pnts_y[1:]).astype(np.float32)
            else:
                df = pd.read_csv(sample["point"], names=["x", "y"])
                pnts_x = df['x'].to_list()
                pnts_y = df['y'].to_list()
                pnts_x = np.array(pnts_x).astype(np.float32)
                pnts_y = np.array(pnts_y).astype(np.float32)
            pnts = np.stack((pnts_x, pnts_y), axis=1)   #(x, y)
            # pnts_after_resize = pnts.copy()
            mask_pts = (pnts[:, 0] >= 2) * (pnts[:, 0] <= 33)
            pnts = pnts[mask_pts]
            pnts[:, 0 ] -= 2    # 36->32  cut 2 pixel
            pnts = torch.tensor(pnts).type(torch.FloatTensor)
            # pnts = torch.stack((pnts[:, 0], pnts[:, 1]), dim=1)  # (x, y)
            pts_label = filter_points(pnts, torch.tensor([W, H])) 
            '''截断150'''
            # if self.top_k:
            #     if pts_label.shape[0] > self.top_k:
            #         pts_label = pts_label[:self.top_k, :]

            # 网络点保持和sift数量一致
            pts = pts[:pts_label.shape[0], :]
            pts_H = pts_H[:pts_label.shape[0], :]

            '''网络复现率'''
            from Model_component import points_to_2D
            homo = sample["homography"]
            warped_pnts = warp_points(pts[:, :2], homo)
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # out_labels = points_to_2D(warped_pnts.squeeze().numpy(), H, W)  # warped_pnts: [u,v]图像坐标系 注意范围
            # out_labels = torch.tensor(out_labels).type(torch.FloatTensor).unsqueeze(0)
            
            if self.output_images:
                # img = sample['labels_2D'].transpose(2,3)
                '''获取标签坐标'''
                # label_2d = sample['labels_2D'].transpose(2, 3)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze()) # 得到坐标值
                
                img_pair = {}
                img_2D = img.cpu().numpy().squeeze()
                img_2D_H = img_H.cpu().numpy().squeeze()
                img_pair.update({'img_1': img_2D})
                img_pair.update({'img_2': img_2D_H})

                '''精度/召回'''
                from Model_component import draw_keypoints
                img_pts = draw_keypoints(img_2D * 255, pts.numpy(), pts_label.numpy())
                f = self.output_dir / (name + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''output a pair of graph'''
                from Model_component import draw_keypoints_pair
                pred = {}
                pred.update({"pts": pts})   # 图像坐标系
                pred.update({'pts_B': pts_H})
                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.squeeze())
                f = self.output_dir / (name + "_netoutput_pair.bmp")
                cv2.imwrite(str(f), img_pts)

            if self.output_ratio:
                '''precision & recall'''
                match_idx, mask_idx = get_point_pair_repeat(pts_label, pts[:, :2], correspond=self.nms)
                precision_ratio = len(set(match_idx.numpy().tolist())) / len(pts)
                recall_ratio = len(set(match_idx.numpy().tolist())) / len(pts_label)
                # print("repetitive rate:{:f} recall:{:f}".format(repeat_ratio, recall_ratio))
                precision_out += precision_ratio
                recall_out += recall_ratio

                '''net repetitive rate'''
                match_idx_rept, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_H[:, :2], correspond=self.nms)
                repeat_ratio = len(set(match_idx_rept.numpy().tolist())) / len(warped_pnts.squeeze())
                # recall_ratio = len(set(match_idx.numpy().tolist())) / len(pts_H.squeeze())
                repeat_out += repeat_ratio
            count += 1
            # if count == 200:
            #     break
            '''output .csv'''
            innerpoint, netpoint = len(set(match_idx.numpy().tolist())), len(pts.squeeze())    # 分子,分母
            innerpoint_rept, warped_netpoint_rept = len(set(match_idx_rept.numpy().tolist())), len(warped_pnts.squeeze())
            trapoint = len(pts_label.squeeze())
            content.append([name, precision_ratio, recall_ratio, innerpoint, netpoint, trapoint, repeat_ratio, innerpoint_rept, warped_netpoint_rept])
            
        df = pd.DataFrame(content, 
                        columns=[
                            'name',
                            'precision_ratio',
                            'recall_ratio',
                            'inner_num',
                            'net_num',
                            'tra_num',
                            'netrepeat_ratio',
                            'innernum_rept',
                            'warped_netnum_rept'
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            print("precision:{:f}  recall:{:f} repetitive-rate:{:f}".format(precision_out / count, recall_out / count, repeat_out / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass

'''C代码测试'''
class C_Test(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        names = []
        image_paths = []

        e = [str(p) for p in Path(img_path).iterdir()]
        n = [(p.split('/')[-1])[:-4] for p in e]
        image_paths.extend(e)
        names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names}
    
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        img_o_A = load_as_float(self.samples[index]['image'])

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = torch.tensor(img_aug, dtype=torch.float32).view(-1, H, W)

        input  = {}
        input.update({'image': img_aug})
        input.update({'name': self.samples[index]['name']})

        return input

    def test_process(self, FPDT):

        count = 0

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img = sample["image"].unsqueeze(0)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,32]

            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            heatmap = FPDT.run_heatmap(img.to(self.device)).detach().cpu()
            outputs = heatmap
            pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)

            '''截断150'''
            if pts.shape[1] > 150:
                pts = pts[:, :150]
    
            pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)
           
            pts[:, 0] += + 2

            if self.output_images:
                img_2D = img.cpu().numpy().squeeze()
                from Model_component import draw_keypoints
                img_pts = draw_keypoints(img_2D * 255, pts.numpy(), None)  # 增加了标签数据点
                f = self.output_dir / (name + ".bmp")
                cv2.imwrite(str(f), img_pts)

            content = pts.tolist()
            df = pd.DataFrame(content, columns=['x', 'y', 'score'])
            df.to_csv(os.path.join(self.output_dir, (name + '_point.csv')))

        pass

'''验证网络输出的标签'''
class Verify(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        names = []
        image_paths = []

        e = [str(p) for p in Path(img_path).iterdir()]
        n = [(p.split('/')[-1])[:-4] for p in e]
        image_paths.extend(e)
        names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names}
    
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        img_o_A = load_as_float(self.samples[index]['image'])

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = torch.tensor(img_aug, dtype=torch.float32).view(-1, H, W)

        input  = {}
        input.update({'image': img_aug})
        input.update({'name': self.samples[index]['name']})

        return input

    def test_process(self, FPDT):

        count = 0

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img = sample["image"].unsqueeze(0)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,32]

            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            heatmap = FPDT.run_heatmap(img.to(self.device))[0].detach().cpu()
            outputs = heatmap
            pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms)  # (x,y, prob)

            '''截断150'''
            # if self.top_k:
            #     if pts_A.shape[1] > self.top_k:
            #         pts_A = pts_A[:, :self.top_k]
            #     if pts_B.shape[1] > self.top_k:
            #         pts_B = pts_B[:, :self.top_k]

            pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)
           
            if self.output_images:
                img_2D = img.cpu().numpy().squeeze()
                from Model_component import draw_keypoints
                img_pts = draw_keypoints(img_2D * 255, pts.numpy(), None)  # 增加了标签数据点
                f = self.output_dir / (name + ".bmp")
                cv2.imwrite(str(f), img_pts)

        pass

'''描述子匹配验证'''
class Descriptor_Verify(object):
    '''ransac出内点，输出匹配关系'''
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        match_path = Path('/hdd/file-input/qint/data/6159_cd_p11s400_enhance_image/log.1217.pick.txt')
        point_path = Path('/hdd/file-input/qint/data/6159_cd_p11s400_enhance_image/log2.feature.point.txt')

        # ------ matched images read ------
        df = pd.read_csv(
                    match_path,
                    header=None,
                    encoding = "gb2312",
                    names=['enroll', 'verify', 'path', 
                        'H0', 'H1', 'H2', 'H3', 'H4', 'H5',
                        'score', 'up']
                    )    # 'gb2312'
        image1_list = df['enroll'].to_list()
        image1_list = [i[8:].rjust(3, '0') for i in image1_list]    # 获得名称并补齐 '1'-> '001'
        image1_list = [i + '_enhance' for i in image1_list]
        image2_list = df['path'].to_list()
        image2_list = [i[7:] for i in image2_list]
        H0 = df['H0'].to_list()
        H0 = [(int)(h[6:]) for h in H0]
        H1 = df['H1'].to_list()
        H2 = df['H2'].to_list()
        H3 = df['H3'].to_list()
        H4 = df['H4'].to_list()
        H5 = df['H5'].to_list()

        for i in range(len(image2_list)):
            suffix = '.bmp'
            path_share = '/'.join(image2_list[i].split('/')[:-1])
            image1_list[i] = path_share + '/' + image1_list[i] + suffix
        trans = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]    # combined into list
        # trans = np.array(trans) # (N, 6)

        image1_name = ['_'.join(k.split('/')[-3:])[:-4] for k in image1_list]
        image2_name = ['_'.join(k.split('/')[-3:])[:-4] for k in image2_list]

        # ------ images points read ------
        coor = []
        coor_group = []
        img_6159_path = []
        df_p = pd.read_csv(point_path, header=None, encoding="gb2312", sep='\t')    # 'gb2312'

        for line in zip(df_p[0]):  # line: [index, .]
            if line[0][1:5] == '/hdd':
                '''换成npy格式(DOG数据)'''
                # tmp = line[0]
                # tmp = tmp.replace('bmp', 'npy')
                # tmp = tmp.replace('6159_cd_p11s400', '6159_cd_p11s400_DOG')
                # img_6159_path.append(tmp[1:])

                img_6159_path.append(line[0][1:])

            if line[0][1] == 'x':
                ls = re.split('[,:]', line[0])
                coor.append(ls[1])    # 提取x,y
                coor.append(ls[3])

            if (line[0][1] == '/') and (len(coor) != 0):
                coor_group.append(coor)
                coor = []

        # ------ dic ------
        files1 = {
            'imgA': image2_list,    
            'imgB': image1_list,       # 模板
            'imgA_name': image2_name,
            'imgB_name': image1_name,
            'trans': trans
        }
        self.files2 = {
            'img_6159': img_6159_path,
            'pts': coor_group
        }
    
        sequence_set1 = []
        sequence_set2 = []
        for (A, N_A, B, N_B, M_H) in zip(
            files1['imgA'], files1['imgA_name'],
            files1['imgB'], files1['imgB_name'],files1['trans']):   # zip返回元组

            '''换成npy格式(DOG数据)'''
            # A = A.replace('bmp', 'npy')
            # A = A.replace('6159_cd_p11s400', '6159_cd_p11s400_DOG')
            # B = B.replace('bmp', 'npy')
            # B = B.replace('6159_cd_p11s400', '6159_cd_p11s400_DOG')

            '''小位移'''
            if (int(N_A[8:11]) >= 200) or (int(N_B[8:11]) >= 200):
                continue
            '''旋转'''
            # if (int(N_A[8:11]) > 200 and int(N_B[8:11]) > 200) or (int(N_A[8:11]) < 200 and int(N_B[8:11]) < 200):
            #     continue

            sample = {
                'imgA': A, 
                'imgA_name': N_A, 
                'imgB': B, 
                'imgB_name': N_B,
                'm_all': M_H
                }   # 无标签时，只有img数据
            sequence_set1.append(sample)
        # for (I_6159, pts) in zip(files2['img_6159'], files2['pts']):   # zip返回元组
        #     sample = {
        #         'img_6159': I_6159,
        #         'pts': pts
        #         }   # 无标签时，只有img数据
        #     sequence_set1.append(sample)

        self.samples1 = sequence_set1
        # self.samples2 = sequence_set2
        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        sample = self.samples1[index]
        # sample2 = self.samples2[index]  # img & traditional points
        imgA_path = sample['imgA']
        imgB_path = sample['imgB']
        homo = np.array(sample['m_all'])
        homo = torch.tensor(homo.reshape(2, 3), dtype=torch.float32)
        homo = 2 * homo / 512.
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo, vec_one), 0)

        if 1:
            img_o_A = load_as_float(sample['imgA'])
            img_o_B = load_as_float(sample['imgB'])
        else:
            img_o_A = np.load(sample['imgA'])
            img_o_B = np.load(sample['imgB'])
        # img_o = (img_o + 32768) / 65535.
            img_o_A = (img_o_A + 3500) / 7000.
            img_o_B = (img_o_B + 3500) / 7000.
            img_o_A[:3, :], img_o_B[:3, :] = 0., 0.
            img_o_A[:, :3], img_o_B[:, :3] = 0., 0.
            img_o_A[(img_o_A.shape[0] - 3):, :], img_o_B[(img_o_B.shape[0] - 3):, :] = 0., 0.
            img_o_A[:, (img_o_A.shape[1] - 3):], img_o_B[:, (img_o_B.shape[1] - 3):] = 0., 0.

        img_A_ori = self.transforms(img_o_A)
        img_B_ori = self.transforms(img_o_B)

        '''传统增强扩边处理-逆：36->32'''
        img_A = img_A_ori[0, :, 2:-2].unsqueeze(0)   # [1, 160, 32]
        img_B = img_B_ori[0, :, 2:-2].unsqueeze(0)

        H, W = img_A.shape[1], img_A.shape[2]

        img_A = torch.tensor(img_A, dtype=torch.float32).view(-1, H, W)
        img_B = torch.tensor(img_B, dtype=torch.float32).view(-1, H, W)
        imgA_warped = inv_warp_image(img_A.squeeze(), homo, mode='bilinear').unsqueeze(0)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        idxA = self.files2['img_6159'].index(imgA_path) # 获取同手指图在点坐标列表中的index
        idxB = self.files2['img_6159'].index(imgB_path)
        ptsA = np.array(self.files2['pts'][idxA], dtype=np.float32).reshape(-1, 2)    #(x, y)
        ptsB = np.array(self.files2['pts'][idxB], dtype=np.float32).reshape(-1, 2)
        ptsA = torch.tensor(ptsA).type(torch.FloatTensor)
        ptsB = torch.tensor(ptsB).type(torch.FloatTensor)

        input  = {}
        input.update({
            'imgA': img_A,  
            'imgB': img_B,  # 模板
            'imgA_warped': imgA_warped, # 非模板旋转
            'imgA_warped_mask': imgA_warped_mask,
            'imgA_name': sample['imgA_name'],
            'imgB_name': sample['imgB_name'],
            'H_AB': homo,
            'ptsA': ptsA,
            'ptsB': ptsB
            })

        return input

    def tensor_to_PIL(self, tensor):

        image = tensor.cpu().clone()
        image = image.squeeze()
        unloader = transforms.ToPILImage()
        image = unloader(image)
        return image

    def image_match(self, index_, imgA, imgB, ptsA, ptsB, descA, descB, nameA, nameB):
        """
        test whether the images matche each other according the key points.
        if the patches matched, connected the two key points by lines
        """
        from Match_component import compute_dists_fast
        from Match_component import ransac_twice

        imgA = self.tensor_to_PIL(imgA) # PIL : [32, 160]
        imgB = self.tensor_to_PIL(imgB)

        if imgA.size[0] == 32 and imgA.size[1] == 160:
            save_img = Image.new("RGB", (imgA.size[0] * 2, imgA.size[1]))
            save_img.paste(imgA, (0, 0))
            save_img.paste(imgB, (imgA.size[0], 0))

            # keypoints_left, keypoints_right, _, _ = sift_points(img_a,img_p)

            matched_left, matched_right, dist_list_return = compute_dists_fast(descA, descB, ptsA, ptsB, 0.2, dist_mod='cos')
            # print(matched_left)   
            # exit()
            image_path = [nameA, nameB, self.output_dir]
            contents = ransac_twice(image_path, imgA, imgB, save_img, index_, matched_left, matched_right, dist_list_return)
        
            imgA.close()
            imgB.close()
            
            return contents
    
    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        tra_repeat_out = 0.
        content = []
        match_content = []

        total_count = len(self.samples1)
        for idx in range(len(self.samples1)):
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            sample = self.get_data(idx)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['H_AB']
            # imgA, imgB= (
            #     imgA.to(self.device),
            #     imgB.to(self.device),
            # )
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['imgA_name']
            nameB = sample['imgB_name']
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # pass through network
            pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''截断150'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]
            
            from Match_component import sample_desc_from_points
            # pointsA = pts_A.transpose(1, 0)[:,[0,1]]    # (x, y)
            # pointsB = pts_B.transpose(1, 0)[:,[0,1]]
            desc_A = sample_desc_from_points(desc_A, pts_A) # (D, num)
            desc_B = sample_desc_from_points(desc_B, pts_B)
            desc_A = torch.tensor(desc_A.transpose(1, 0) , dtype=torch.float32)
            desc_B = torch.tensor(desc_B.transpose(1, 0) , dtype=torch.float32)

            # compute descriptor match
            re_content = self.image_match(idx, imgA, imgB, pts_A, pts_B, desc_A, desc_B, nameA, nameB)
            if re_content is not None:
                match_content.extend(re_content)

            # A,B点集
            H, W = imgA.shape[2], imgA.shape[3]   # img:[1,1,160,32]
            # from utils.utils import warp_points
            pts_A = torch.tensor(pts_A).type(torch.FloatTensor)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor)
            warped_pnts = warp_points(pts_A[:2, :].transpose(1, 0), mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, _ = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            # B_pnts_mask = points_to_2D(pts_B.cpu().squeeze().numpy().transpose(1, 0), H, W)
            # B_label = imgA_warped_mask.cpu().squeeze().numpy() * B_pnts_mask
            # pts_B = getPtsFromLabels2D(B_label)

            ## top K points
            pts_A = pts_A.transpose(1, 0).squeeze()
            pts_B = pts_B.transpose(1, 0).squeeze()
            # print("total net points: ", pts_A.shape)

            pred = {}
            pred.update({"pts": pts_A})   # 图像坐标系
            pred.update({'pts_B': pts_B})

            '''Tradition'''
            '''传统扩边-逆:36->32'''
            tra_ptsA, tra_ptsB = sample['ptsA'].squeeze(), sample['ptsB'].squeeze()
            mask_pts = (tra_ptsA[:, 0] >= 2) * (tra_ptsA[:, 0] <= 33)
            tra_ptsA = tra_ptsA[mask_pts]
            tra_ptsA[:, 0 ] -= 2    # 36->32  cut 2 pixel
            mask_pts = (tra_ptsB[:, 0] >= 2) * (tra_ptsB[:, 0] <= 33)
            tra_ptsB = tra_ptsB[mask_pts]
            tra_ptsB[:, 0 ] -= 2

            if self.top_k:
                if tra_ptsA.shape[0] > self.top_k:
                    tra_ptsA = tra_ptsA[:self.top_k, :]
                if tra_ptsB.shape[0] > self.top_k:
                    tra_ptsB = tra_ptsB[:self.top_k, :]
            
            tra_warped_pts = warp_points(tra_ptsA, mat_H)
            tra_warped_pts, _ = filter_points(tra_warped_pts.squeeze(), torch.tensor([W, H]), return_mask=True)  # imgB points

            # tra_heatmapB = getLabel2DFromPts(tra_ptsB, H, W)  # to mask B's points
            # tra_heatmapB = tra_heatmapB * imgA_warped_mask.squeeze()
            # tra_warped_pts_masked = getPtsFromLabels2D(tra_heatmapB).transpose(1, 0) # heatmap -> imgB points with mask

            pred.update({"tra_pts": tra_ptsA})
            pred.update({'tra_pts_B': tra_ptsB})   # (x, y, 1)

            ## output images for visualization labels
            if self.output_images:
                img_pair = {}
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_pair.update({'img_1': img_2D_A, 'img_2': img_2D_B})

                img_pts = draw_keypoints_pair(img_pair, pred, warped_pnts.numpy(), radius=1, s=1)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + ".bmp")
                cv2.imwrite(str(f), img_pts)

                '''Tradition'''
                tra_img_pts = draw_keypoints_pair_tradition(img_pair, pred, tra_warped_pts.numpy(), radius=1, s=1)
                f = self.output_dir / (nameAB + '_sift.bmp')
                cv2.imwrite(str(f), tra_img_pts)

                require_image = True
                if require_image:
                    img_2D_warped = sample['imgA_warped'].numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_merge_sift.bmp'), image_merge)

            if self.output_ratio:   
                to_floatTensor = lambda x: torch.tensor(x).type(torch.FloatTensor)
                eps = 1e-6
                '''NET'''
                pts_B = torch.tensor(pts_B, dtype=torch.float32)    # 预测点
                match_idx, mask_idx = get_point_pair_repeat(warped_pnts.squeeze(), pts_B[:, :2])
                repeat_ratio = len(set(match_idx.numpy().tolist())) / (len(warped_pnts.squeeze()) + eps)
                # recall_ratio = len(set(match_idx)) / len(pts_B.squeeze())
                repeat_out += repeat_ratio
                # print("repetitive rate:{:f}".format(repeat_ratio))

                '''Tradition'''
                tra_ptsB = torch.tensor(tra_ptsB, dtype=torch.float32)    # sift点
                tra_match_idx, mask_idx = get_point_pair_repeat(tra_warped_pts.squeeze(), tra_ptsB[:, :2])
                repeat_ratio_tra = len(set(tra_match_idx.numpy().tolist())) / (len(tra_warped_pts.squeeze()) + eps)

                tra_repeat_out += repeat_ratio_tra
                # print("traditional repetitive rate:{:f}".format(repeat_ratio_tra))

            '''output .csv'''
            numA, numB = pred['pts'].shape[0], pred['pts_B'].shape[0]
            tra_numA, tra_numB = pred['tra_pts'].shape[0], pred['tra_pts_B'].shape[0]
            numerator, denominator = len(set(match_idx.numpy().tolist())), len(warped_pnts.squeeze())    # 分子,分母
            tra_numerator, tra_denominator = len(set(tra_match_idx.numpy().tolist())), len(tra_warped_pts.squeeze())
            content.append([nameA, nameB, repeat_ratio, numerator, denominator, numA, numB, repeat_ratio_tra, tra_numerator, tra_denominator, tra_numA, tra_numB])

            count += 1
            if count == 50000:
                break
        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'net repeat ratio',
                    'numerator',
                    'denominator',
                    'numA',
                    'numB',
                    'traditional repeat ratio',
                    'tra_numerator',
                    'tra_denominator',
                    'tra_numA',
                    'tra_numB'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        df=pd.DataFrame(match_content, columns=['path1', 'path2', 'save_path', 
                        'matched_num', 'match_after_ransac_num', 'model_00', 'model_01', 'model_02', 'model_10', 'model_11', 'model_12'])
        df.to_csv(os.path.join(self.output_dir, 'match_results.csv'), index=False)

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            # print("repetitive rate:{:f} ".format(repeat_out / count))      # 均值
            print("net repetitive ratio:{:f}  traditional repetitive ratio:{:f}".format(repeat_out / count, tra_repeat_out / count))      # 均值
        else:
            print("the count is 0, please check the path of the labels!!! Or check the 'check_exist'!!!")

        pass

'''NMS与SOFT-NMS对比'''
class SOFT_NMS(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        names = []
        image_paths = []

        e = [str(p) for p in Path(img_path).iterdir()]
        n = [(p.split('/')[-1])[:-4] for p in e]
        image_paths.extend(e)
        names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names}
    
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        img_o_A = load_as_float(self.samples[index]['image'])

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = torch.tensor(img_aug, dtype=torch.float32).view(-1, H, W)

        input  = {}
        input.update({'image': img_aug})
        input.update({'name': self.samples[index]['name']})

        return input

    def test_process(self, FPDT):

        count = 0

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img = sample["image"].unsqueeze(0)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,32]

            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            heatmap = FPDT.run_heatmap(img.to(self.device)).detach().cpu()
            outputs = heatmap
            pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)
            pts_soft = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms, soft_nms=True)  # (x,y, prob)

            if pts.shape[1] > pts_soft.shape[1]:
                pts = pts[:, : pts_soft.shape[1]]
            if pts.shape[1] < pts_soft.shape[1]:
                pts_soft = pts_soft[:, : pts.shape[1]]
            '''截断150'''
            # if self.top_k:
            #     if pts_A.shape[1] > self.top_k:
            #         pts_A = pts_A[:, :self.top_k]
            #     if pts_B.shape[1] > self.top_k:
            #         pts_B = pts_B[:, :self.top_k]

            pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)
            pts_soft = torch.tensor(pts_soft).type(torch.FloatTensor).transpose(1, 0)

            pts_pair = {}
            pts_pair.update({"pts": pts, "pts_soft": pts_soft})
            if self.output_images:
                img_2D = img.cpu().numpy().squeeze()
                from Model_component import draw_keypoints_compareNMS
                img_pts = draw_keypoints_compareNMS(img_2D, pts_pair)  # 增加了标签数据点
                f = self.output_dir / (name + ".bmp")
                cv2.imwrite(str(f), img_pts)

        pass

'''网络输出标签（.csv）'''
class Output_label(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']
        self.batch         = 5  # 进行5次光学变换提点

        self.output_dir     = Path(config['output_dir']) / 'output'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ load imagefiles ------
        names = []
        image_paths = []
        path_part = []

        for p in Path(img_path).iterdir(): 
            # filename = '_'.join(str(s).split('/')[-5:])
            image_paths.append(str(p))
            names.append(str(p.name)[:-4])
            path_part.append('/'.join(str(p).split('/')[-3:-1]))    # 'train/pic'
        
        sequence_set = []
        for (image, N, P) in zip(image_paths, names, path_part):   # zip返回元组
            sample = {
                'img': image, 
                'name': N,
                'path_part': P
                }   # 无标签时，只有img数据
            sequence_set.append(sample)

        self.samples = sequence_set
        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        sample = self.samples[index]

        # img = imread(sample['img'])
        # img = img[:, 2:-2]
        # img = np.uint8(img)
        img_ori = load_as_float(sample['img'])
        img_ori = self.transforms(img_ori)
        
        # '''传统增强扩边处理-逆：36->32'''
        img_ori = img_ori[0, :, 2:-2].unsqueeze(0).unsqueeze(1)   # [1, 136, 32]
        
        input  = {}
        input.update({
            'img_ori': img_ori,
            'name': sample['name'],
            'path_part': sample['path_part'],
            })
        return input

    def test_process(self, FPDT):

        count = 0
        repeat_out = 0.
        tra_repeat_out = 0.
        content = []

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((idx + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img_ori, name = sample["img_ori"], sample['name']

            out_dir = self.output_dir / Path(sample['path_part'])
            os.makedirs(out_dir, exist_ok=True)
            # logging.info(f"name: {name}")

            heatmap = FPDT.run_heatmap(img_ori.to(self.device))
            outputs = heatmap
            pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze().numpy(), self.conf_thresh, self.nms, soft_nms=False)

            pts = pts.transpose(1, 0)
            mask = (pts[:, 0] >= 2) * (pts[:, 0] <= 29)
            pts = pts[mask]
            pts = pts[:150, :]
            # img_ori = img_ori * 255
            # img = np.repeat(cv2.resize(img_ori.numpy().squeeze(), None, fx=3, fy=3)[..., np.newaxis], 3, -1)
            # for c in np.stack(pts):
            #     cv2.circle(img, tuple((3 * c[:2]).astype(int)), radius=3, color=(0, 255, 0), thickness=-1)
            # f = out_dir / (name + ".bmp")
            # cv2.imwrite(str(f), img)

            '''save csv'''
            pts[:, 0] += 2
            # pts_x, pts_y = pts_batch[:, 0].tolist(), pts_batch[:, 1].tolist()
            content = pts[:, :2].tolist()
            df = pd.DataFrame(content, columns=['x', 'y'])
            df.to_csv(os.path.join(out_dir, (name + '.csv')))
        
        pass

'''读取外部标签并在图上标记'''
class Verify_Labels(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        names = []
        points = []
        image_paths = []

        e = [str(p) for p in Path(img_path).iterdir()]
        f = [p.replace("pic", "csv") for p in e]
        # f = [p.replace("enhance", "point") for p in f]
        f = [p.replace("bmp", "csv") for p in f]
        n = [(p.split('/')[-1])[:-4] for p in f]

        image_paths.extend(e)
        points.extend(f)
        names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'points': points, 'names': names}
    
        sequence_set = []
        for (img, point, name) in zip(files['image_paths'], files['points'], files['names']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'point': point, 'name': name}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        img_o_A = load_as_float(self.samples[index]['image'])
        # img = pd.read_csv(self.samples[index]['image'], header=None).to_numpy()
        # img_o_A = img.astype(np.float32) / 255

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = torch.tensor(img_aug, dtype=torch.float32).view(-1, H, W)

        "=====labels====="
        df = pd.read_csv(self.samples[index]["point"])
        pnts_x = df['x'].to_list()
        pnts_y = df['y'].to_list()
        pnts_x = np.array(pnts_x).astype(np.float32)
        pnts_y = np.array(pnts_y).astype(np.float32)
        pnts = np.stack((pnts_x, pnts_y), axis=1)   #(x, y)
        # pnts_after_resize = pnts.copy()
        mask_pts = (pnts[:, 0] >= 2) * (pnts[:, 0] <= 33)
        pnts = pnts[mask_pts]
        pnts[:, 0 ] -= 2    # 36->32  cut 2 pixel
        pnts = torch.tensor(pnts).float()
        # pnts = torch.stack((pnts[:, 0], pnts[:, 1]), dim=1)  # (x, y)
        pnts = filter_points(pnts, torch.tensor([W, H])) 
        input  = {}
        input.update({'image': img_aug})
        input.update({'point': pnts})
        input.update({'name': self.samples[index]['name']})

        return input

    def test_process(self, FPDT):

        count = 0

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img = sample["image"].unsqueeze(0)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,32]

            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            # heatmap = FPDT.run_heatmap(img.to(self.device)).detach().cpu()
            # outputs = heatmap
            # pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms)  # (x,y, prob)

            '''截断150'''
            # if self.top_k:
            #     if pts_A.shape[1] > self.top_k:
            #         pts_A = pts_A[:, :self.top_k]
            #     if pts_B.shape[1] > self.top_k:
            #         pts_B = pts_B[:, :self.top_k]
            
            pts = sample["point"]
            pts = torch.tensor(pts).type(torch.FloatTensor)
           
            if self.output_images:
                img_2D = img.cpu().numpy().squeeze()
                from Model_component import draw_keypoints
                img_pts = draw_keypoints(img_2D * 255, pts.numpy(), None, (255, 0, 0))  # 增加了标签数据点
                f = self.output_dir / (name + ".bmp")
                cv2.imwrite(str(f), img_pts)

        pass


'''描述子汉明距测试'''
class Descriptor_Hamming_Test(object):
    '''
    output:
        网络描述子与标签的汉明距均值
        网络描述子A与A_H匹配对的汉明均值
        网络描述子A与A_H非匹配对的汉明均值
    '''
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.newLabels      = config['newLabels']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.homo_max_angle = config['augmentation']['homo']['params']['max_angle']
        self.homo_n_angles  = config['augmentation']['homo']['params']['n_angles']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_rot_p20s80",
            # "6159_S1_p20s80",
            # "03_大数据库",
            "6159_cd_p11s400",
            "6159_limit_p20s80",
            "6159_p20s80",
        ]
        # ------ matched images ------
        names = []
        image_paths = []
        points_list = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            path = Path(img_path, select, 'val', 'pic')
            e = [str(p) for p in path.iterdir() if str(p)[-4:] == '.bmp']
            f = [p.replace("pic", "csv") for p in e]
            f = [p.replace(".bmp", ".csv") for p in f]
            n = [(p.split('/')[-1])[:-4] for p in f]

            image_paths.extend(e)
            points_list.extend(f)
            names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names, 'points': points_list}
    
        sequence_set = []
        for (img, name, point) in zip(files['image_paths'], files['names'], files['points']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name, 'point': point}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    def get_data(self, index):
        from Model_component import sample_homography_cv
        from Model_component import imgPhotometric

        input  = {}

        img_o_A = load_as_float(self.samples[index]['image'])

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = img_aug.view(-1, H, W)

        '''points & descriptor labels'''
        df = pd.read_csv(self.samples[index]["point"], encoding = "gb2312", names=['sort', 'x', 'y', 'desc1', 'desc2'])
        pnts_x = np.array(df['x'].to_list()[1:]).astype(np.int32)
        pnts_y = np.array(df['y'].to_list()[1:]).astype(np.int32)
        desc1_list = df['desc1'].to_list()[1:]
        desc2_list = df['desc2'].to_list()[1:]
        desc1 = np.array([np.fromiter(p, dtype=np.int) for p in desc1_list], dtype="float32")
        desc2 = np.array([np.fromiter(p[:-1], dtype=np.int) for p in desc2_list], dtype="float32")
        desc = np.concatenate((desc1, desc2), axis=1)
        # desc = np.array([np.fromiter(p, dtype=np.int) for p in desc1_list], dtype="float32")
        pnts = np.stack((pnts_x, pnts_y), axis=1)   #(x, y)
        mask_pts = (pnts[:, 0] >= 2) * (pnts[:, 0] <= 33)
        pnts = pnts[mask_pts]
        desc = desc[mask_pts]
        pnts[:, 0 ] -= 2    # 36->32  cut 2 pixel
        pnts = torch.tensor(pnts).float()
        desc = torch.tensor(desc).float()
        # pnts = torch.stack((pnts[:, 0], pnts[:, 1]), dim=1)  # (x, y)
        pnts, mask_filter = filter_points(pnts, torch.tensor([W, H]), return_mask=True)
        desc = desc[mask_filter]

        '''截断150'''
        cut = self.top_k
        pnts = pnts[:cut, :]
        desc = desc[:cut, :]
        
        # if desc.shape[0] < cut:
        #     zeros_mat = torch.zeros([cut - desc.shape[0], desc.shape[1]], dtype=torch.float)
        #     desc_fill = torch.cat((desc, zeros_mat), dim=0)  # 统一desc的维度
        #     input.update({"desc": desc_fill})
        # else:
        #     input.update({"desc": desc})

        '''warped image'''
        homography = sample_homography_cv(H, W, max_angle=self.homo_max_angle, n_angles=self.homo_n_angles)
        homography = torch.tensor(homography, dtype=torch.float32)
        
        warped_img = imgPhotometric(img_aug.squeeze().numpy(), **self.config)
        warped_img = inv_warp_image(torch.tensor(warped_img.squeeze()).type(torch.FloatTensor), homography, mode='bilinear').unsqueeze(0)
        valid_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homography)

        input.update({"desc": desc})
        input.update({'image': img_aug})
        input.update({"image_warped": warped_img})
        input.update({"valid_mask": valid_mask})
        input.update({'point': pnts})   # label points
        input.update({'name': self.samples[index]['name']})
        input.update({'homo': homography})

        return input

    def test_process(self, FPDT):

        count = 0
        precision_out = 0.
        recall_out = 0.
        repeat_out = 0.
        content = []
        total_hamming_net_label = 0.
        total_hamming_netAB_match = 0.
        total_hamming_netAB_no_match = 0.

        total_count = len(self.samples)
        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")

            sample = self.get_data(idx)
            img, img_B = sample["image"].unsqueeze(0), sample["image_warped"].unsqueeze(0)
            pts_label = sample["point"]
            desc_label = sample["desc"]
            mask_2D = sample["valid_mask"].to(self.device)
            mat_H = sample["homo"]
            # img = img.transpose(0, 1)
            # mask_2D = mask_2D.transpose(0, 1)

            img, img_B, pts_label, desc_label = img.to(self.device), img_B.to(self.device), pts_label.to(self.device), desc_label.to(self.device)
            # sample = test_set[i]
            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            pts, desc = FPDT.run_pts_desc(img, self.conf_thresh, self.nms)
            heatmap_B, desc_B = FPDT.run_heatmap(img_B)
            heatmap_B *= mask_2D.unsqueeze(0)
            desc_B *= mask_2D.unsqueeze(0)
            pts_B = getPtsFromHeatmap(heatmap_B.detach().cpu().numpy(), self.conf_thresh, self.nms, 0, soft_nms=False)

            H, W = img.shape[2], img.shape[3]   # img:[1,1,160,32]
            # from utils.utils import warp_points
            pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)
            pts_B = torch.tensor(pts_B).type(torch.FloatTensor).transpose(1, 0)
            warped_pnts = warp_points(pts[:, :2], mat_H)  # 利用变换矩阵变换坐标点
            warped_pnts, warped_pnts_mask = filter_points(warped_pnts.squeeze(), torch.tensor([W, H]), return_mask=True)
            '''截断150'''
            if self.top_k:
                if pts.shape[1] > self.top_k:
                    pts = pts[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]

            ########
            '''Descriptor corresponding to label_points (Net) <-> label_descriptor (Label) ---> Hamming'''
            # select the descriptors from the label position
            desc_select = desc[0, :, pts_label[:, 1].long(), pts_label[:, 0].long()].transpose(1, 0)
            '''Hamming'''
            desc_select = torch.sigmoid(desc_select)
            desc_select_hard = torch.where(desc_select > 0.5, torch.ones_like(desc_select), torch.zeros_like(desc_select))

            # dist_list = []
            # for (e1, e2) in zip(desc_select_hard, desc_label):  # 网络与标签的Hamming
            #     dist_list.append(sum(e1 != e2))
            dist_list = torch.sum(desc_select_hard != desc_label, dim=-1)
            dist_mean = sum(dist_list) / (dist_list.shape[0] + 1e-6)
            print("net&label_dist_mean {:d}:{:f}".format(idx, dist_mean))
            # hanmingdist = sum([e1!=e2 for (e1, e2) in zip(desc_select_hard, desc_label)])

            ########
            '''Calculate Hamming of A & B (descriptor corresponding to net_points_A and net_points_B)'''
            desc_pts_select = desc[0, :, pts[:, 1].long(), pts[:, 0].long()].transpose(1, 0)
            desc_pts_B_select = desc_B[0, :, pts_B[:, 1].long(), pts_B[:, 0].long()].transpose(1, 0)
            desc_pts_select = torch.sigmoid(desc_pts_select)
            desc_pts_B_select = torch.sigmoid(desc_pts_B_select)
            desc_pts_select_hard = torch.where(desc_pts_select > 0.5, torch.ones_like(desc_pts_select), torch.zeros_like(desc_pts_select))
            desc_pts_B_select_hard = torch.where(desc_pts_B_select > 0.5, torch.ones_like(desc_pts_B_select), torch.zeros_like(desc_pts_B_select))

            match_idx_rept, mask_idx = get_point_pair_repeat(warped_pnts[:, :2], pts_B[:, :2], correspond=self.nms)
            desc_pts_select_hard = desc_pts_select_hard[warped_pnts_mask]
            desc_matched = desc_pts_select_hard[mask_idx]
            desc_B_matched = desc_pts_B_select_hard[match_idx_rept, :]

            # dist_list = []
            # for (e1, e2) in zip(desc_matched, desc_B_matched):
            #     dist_list.append(sum(e1 != e2))
            dist_list = torch.sum(desc_matched != desc_B_matched, dim=-1)
            net_match_dist_mean = sum(dist_list) / (dist_list.shape[0] + 1e-6)
            print("netA&AH_match_dist_mean {:d}:{:f}".format(idx, net_match_dist_mean))

            ########
            '''非匹配点对汉明距测试 (net_pointsA 与 net_pointsB)'''
            # from Model_component import get_point_pair_inverse
            # no_match_idx, no_mask_idx = get_point_pair_inverse(pts[:, :2], pts_B[:, :2], correspond=self.nms)
            # desc_no_matched = desc_pts_select_hard[no_mask_idx]
            # descB_no_matched = desc_pts_B_select_hard[no_match_idx]
            from Model_component import get_point_pair_inverse_all
            coor = get_point_pair_inverse_all(warped_pnts[:, :2], pts_B[:, :2], correspond=self.nms)
            desc_no_matched = desc_pts_select_hard[coor[0]]
            descB_no_matched = desc_pts_B_select_hard[coor[1]]

            dist_list = torch.sum(desc_no_matched != descB_no_matched, dim=-1)
            net_no_match_dist_mean = sum(dist_list) / (dist_list.shape[0] + 1e-6)
            print("netA&AH_no_match_dist_mean {:d}:{:f}".format(idx, net_no_match_dist_mean))

            total_hamming_net_label += dist_mean
            total_hamming_netAB_match += net_match_dist_mean
            total_hamming_netAB_no_match += net_no_match_dist_mean
            count += 1
            # if count == 200:
            #     break
            # '''output .csv'''
            # innerpoint, netpoint = len(set(match_idx.numpy().tolist())), len(pts.squeeze())    # 分子,分母
            # innerpoint_rept, warped_netpoint_rept = len(set(match_idx_rept.numpy().tolist())), len(warped_pnts.squeeze())
            # trapoint = len(pts_label.squeeze())
            content.append([name, dist_mean.detach().cpu().numpy(), net_match_dist_mean.detach().cpu().numpy(), net_no_match_dist_mean.detach().cpu().numpy()])
            
        df = pd.DataFrame(content, 
                        columns=[
                            'name',
                            'dist_mean',
                            'net_match_dist_mean',
                            'net_no_match_dist_mean',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

        if count:   # 防止count为0
            print("output pseudo ground truth: ", count)
            print("Net-label_Hamming_dis_mean:{:f} ".format(total_hamming_net_label / count))    # 均值
            print("Net-AB_match_Hamming_dis_mean:{:f} ".format(total_hamming_netAB_match / count))    # 均值
            print("Net-AB_no_match_Hamming_dis_mean:{:f} ".format(total_hamming_netAB_no_match / count))    # 均值
        else:
            print("the count is 0, please check the path of the labels!!!")

        pass

'''识别FR分析'''
class Succandfail_Analyse(object):
    '''
    读取succ/fail文件（包含匹配对路径）
    输出成功匹配对图像
    '''
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.newLabels      = config['newLabels']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.homo_max_angle = config['augmentation']['homo']['params']['max_angle']
        self.homo_n_angles  = config['augmentation']['homo']['params']['n_angles']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        sift_succ_path = Path('/hdd/file-input/qint/data/toQt_Jy/6159_p21s600_data/NET_Trans/')
        sift_fail_path = Path('/hdd/file-input/qint/data/toQt_Jy/6159_p21s600_data/NET_Trans/')

        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_rot_p20s80",
            # "6159_S1_p20s80",
            # "03_大数据库",
            "6159_cd_p11s400",
            "6159_limit_p20s80",
            "6159_p20s80",
        ]
        # ------ matched images ------
        names = []
        image_paths = []
        points_list = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            path = Path(img_path, select, 'val', 'pic')
            e = [str(p) for p in path.iterdir() if str(p)[-4:] == '.bmp']
            f = [p.replace("pic", "csv") for p in e]
            f = [p.replace(".bmp", ".csv") for p in f]
            n = [(p.split('/')[-1])[:-4] for p in f]

            image_paths.extend(e)
            points_list.extend(f)
            names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names, 'points': points_list}
    
        sequence_set = []
        for (img, name, point) in zip(files['image_paths'], files['names'], files['points']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name, 'point': point}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

'''HeatMap可视化'''
class HeatMap_Visualization(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        names = []
        image_paths = []

        e = [str(p) for p in Path(img_path).iterdir()]
        n = [(p.split('/')[-1])[:-4] for p in e]
        image_paths.extend(e)
        names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names}
    
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        img_o_A = load_as_float(self.samples[index]['image'])

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = torch.tensor(img_aug, dtype=torch.float32).view(-1, H, W)

        input  = {}
        input.update({'image': img_aug})
        input.update({'name': self.samples[index]['name']})

        return input

    def test_process(self, FPDT):

        count = 0

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img = sample["image"].unsqueeze(0)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,32]

            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            heatmap = FPDT.run_heatmap(img.to(self.device))[0].detach().cpu()
            outputs = heatmap 
            # print(heatmap)   

            if self.output_images:
                f = self.output_dir / (name + "_heatmap.bmp")
                vutils.save_image(outputs, str(f), normalize=True)
        pass

'''验证自监督网络输出的标签'''
class Verify_Self(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']
        self.w_size         = config['w_size']

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        print("load dataset_files from: ", img_path)
        # ------ matched images ------
        names = []
        image_paths = []

        e = [str(p) for p in Path(img_path).iterdir()]
        n = [(p.split('/')[-1])[:-4] for p in e]
        image_paths.extend(e)
        names.extend(n)

        # ------ dic ------
        files = {'image_paths': image_paths, 'names': names}
    
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):   # zip返回元组 （成对的bmp&npz）
            sample = {'image': img, 'name': name}   # 无标签时，只有img数据
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    # def __len__(self):
    #     return len(self.samples)

    def get_data(self, index):

        img_o_A = load_as_float(self.samples[index]['image'])

        '''传统增强扩边处理-逆：36->32'''  
        img_o_A = self.transforms(img_o_A)
        img_aug = img_o_A[:, :, 2:-2]      # [1, 136, 32]

        H, W = img_aug.shape[1], img_aug.shape[2]
        img_aug = torch.tensor(img_aug, dtype=torch.float32).view(-1, H, W)

        input  = {}
        input.update({'image': img_aug})
        input.update({'name': self.samples[index]['name']})

        return input

    def test_process(self, FPDT):

        count = 0

        for idx in range(len(self.samples)):
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            img = sample["image"].unsqueeze(0)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,32]

            name = sample["name"]
            name = name
            # logging.info(f"name: {name}")

            '''NET'''
            # pass through network
            heatmap = FPDT.run_heatmap(img.to(self.device))[0]
            outputs = heatmap
            pts = getPtsFromHeatmapByCoordinates(outputs, self.conf_thresh, self.w_size, bord=0)

            '''截断150'''
            # if self.top_k:
            #     if pts_A.shape[1] > self.top_k:
            #         pts_A = pts_A[:, :self.top_k]
            #     if pts_B.shape[1] > self.top_k:
            #         pts_B = pts_B[:, :self.top_k]

            pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)
           
            if self.output_images:
                img_2D = img.cpu().numpy().squeeze()
                from Model_component import draw_keypoints
                img_pts = draw_keypoints(img_2D * 255, pts.numpy(), None)  # 增加了标签数据点
                f = self.output_dir / (name + ".bmp")
                cv2.imwrite(str(f), img_pts)

        pass


class Distribution_Match_VS_Nomatch(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------
        sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
        self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = torch.ones((des_a.shape[0], des_b.shape[0]), device=self.device) * des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_wht(self, des_a, des_b):
        wht = WHT.WalshHadamardTransform()
        wht_desc_a, wht_desc_b = wht.transform(des_a), wht.transform(des_b)
        desc_binary_a = torch.where(wht_desc_a > 0.2, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0.2, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist


    def get_match_point_pair(self, dis, dis_thre=-1):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre

        match_mask = torch.zeros_like(dis, device=self.device)    

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        b2a_min_id = torch.argmin(dis, dim=0)  # N X 1
        mid_mask = b2a_min_id[a2b_min_id]

        # 双向最近邻
        len_p = len(a2b_min_id)
        len_a = len(b2a_min_id)
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_bs = torch.tensor(list(range(len_a)), device=self.device)
        mask = reshape_as == mid_mask
        a_indexes = reshape_as[mask]
        b_indexes = a2b_min_id[mask]
        match_mask[reshape_as, a2b_min_id] = 1
        match_mask[b2a_min_id, reshape_bs] = 1

        ch = dis[a_indexes, b_indexes] < correspond
        a_s = a_indexes[ch]
        b_s = b_indexes[ch]
        d_k = dis[a_s, b_s]
     
        return a_s, b_s, d_k, match_mask

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]
     
        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        return dis_max, dis_min, dis_mean

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def test_process(self, FPDT):
        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = True
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']    # 136x36 trans
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # pass through network
            pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''截断150'''
            if self.top_k:
                if pts_A.shape[1] > self.top_k:
                    pts_A = pts_A[:, :self.top_k]
                if pts_B.shape[1] > self.top_k:
                    pts_B = pts_B[:, :self.top_k]
            
            from Match_component import sample_desc_from_points
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)
            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)
            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32 -> 36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
            warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
            pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis)  # p -> k
            assert pos_repeatA_mask.shape[0] > 0 

            net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
            net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
            net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
            net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
            net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
            net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]

            net_pointsA_norm = pointsA / pointsA.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
            net_pointsB_norm = pointsB / pointsB.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1

            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)

            net_pos_nnA = pointsA_o[net_match_indicesA, :]
            net_pos_nnB = pointsB_o[net_match_indicesB, :]
            # if not self.isDilation:
            #     net_pos_nnA[:, 0] += 2
            #     net_pos_nnB[:, 0] += 2
            pred = {}
            pred.update({
                "pts": pointsA_o.detach().cpu().numpy(),
                "pts_H": pointsB_o.detach().cpu().numpy(),
                "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                "pts_nnB": net_pos_nnB.detach().cpu().numpy()
            })   # 图像坐标系
            img_pair = {}
            img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
            img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
            # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
            f = self.output_dir / (nameAB + "_net_line.bmp")
            cv2.imwrite(str(f), img_pts)

            # 画Net配准图
            if require_image:
                Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)


            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()
            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis)  # p -> k
            assert sift_pos_repeatA_mask.shape[0] > 0 

            sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
            sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            sift_ptsA_now = copy.deepcopy(sift_ptsA)
            sift_ptsB_now = copy.deepcopy(sift_ptsB)
            if not self.isDilation:
                sift_ptsA_now[:, 0] -= 2
                sift_ptsB_now[:, 0] -= 2
            sift_ptsA_norm = sift_ptsA_now / sift_ptsA_now.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
            sift_ptsB_norm = sift_ptsB_now / sift_ptsB_now.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
            siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis(sift_ptsA_norm, sift_ptsB_norm, desc_A.unsqueeze(0), desc_B.unsqueeze(0), sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            # 画Sift配准图
            if require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_match_mask == 1], sift_hanmingdist_AtoB[sift_match_mask == 0]
            
            if idx == 0:
                net_match_hanming_dis_all = net_match_hanming_dis
                net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        content = []
        net_match_max, net_match_min, net_match_mean = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match')
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch')
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])

        net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
        net_match_ones = torch.ones_like(net_match_hanming_dis_all)
        net_match_num = net_match_hanming_dis_all.shape[0]
        net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
            net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
            net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])
            
        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Match_VS_Nomatch_ALikeWithHard(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 300
        self.th2            = 300
        self.isrot          = True
        self.isrot91        = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_enhance_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * 10000) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * 10000) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        return dis_max, dis_min, dis_mean

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = False
        remove_border_r = 2
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)


            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)  
                    
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                if pos_repeatA_mask.shape[0] > 0: 
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                    net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                    net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_match_mask, self.th1, self.th2)

                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)
                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)

                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                # if not self.isDilation:
                #     net_pos_nnA[:, 0] += 2
                #     net_pos_nnB[:, 0] += 2
                if require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    f = self.output_dir / (nameAB + "_net_line.bmp")
                    cv2.imwrite(str(f), img_pts)

                # 画Net配准图
                if require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            

            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            if self.isdense:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                if self.has45:
                    siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                else:
                    siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                    siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

            else:
                siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            # 画Sift配准图
            if require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                    sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                    sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                    sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                    sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                    sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1 * 2,  self.th2 * 2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_match_mask == 1], sift_hanmingdist_AtoB[sift_match_mask == 0]
            
            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        content = []
        net_match_max, net_match_min, net_match_mean = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Match_VS_Nomatch_ALikeWithHard_Ext(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 300
        self.th2            = 300
        self.isrot          = True
        self.isrot91        = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6_7024/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * 10000) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * 10000) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = False
        remove_border_r = 2
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_t2s(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)


            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)  
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        # desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        # desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                if pos_repeatA_mask.shape[0] > 0: 
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                    net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                    net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_match_mask, self.th1, self.th2)

                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)
                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)

                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                # if not self.isDilation:
                #     net_pos_nnA[:, 0] += 2
                #     net_pos_nnB[:, 0] += 2
                if require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    f = self.output_dir / (nameAB + "_net_line.bmp")
                    cv2.imwrite(str(f), img_pts)

                # 画Net配准图
                if require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            

            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            if self.isdense:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                if self.has45:
                    # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                    siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                else:
                    siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                    siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

            else:
                siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            # 画Sift配准图
            if require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                    sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                    sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                    sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                    sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                    sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1 * 2,  self.th2 * 2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_match_mask == 1], sift_hanmingdist_AtoB[sift_match_mask == 0]
            
            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Descriptor_Hamming_Test_V3(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32
        self.isrot          = False
        self.outcsv         = True
        self.output_dir     = Path(config['output_dir']) / 'val'
        
        self.templet_num    = 14
        self.num_person     = '0002' # '0002'
        self.max_number     = 10000
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------

        # normal lib
        sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')  
        sift_total_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/')
        self.succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600') 
        self.total_dict, self.sequence_dict, self.total_set, self.finger_rank = self.Decode_total_file(sift_total_path, '6159_p21s600')   
        self.desc_path = '/hdd/file-input/linwc/Descriptor/data/repeat/desc_small_patch_98400_28_hadama_0002/' 

        # # rot lib 
        # sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')  
        # sift_total_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/') 
        # self.succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data') 
        # self.total_dict, self.sequence_dict, self.total_set, self.finger_rank = self.Decode_total_file_rot(sift_total_path, 'img_enhance_data') 
        # self.desc_path = '/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/desc_smaller_patch_98400_28_hadama_0000_thr/' 
        
        pass

    def Decode_total_file(self, path, pts_desc_dir):
        img_path = Path(path, pts_desc_dir)
        # pts_path = Path()

        im = [str(p) for p in img_path.iterdir() if str(p)[-4:] == '.csv']
        p_d = [str(p).replace('6159_p21s600/', 'SIFT_Point_6159_p21s600/') for p in im]
        p_d = [str(p).replace('_enhance', '') for p in p_d]
        names = [str(p).split('/')[-1][:-4] for p in p_d]

        n_p = [str(p).split('_')[0] for p in names]  # 人       0001
        n_f = [str(p).split('_')[1] for p in names]  # 手指      L2
        n_n = [str(p).split('_')[2] for p in names]  # 张数序号 0223

        files = {
            'img': im,
            'pts_desc': p_d,
            'name': names
        }
        sequence_list = []
        for (img, pts_desc, name) in zip(
            files['img'],
            files['pts_desc'],
            files['name']
            ):
            sample = {'img': img, 'pts_desc': pts_desc, 'name': name}
            sequence_list.append(sample)

        '''
        e.g.  total_dict
        |--0015
            |--L2
                |--[{'nn': 0001, 'img': xx, 'pts_desc': xx, 'name': xx}]
                |--[{'nn': 0002, 'img': xx, 'pts_desc': xx, 'name': xx}]
        '''
        sequence_dict = {}
        print("The index is being built now...")
        for (np, nf, nn, img, pts_desc, name) in zip(n_p, n_f, n_n, files['img'], files['pts_desc'], files['name']):
            if sequence_dict.get(np) == None:
                # total_dict[str(np)][str(nf)][str(nn)] = [ ]
                sequence_dict.update({np: {nf: [ ]}})
            elif sequence_dict.get(np).get(nf) == None:
                sequence_dict[np].update({nf: [ ]})
            # elif total_dict.get(np).get(nf).get(nn) == None:
            #     total_dict[np][nf].update({nn: [ ]})
            # total_dict[str(np)][str(nf)].append()

            nn_info = {'nn': nn, 'img': img, 'pts_desc': pts_desc, 'name': name}
            sequence_dict[np][nf].append(nn_info)
        
        sequence_dict_p = sequence_dict[self.num_person]
        sequence_set = []
        finger_rank = {}
        f_step = 6
        # finger_index_dict = {'L1': 0, 'L2': 1, 'L3': 2, 'R1': 3, 'R2': 4, 'R3': 5}
        for temp_f in sequence_dict_p.keys(): 
            keys_all = list(sequence_dict_p.keys())
            keys_all.pop(keys_all.index(temp_f))
            for sample_f in keys_all:
                # print(sample_f)
                for sample in sequence_dict_p[sample_f]:
                    # print(int(sample['nn']))
                    if int(sample['nn']) % f_step != 0:
                        continue
                    temp_count = 0
                    for temp in sequence_dict_p[temp_f]:
                        if temp_count >= self.templet_num:
                            break
                        temp_count += 1
                        pair = {'img_vf': sample['img'], 'img_en': temp['img'], 'pts_vf': sample['pts_desc'], 'pts_en': temp['pts_desc'], 'name_vf': sample['name'], 'name_en': temp['name'], 'homo': None, 'NorS': 'S', 'SorF': 'F'}
                        sequence_set.append(pair)
        
        keys_p = list(sequence_dict_p.keys())
        keys_p.sort()
        for temp_f in keys_p: 
            keys_all = list(sequence_dict_p.keys())
            keys_all.pop(keys_all.index(temp_f))
            keys_all_expand = [self.num_person + '_' + sf for sf in keys_all]
            finger_rank.update({self.num_person + '_' + temp_f: keys_all_expand})
        print('Finger Rank:')
        print(finger_rank)

        return sequence_list, sequence_dict, sequence_set, finger_rank
    
    def Decode_total_file_rot(self, path, pts_desc_dir):
        # img_path = path + pts_desc_dir
        img_path = Path(path, pts_desc_dir)
        
        all_files_path = []
        for n1, _, n3 in os.walk(str(img_path)):
            if 'L' in n1 or 'R' in n1:
                all_files_path.extend([n1 + '/' + fn for fn in n3])
        all_files_path.sort()

        im = [str(p) for p in all_files_path if str(p)[-4:] == '.bmp']
        p_d = [str(p).replace('img_enhance_data/', 'pnt_desc_data/') for p in im]
        p_d = [str(p).replace('_enhance', '') for p in p_d]
        names = [(p.split('/')[-3] + '_' + p.split('/')[-2] + '_' + (p.split('/')[-1])[:-4].replace('_enhance', '')) for p in p_d]
        # names = [str(p).split('/')[-1][:-4] for p in p_d]

        n_p = [str(p).split('_')[0] for p in names]  # 人       0001
        n_f = [str(p).split('_')[1] for p in names]  # 手指      L2
        n_n = [str(p).split('_')[2] for p in names]  # 张数序号 0223

        files = {
            'img': im,
            'pts_desc': p_d,
            'name': names
        }
        sequence_list = []
        for (img, pts_desc, name) in zip(
            files['img'],
            files['pts_desc'],
            files['name']
            ):
            sample = {'img': img, 'pts_desc': pts_desc, 'name': name}
            sequence_list.append(sample)

        '''
        e.g.  total_dict
        |--0015
            |--L2
                |--[{'nn': 0001, 'img': xx, 'pts_desc': xx, 'name': xx}]
                |--[{'nn': 0002, 'img': xx, 'pts_desc': xx, 'name': xx}]
        '''
        sequence_dict = {}
        print("The index is being built now...")
        for (np, nf, nn, img, pts_desc, name) in zip(n_p, n_f, n_n, files['img'], files['pts_desc'], files['name']):
            if sequence_dict.get(np) == None:
                # total_dict[str(np)][str(nf)][str(nn)] = [ ]
                sequence_dict.update({np: {nf: [ ]}})
            elif sequence_dict.get(np).get(nf) == None:
                sequence_dict[np].update({nf: [ ]})
            # elif total_dict.get(np).get(nf).get(nn) == None:
            #     total_dict[np][nf].update({nn: [ ]})
            # total_dict[str(np)][str(nf)].append()

            nn_info = {'nn': nn, 'img': img, 'pts_desc': pts_desc, 'name': name}
            sequence_dict[np][nf].append(nn_info)
        
        sequence_dict_p = sequence_dict[self.num_person]
        sequence_set = []
        finger_rank = {}
        f_step = 6
        # finger_index_dict = {'L1': 0, 'L2': 1, 'L3': 2, 'R1': 3, 'R2': 4, 'R3': 5}
        for temp_f in sequence_dict_p.keys(): 
            keys_all = list(sequence_dict_p.keys())
            keys_all.pop(keys_all.index(temp_f))
            for sample_f in keys_all:
                # print(sample_f)
                for sample in sequence_dict_p[sample_f]:
                    # print(int(sample['nn']))
                    if int(sample['nn']) % f_step != 0:
                        continue
                    temp_count = 0
                    for temp in sequence_dict_p[temp_f]:
                        if temp_count >= self.templet_num:
                            break
                        temp_count += 1
                        pair = {'img_vf': sample['img'], 'img_en': temp['img'], 'pts_vf': sample['pts_desc'], 'pts_en': temp['pts_desc'], 'name_vf': sample['name'], 'name_en': temp['name'], 'homo': None, 'NorS': 'S', 'SorF': 'F'}
                        sequence_set.append(pair)
        
        keys_p = list(sequence_dict_p.keys())
        keys_p.sort()
        for temp_f in keys_p: 
            keys_all = list(sequence_dict_p.keys())
            keys_all.pop(keys_all.index(temp_f))
            keys_all_expand = [self.num_person + '_' + sf for sf in keys_all]
            finger_rank.update({self.num_person + '_' + temp_f: keys_all_expand})
        print('Finger Rank:')
        print(finger_rank)

        return sequence_list, sequence_dict, sequence_set, finger_rank

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_enhance_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def pts_desc_decode(self, path):
        name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, names=name)
        pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
        pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
        pts_ori = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
        pts = copy.deepcopy(pts_ori)
        # if self.isDilation:
        #     pts[:, 0] += 2    # 36->32  cut 2 pixel
        #     mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
        # else:
        #     pts[:, 0] -= 2
        #     mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] <= 33)
        # pts = pts[mask_pts]
        desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
        desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
        desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
        desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        return pts_ori, pts, desc_front, desc_back

    def Extract_Total_Csv(self, index, samples):
        img = pd.read_csv(samples[index]['img'], header=None).to_numpy()
        img_o = img.astype(np.float32) / 255.
        img_o = img_o[:, :36]
        img_o = self.transforms(img_o)
        if self.isDilation:
            import torch.nn.functional as F
            img = F.pad(img_o, (2, 2), 'constant')     # 36 -> 40    constant/reflect
        else:
            img = img_o[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
        H, W = img.shape[1], img.shape[2]

        pts_ori, pts, desc_front, desc_back = self.pts_desc_decode(samples[index]['pts_desc'])

        name = samples[index]['name'][:-4]

        input  = {}
        input.update({
            'img': img,
            'img_o': img_o,
            'pts': pts,
            'pts_ori': pts_ori,
            'desc_front': desc_front,
            'desc_back': desc_back,
            'name': name,
            'homo': None,
        })

        return input

    def Extract_Succ_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_ori_vf, pts_vf, desc_front_vf, desc_back_vf = self.pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_ori_en, pts_en, desc_front_en, desc_back_en = self.pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': pts_ori_vf, 'pts_ori_en': pts_ori_en})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) / 1e4
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        # if self.isDilation:
        #     import torch.nn.functional as F
        #     imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
        #     imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
        #     imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': img_o_A, 'imgB': img_o_B, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        desc_binary_double_a = torch.cat((desc_binary_a, desc_binary_a), dim=-1)        # N X 256
        desc_binary_double_b = torch.cat((desc_binary_b, desc_binary_b), dim=-1)
        hanming_dist = desc_binary_double_a.shape[1] - desc_binary_double_a @ desc_binary_double_b.t() - (1 - desc_binary_double_a) @ (1 - desc_binary_double_b.t())
        return hanming_dist
    
    def get_des_hanmingdis_wht_big(self, des_a, des_b, phase='sift'):
        if phase != 'sift':
            hadama_trans = torch.tensor(hadamard(des_a.shape[-1]), device=des_a.device).float()
            wht_desc_a, wht_desc_b = torch.einsum('and,de->ane', des_a, hadama_trans), torch.einsum('bnd,de->bne', des_b, hadama_trans)
            desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
            desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        else:
            desc_binary_a, desc_binary_b = des_a, des_b
        hanming_dist = des_a.shape[-1] - torch.einsum('and,bmd->anbm', desc_binary_a, desc_binary_b) - torch.einsum('and,bmd->anbm', 1-desc_binary_a, 1-desc_binary_b)
        return hanming_dist
    
    def get_des_hanmingdis_wht_big_rot(self, des_a, des_b, phase='siftnet'):
        if phase == 'sift':
            index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        else:
            index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        half_dim = des_a.shape[-1] // 2 
        # if phase != 'sift':
        #     hadama_trans = torch.tensor(hadamard(des_a.shape[-1]), device=des_a.device).float()
        #     wht_desc_a, wht_desc_b = torch.einsum('and,de->ane', des_a, hadama_trans), torch.einsum('bnd,de->bne', des_b, hadama_trans)
        #     desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        #     desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        # else:
        desc_binary_a, desc_binary_b = des_a, des_b
        hanming_dist_part_begin = half_dim - torch.einsum('and,bmd->anbm', desc_binary_a[:, :, index], desc_binary_b[:, :, index]) - torch.einsum('and,bmd->anbm', 1 - desc_binary_a[:, :, index], 1 - desc_binary_b[:, :, index])   
        hanming_dist_part_end = half_dim - torch.einsum('and,bmd->anbm', desc_binary_a[:, :, index==False], desc_binary_b[:, :, index==False]) - torch.einsum('and,bmd->anbm', 1 - desc_binary_a[:, :, index==False], 1 - desc_binary_b[:, :, index==False])
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        hanming_dist = hanming_dist_part_begin + hanming_dist_part_end_min
        # hanming_dist = des_a.shape[-1] - torch.einsum('and,bmd->anbm', desc_binary_a, desc_binary_b) - torch.einsum('and,bmd->anbm', 1-desc_binary_a, 1-desc_binary_b)
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        # desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        # desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        desc_binary_a, desc_binary_b = des_a, des_b
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_net(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        desc_binary_double_a = torch.cat((desc_binary_a, desc_binary_a), dim=-1)        # N X 256
        desc_binary_double_b = torch.cat((desc_binary_b, desc_binary_b), dim=-1)
        # desc_binary_a, desc_binary_b = des_a, des_b
        hanming_dist = desc_binary_double_a.shape[1] - desc_binary_double_a @ desc_binary_double_b.t() - (1 - desc_binary_double_a) @ (1 - desc_binary_double_b.t())
        return hanming_dist

    def get_match_point_pair(self, dis, dis_thre=-1):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre

        match_mask = torch.zeros_like(dis, device=self.device)    

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        b2a_min_id = torch.argmin(dis, dim=0)  # N X 1
        mid_mask = b2a_min_id[a2b_min_id]

        # 双向最近邻
        len_p = len(a2b_min_id)
        len_a = len(b2a_min_id)
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_bs = torch.tensor(list(range(len_a)), device=self.device)
        mask = reshape_as == mid_mask
        a_indexes = reshape_as[mask]
        b_indexes = a2b_min_id[mask]
        match_mask[reshape_as, a2b_min_id] = 1
        match_mask[b2a_min_id, reshape_bs] = 1

        ch = dis[a_indexes, b_indexes] < correspond
        a_s = a_indexes[ch]
        b_s = b_indexes[ch]
        d_k = dis[a_s, b_s]
     
        return a_s, b_s, d_k, match_mask

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis_net(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        hadamadist_AtoB = self.get_des_hanmingdis_wht((desA + 1)/2, (desB + 1)/2)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hadamadist_AtoB, dis_thre=257)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_des_hanmingdis_wht_permute(self, des_a, des_b, phase='siftnet'):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b)) # Mxdim
        if phase == 'sift':
            index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        else:
            index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, index] @ desc_binary_b[:, index].t() - (1 - desc_binary_a[:, index]) @ (1 - desc_binary_b[:, index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, index==False] @ desc_binary_b[:, index==False].t() - (1 - desc_binary_a[:, index==False]) @ (1 - desc_binary_b[:, index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min

    def get_match_nomatch_dis_nosampler_hadama_rot(self, desA, desB, repeat_mask, thr1=257, thr2=257):
        hanming_dist_part_begin, hanming_dist_part_end = self.get_des_hanmingdis_wht_permute((desA + 1)/2, (desB + 1)/2)    # N X M
        hadamadist_AtoB = hanming_dist_part_begin + hanming_dist_part_end
        candiate_begin_mask = hanming_dist_part_begin < thr1
        # candiate_all_mask = hadamadist_AtoB < thr2

        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hadamadist_AtoB, dis_thre=thr2)      # <= thr1:30

        return match_indicesA, match_indicesB, 2 * hadamadist_AtoB[torch.logical_and(repeat_mask == 1, candiate_begin_mask)], 2 * hadamadist_AtoB[torch.logical_and(repeat_mask == 0, candiate_begin_mask)]

    def get_match_nomatch_dis_nosampler_hadama_sift(self, desA, desB, repeat_mask):
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA[:, :128], desB[:, :128])
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hadamadist_AtoB, dis_thre=257)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]
    
    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.unsqueeze(-1).expand_as(descs)
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm))
        return descs

    def get_match_nomatch_dis_nosampler_hadama_sift_rot(self, desA, desB, repeat_mask):
        descA_th = self.thresholding_desc(desA[:, :128])
        descB_th = self.thresholding_desc(desB[:, :128])
        hanming_dist_part_begin, hanming_dist_part_end = self.get_des_hanmingdis_wht_permute(descA_th, descB_th, phase='sift')
        hadamadist_AtoB = hanming_dist_part_begin + hanming_dist_part_end
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hadamadist_AtoB, dis_thre=257)      
        return match_indicesA, match_indicesB, 2 * hadamadist_AtoB[repeat_mask == 1], 2 * hadamadist_AtoB[repeat_mask == 0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]
     
        return a_s, b_s, d_k

    def cal_max_min_mean(self, hist_all, phase):
        indexes = torch.tensor(list(range(hist_all.shape[0])), device=self.device)
        dis_max = indexes[hist_all > 0][-1].detach().cpu().numpy()
        dis_min = indexes[hist_all > 0][0].detach().cpu().numpy()
        dis_mean = (torch.sum(hist_all*indexes, dim=-1) / torch.sum(hist_all, dim=-1)).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        return dis_max, dis_min, dis_mean

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bord, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
        toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def get_hist_by_npy(self, temp_path, sample_path, phase='siftnet'):
        if phase == 'sift':
            temp_all_desc = torch.tensor(np.load(temp_path + '_sift_desc.npy')[:self.templet_num, :, :]).to(self.device)       # 14x150xd
            sample_all_desc = torch.tensor(np.load(sample_path + '_sift_desc.npy')).to(self.device)                              # nx150xd
        else:
            temp_all_desc = torch.tensor(np.load(temp_path + '_desc.npy')[:self.templet_num, :, :]).to(self.device)       # 14x150xd
            sample_all_desc = torch.tensor(np.load(sample_path + '_desc.npy')).to(self.device)                              # nx150xd
        temp_mask = torch.tensor(np.load(temp_path + '_mask.npy')[:self.templet_num, :, :]).to(self.device)     # 14x150x1
        sample_mask = torch.tensor(np.load(sample_path + '_mask.npy')).to(self.device)                           # nx150x1
        temp_sam_sim = self.get_des_hanmingdis_wht_big(temp_all_desc, sample_all_desc)         # 14x150xnx150 
        temp_sam_mask = torch.einsum('and,bmd->anbm', temp_mask, sample_mask)                   # 14x150xnx150
        valid_sim = temp_sam_sim[temp_sam_mask==1]                 # 14 x va x n x vb
        # print(valid_sim.shape)
        valid_sim_hist = torch.histc(valid_sim, bins=257, min=0, max=256).int() / self.max_number 
        return valid_sim_hist

    def get_hist_by_npy_rot(self, temp_path, sample_path, phase='siftnet'):
        if phase == 'sift':
            temp_all_desc = torch.tensor(np.load(temp_path + '_sift_desc.npy')[:self.templet_num, :, :]).to(self.device)       # 14x150xd
            sample_all_desc = torch.tensor(np.load(sample_path + '_sift_desc.npy')).to(self.device)                              # nx150xd
        else:
            temp_all_desc = torch.tensor(np.load(temp_path + '_desc.npy')[:self.templet_num, :, :]).to(self.device)       # 14x150xd
            sample_all_desc = torch.tensor(np.load(sample_path + '_desc.npy')).to(self.device)                              # nx150xd
        temp_mask = torch.tensor(np.load(temp_path + '_mask.npy')[:self.templet_num, :, :]).to(self.device)     # 14x150x1
        sample_mask = torch.tensor(np.load(sample_path + '_mask.npy')).to(self.device)                           # nx150x1
        temp_sam_sim = self.get_des_hanmingdis_wht_big_rot(temp_all_desc[:, :, :128], sample_all_desc[:, :, :128], phase)         # 14x150xnx150 
        temp_sam_mask = torch.einsum('and,bmd->anbm', temp_mask, sample_mask)                   # 14x150xnx150
        valid_sim = 2 * temp_sam_sim[temp_sam_mask==1]                 # 14 x va x n x vb
        # print(valid_sim.shape)
        valid_sim_hist = torch.histc(valid_sim, bins=257, min=0, max=256).int() / self.max_number 
        return valid_sim_hist

    def test_process(self, FPDT):
        count = 0
        import datetime

        # starttime = datetime.datetime.now()
        # print("start time is {}".format(starttime))
        quantize = True
        repeat_dis = 2
        print("quantize switch:", quantize)
        print("nms:{} dis<{}".format(self.nms, repeat_dis))
        require_image = False
        remove_border_r = 0
        # net_desc_dim = 128
        # max_numnber = 1
        
        succ_count = len(self.succ_dict) 
        # total_count = len(self.total_set) 
        # print(total_count, succ_count)

        # Match Pairs
        print("Begin Match Pairs:")
        for idx in range(succ_count):
            count += 1
            print("Progress:{0}%".format(round((count + 1) * 100 / succ_count)), end="\r")
            # print("it: ", count)
            if idx < succ_count:
                sample = self.Extract_Succ_Csv(idx, self.succ_dict) if not self.isrot else self.Extract_Succ_Csv_rot(idx, self.succ_dict)
            else:
                sample = self.Extract_Succ_Csv(idx - succ_count, self.total_set)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            # '''Tradition'''
            # pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            # from Match_component import sample_desc_from_points
            # pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            # pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            # pointsA_o = copy.deepcopy(pointsA)
            # pointsB_o = copy.deepcopy(pointsB)
            # '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                if not self.isrot:
                    W_o += 4            # 32->36
                # pointsA_o[:, 0] += 2
                # pointsB_o[:, 0] += 2
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                if not self.isrot:
                    W_o -= 4    # 40->36
                # mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                # pointsA_o = pointsA_o[mask_ptsA, :]
                # mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                # pointsB_o = pointsB_o[mask_ptsB, :]
                # pointsA_o[:, 0] -= 2
                # pointsB_o[:, 0] -= 2
            # # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            # if remove_border_r > 0:
            #     pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, W_o, H_o)
            #     pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, W_o, H_o)
            
            # '''截断150'''
            # if self.top_k:
            #     if pointsA_o.shape[0] > self.top_k:
            #         pointsA_o = pointsA_o[:self.top_k, :]
            #     if pointsB_o.shape[0] > self.top_k:
            #         pointsB_o = pointsB_o[:self.top_k, :]

            # desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            # desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            # net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
            # if mat_H is not None:
            #     warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            #     warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            #     key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
            #     pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis)  # p -> k

            #     if pos_repeatA_mask.shape[0] > 0: 
            #         net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
            #         net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
            #         net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
            #         net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
            #         net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]

            # # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
            # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)

            # net_match_hanming_hist = torch.histc(net_match_hanming_dis, bins=net_desc_dim+1, min=0, max=net_desc_dim) if net_match_hanming_dis is not None else torch.zeros(net_desc_dim+1, device=self.device).float()
            # net_nomatch_hanming_hist = torch.histc(net_nomatch_hanming_dis, bins=net_desc_dim+1, min=0, max=net_desc_dim)

            # net_pos_nnA = pointsA_o[net_match_indicesA, :]
            # net_pos_nnB = pointsB_o[net_match_indicesB, :]
            # # if not self.isDilation:
            # #     net_pos_nnA[:, 0] += 2
            # #     net_pos_nnB[:, 0] += 2
            # if require_image:
            #     pred = {}
            #     pred.update({
            #         "pts": pointsA_o.detach().cpu().numpy(),
            #         "pts_H": pointsB_o.detach().cpu().numpy(),
            #         "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
            #         "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
            #         "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
            #         "pts_nnB": net_pos_nnB.detach().cpu().numpy()
            #     })   # 图像坐标系
            #     img_pair = {}
            #     img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
            #     img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
            #     # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
            #     f = self.output_dir / (nameAB + "_net_line.bmp")
            #     cv2.imwrite(str(f), img_pts)

            # # 画Net配准图
            # if require_image:
            #     Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
            #     if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
            #         imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
            #         img_2D_A = imgA.numpy().squeeze()
            #         img_2D_B = imgB.numpy().squeeze()
            #         img_2D_warped = imgA_o_warped.numpy().squeeze()
            #         b = np.zeros_like(img_2D_A)
            #         g = img_2D_warped * 255  #  旋转后的模板
            #         r = img_2D_B * 255    
            #         image_merge = cv2.merge([b, g, r])
            #         image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
            #         cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if remove_border_r > 0:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if mat_H is not None:
                warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
                sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis)  # p -> k

                if sift_pos_repeatA_mask.shape[0] > 0:
                    sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                    sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                    sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
            siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            if self.isrot:
                _, _, siftnet_match_hanming_dis, _ = self.get_match_nomatch_dis_nosampler_hadama_rot(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            else:
                _, _, siftnet_match_hanming_dis, _ = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            siftnet_match_hanming_hist = (torch.histc(siftnet_match_hanming_dis, bins=257, min=0, max=256).int() / self.max_number) if siftnet_match_hanming_dis is not None else torch.zeros(257, device=self.device).float() 
            # siftnet_nomatch_hanming_hist = (torch.histc(siftnet_nomatch_hanming_dis, bins=net_desc_dim+1, min=0, max=net_desc_dim).int() / max_numnber) if mat_H is not None else torch.zeros(net_desc_dim+1, device=self.device).float() 
            
            # siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            # siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            # # 画Sift配准图
            # if require_image:
            #     Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
            #     if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
            #         imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
            #         img_2D_A = imgA.numpy().squeeze()
            #         img_2D_B = imgB.numpy().squeeze()
            #         img_2D_warped = imgA_o_warped.numpy().squeeze()
            #         b = np.zeros_like(img_2D_A)
            #         g = img_2D_warped * 255  #  旋转后的模板
            #         r = img_2D_B * 255    
            #         image_merge = cv2.merge([b, g, r])
            #         image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
            #         cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if remove_border_r > 0:
                sift_descA = sift_descA[borderA, :]
                sift_descB = sift_descB[borderB, :]
            if self.isrot:
                 _, _, sift_match_hanming_dis, _ = self.get_match_nomatch_dis_nosampler_hadama_sift_rot(sift_descA, sift_descB, sift_match_mask)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
                # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
                sift_match_hanming_dis = sift_hanmingdist_AtoB[sift_match_mask == 1]

            sift_match_hanming_hist = (torch.histc(sift_match_hanming_dis, bins=257, min=0, max=256).int() / self.max_number) if sift_match_hanming_dis is not None else torch.zeros(257, device=self.device)
            # sift_nomatch_hanming_hist = (torch.histc(sift_nomatch_hanming_dis, bins=257, min=0, max=256).int() / max_numnber) if mat_H is not None else torch.zeros(257, device=self.device)
            
            if idx == 0:
                # net_match_hanming_hist_all = net_match_hanming_hist
                # net_nomatch_hanming_hist_all = net_nomatch_hanming_hist
                siftnet_match_hanming_hist_all = siftnet_match_hanming_hist
                # siftnet_nomatch_hanming_hist_all = siftnet_nomatch_hanming_hist
                sift_match_hanming_hist_all = sift_match_hanming_hist
                # sift_nomatch_hanming_hist_all = sift_nomatch_hanming_hist
            else:
                # net_match_hanming_hist_all = net_match_hanming_hist_all + net_match_hanming_hist
                # net_nomatch_hanming_hist_all = net_nomatch_hanming_hist_all + net_nomatch_hanming_hist
                siftnet_match_hanming_hist_all = siftnet_match_hanming_hist_all + siftnet_match_hanming_hist
                # siftnet_nomatch_hanming_hist_all = siftnet_nomatch_hanming_hist_all + siftnet_nomatch_hanming_hist
                sift_match_hanming_hist_all = sift_match_hanming_hist_all + sift_match_hanming_hist
                # sift_nomatch_hanming_hist_all = sift_nomatch_hanming_hist_all + sift_nomatch_hanming_hist

        # Nomatch Pairs
        print('Begin Nomatch Pairs:')
        siftnet_nomatch_hanming_hist_all = torch.zeros(257, device=self.device)
        sift_nomatch_hanming_hist_all = torch.zeros(257, device=self.device) 
        for temp_f in self.finger_rank:
            for sample_f in self.finger_rank[temp_f]:
                print(temp_f + '->' + sample_f)
                temp_path = self.desc_path + temp_f
                sample_path = self.desc_path + sample_f
                if self.isrot:
                    sift_nomatch_hanming_hist = self.get_hist_by_npy_rot(temp_path, sample_path, 'sift')
                    siftnet_nomatch_hanming_hist = self.get_hist_by_npy_rot(temp_path, sample_path, 'siftnet')
                else:
                    sift_nomatch_hanming_hist = self.get_hist_by_npy(temp_path, sample_path, 'sift')
                    siftnet_nomatch_hanming_hist = self.get_hist_by_npy(temp_path, sample_path, 'siftnet')
                siftnet_nomatch_hanming_hist_all += siftnet_nomatch_hanming_hist
                sift_nomatch_hanming_hist_all += sift_nomatch_hanming_hist

        # net_indexes = torch.tensor(list(range(net_desc_dim + 1)), device=self.device)
        sift_indexes = torch.tensor(list(range(sift_nomatch_hanming_hist_all.shape[0])), device=self.device)
        # net_nomatch_hanming_hist_norm = net_nomatch_hanming_hist_all / torch.sum(net_nomatch_hanming_hist_all, dim=-1)
        # net_nomatch_hanming_hist_norm_cum = torch.cumsum(net_nomatch_hanming_hist_norm, dim=-1)
        
        # eps = 0
        sift_match_hanming_hist_norm = sift_match_hanming_hist_all / torch.sum(sift_match_hanming_hist_all, dim=-1) 
        sift_nomatch_hanming_hist_norm = sift_nomatch_hanming_hist_all / torch.sum(sift_nomatch_hanming_hist_all, dim=-1)  #  1 x 128
        sift_nomatch_hanming_hist_norm_cum = torch.cumsum(sift_nomatch_hanming_hist_norm, dim=-1)
        
        siftnet_match_hanming_hist_norm = siftnet_match_hanming_hist_all / torch.sum(siftnet_match_hanming_hist_all, dim=-1) 
        siftnet_nomatch_hanming_hist_norm = siftnet_nomatch_hanming_hist_all / torch.sum(siftnet_nomatch_hanming_hist_all, dim=-1) 
        # print(self.max_number * torch.sum(sift_match_hanming_hist_all, dim=-1), self.max_number * torch.sum(sift_nomatch_hanming_hist_all, dim=-1))
        # siftnet_match_hanming_hist_norm_expand = torch.zeros_like(sift_match_hanming_hist_norm)
        # siftnet_nomatch_hanming_hist_norm_expand = torch.zeros_like(sift_nomatch_hanming_hist_norm)
        # siftnet_match_hanming_hist_norm_expand[torch.linspace(0, 256, 257) % 2 == 0] = copy.deepcopy(siftnet_match_hanming_hist_norm)
        # siftnet_nomatch_hanming_hist_norm_expand[torch.linspace(0, 256, 257) % 2 == 0] = copy.deepcopy(siftnet_nomatch_hanming_hist_norm)
        
        siftnet_nomatch_hanming_hist_norm_cum = torch.cumsum(siftnet_nomatch_hanming_hist_norm, dim=-1)
        # print(siftnet_nomatch_hanming_hist_norm_cum)

        if self.outcsv:
            bins_series = torch.tensor(np.array(range(257)), device=siftnet_match_hanming_hist_norm.device).detach().unsqueeze(0)
            # density distribution
            hist_match = siftnet_match_hanming_hist_norm.detach().unsqueeze(0)
            hist_no_match = siftnet_nomatch_hanming_hist_norm.detach().unsqueeze(0)
            hist_sift_match = sift_match_hanming_hist_norm.detach().unsqueeze(0)
            hist_sift_no_match = sift_nomatch_hanming_hist_norm.detach().unsqueeze(0)
            # cumsum distribution
            hist_match_cumsum = torch.cumsum(siftnet_match_hanming_hist_norm, dim=-1).detach().unsqueeze(0)
            hist_no_match_cumsum = siftnet_nomatch_hanming_hist_norm_cum.detach().unsqueeze(0)
            hist_sift_match_cumsum = torch.cumsum(sift_match_hanming_hist_norm, dim=-1).detach().unsqueeze(0)
            hist_sift_no_match_cumsum = sift_nomatch_hanming_hist_norm_cum.detach().unsqueeze(0)
            content_hist = torch.cat((bins_series, hist_match, hist_no_match, hist_sift_match, hist_sift_no_match, hist_match_cumsum, hist_no_match_cumsum, hist_sift_match_cumsum, hist_sift_no_match_cumsum), dim=0).transpose(0, 1).cpu().numpy().tolist()
            df_h = pd.DataFrame(content_hist, 
                                columns=[
                                    'bins',
                                    # 'net_nomatch_thres',
                                    # 'net_match',
                                    'hist_match',
                                    'hist_no_match',
                                    'hist_sift_match',
                                    'hist_sift_no_match',
                                    'hist_match_cumsum',
                                    'hist_no_match_cumsum',
                                    'hist_sift_match_cumsum',
                                    'hist_sift_no_match_cumsum',
                                    ])
            # df_h = pd.DataFrame([
            #     bins_series,
            #     hist_match,
            #     hist_no_match,
            #     hist_sift_match,
            #     hist_sift_no_match],
            #     index=[
            #         'bins',
            #         'hist_match',
            #         'hist_no_match',
            #         'hist_sift_match',
            #         'hist_sift_no_match'
            #     ]
            # )
            df_h.to_csv(os.path.join(self.output_dir, 'Distance_dis' + str(repeat_dis) + '_Hist.csv'))

        content = []
        # net_match_max, net_match_min, net_match_mean = self.cal_max_min_mean(net_match_hanming_hist_all, phase='net_match')
        # print('-------------------------------')
        # net_nomatch_max, net_nomatch_min, net_nomatch_mean = self.cal_max_min_mean(net_nomatch_hanming_hist_all, phase='net_nomatch')
        # print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean = self.cal_max_min_mean(sift_match_hanming_hist_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean = self.cal_max_min_mean(sift_nomatch_hanming_hist_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean = self.cal_max_min_mean(siftnet_match_hanming_hist_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean = self.cal_max_min_mean(siftnet_nomatch_hanming_hist_all, phase='siftnet_nomatch')

        # content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        # content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        # content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])

        content.append(['Max', sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])

        begin = 5
        stop = 95
        step = 5
        for ratio in range(begin, stop + step, step):
            # net_current_thres = net_indexes[net_nomatch_hanming_hist_norm_cum >= (ratio / 100.)][0].long()
            # net_match_ratio = torch.sum(net_match_hanming_hist_all[:net_current_thres], dim=-1) / torch.sum(net_match_hanming_hist_all, dim=-1) * 100

            sift_current_thres = sift_indexes[sift_nomatch_hanming_hist_norm_cum >= round(ratio / 100., 5)][0].long()
            sift_match_ratio = torch.sum(sift_match_hanming_hist_all[:sift_current_thres], dim=-1) / torch.sum(sift_match_hanming_hist_all, dim=-1) * 100

            siftnet_current_thres = sift_indexes[siftnet_nomatch_hanming_hist_norm_cum >= round(ratio / 100., 5)][0].long()
            siftnet_match_ratio = torch.sum(siftnet_match_hanming_hist_all[:siftnet_current_thres], dim=-1) / torch.sum(siftnet_match_hanming_hist_all, dim=-1) * 100
    
            content.append([ratio, 
                            # net_current_thres.detach().cpu().numpy(), 
                            # net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])


        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            # 'net_nomatch_thres',
                            # 'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results_dis.csv'))


class Thres_Ratio_Match_VS_Nomatch_ALikeWithHard(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()

        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        # ------ matched images ------
        sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
        self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a)).int()
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b)).int()
        hanming_dist = torch.ones((des_a.shape[0], des_b.shape[0]), device=self.device) * des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, repeat_mask): 
        # # 计算重拍后的thr1
        # desc_binary_a = copy.deepcopy(des_a)
        # desc_binary_b = copy.deepcopy(des_b)
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))     # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # print(torch.sum(index.int()))
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)

        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())
        return hanming_dist_begin[repeat_mask == 1], hanming_dist_begin[repeat_mask == 0]

    def get_des_hanmingdis_thr(self, des_a, des_b, repeat_mask, thr=60):
        # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        candiate_begin_mask = hanming_dist_begin < thr
        return hanming_dist[torch.logical_and(repeat_mask == 1, candiate_begin_mask)], hanming_dist[torch.logical_and(repeat_mask == 0, candiate_begin_mask)]

    def get_match_point_pair(self, dis, dis_thre=-1):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre

        match_mask = torch.zeros_like(dis, device=self.device)    

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        b2a_min_id = torch.argmin(dis, dim=0)  # N X 1
        mid_mask = b2a_min_id[a2b_min_id]

        # 双向最近邻
        len_p = len(a2b_min_id)
        len_a = len(b2a_min_id)
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_bs = torch.tensor(list(range(len_a)), device=self.device)
        mask = reshape_as == mid_mask
        a_indexes = reshape_as[mask]
        b_indexes = a2b_min_id[mask]
        match_mask[reshape_as, a2b_min_id] = 1
        match_mask[b2a_min_id, reshape_bs] = 1

        ch = dis[a_indexes, b_indexes] < correspond
        a_s = a_indexes[ch]
        b_s = b_indexes[ch]
        d_k = dis[a_s, b_s]
     
        return a_s, b_s, d_k, match_mask

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        hadamadist_AtoB = self.get_des_hanmingdis_wht((desA + 1)/2, (desB + 1)/2)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, thr1=35, thr2=129):
        # hanming_dist_part_begin, hanming_dist_part_end = self.get_des_hanmingdis_wht_permute((desA + 1)/2, (desB + 1)/2)    # N X M
        hanming_dist_part_begin, hanming_dist_part_end = self.get_des_hanmingdis_wht_permute(desA + 0.5, desB + 0.5)
        hadamadist_AtoB = hanming_dist_part_begin + hanming_dist_part_end
        # candiate_begin_mask = hanming_dist_part_begin < thr1
        # candiate_all_mask = hadamadist_AtoB < thr2

        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hadamadist_AtoB, dis_thre=thr2)      # <= thr1:30

        return match_indicesA, match_indicesB, hanming_dist_part_begin[repeat_mask == 1], hanming_dist_part_begin[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_thr(self, desA, desB, repeat_mask, thr1=35, thr2=129):
        # hanming_dist_part_begin, hanming_dist_part_end = self.get_des_hanmingdis_wht_permute((desA + 1)/2, (desB + 1)/2)    # N X M
        hanming_dist_part_begin, hanming_dist_part_end = self.get_des_hanmingdis_wht_permute(desA + 0.5, desB + 0.5)
        hadamadist_AtoB = hanming_dist_part_begin + hanming_dist_part_end
        candiate_begin_mask = hanming_dist_part_begin < thr1
        # candiate_all_mask = hadamadist_AtoB < thr2

        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hadamadist_AtoB, dis_thre=thr2)      # <= thr1:30

        return match_indicesA, match_indicesB, hadamadist_AtoB[torch.logical_and(repeat_mask == 1, candiate_begin_mask)], hadamadist_AtoB[torch.logical_and(repeat_mask == 0, candiate_begin_mask)]


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]
     
        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        return dis_max, dis_min, dis_mean

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bord, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
        toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def test_process(self, FPDT):
        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = False
        remove_border_r = 2
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            # print(nameA, nameB)
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            from Match_component import sample_desc_from_points
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)
            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, W_o, H_o)
            
            '''截断150'''
            if self.top_k:
                if pointsA_o.shape[0] > self.top_k:
                    pointsA_o = pointsA_o[:self.top_k, :]
                if pointsB_o.shape[0] > self.top_k:
                    pointsB_o = pointsB_o[:self.top_k, :]

            desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

            warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
            pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis)  # p -> k
            net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
            if pos_repeatA_mask.shape[0] > 0: 
                net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
                net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]

            # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
            # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)
            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_thr(desc_A, desc_B, net_match_mask, thr1=30)

            net_pos_nnA = pointsA_o[net_match_indicesA, :]
            net_pos_nnB = pointsB_o[net_match_indicesB, :]
            # if not self.isDilation:
            #     net_pos_nnA[:, 0] += 2
            #     net_pos_nnB[:, 0] += 2
            if require_image:
                pred = {}
                pred.update({
                    "pts": pointsA_o.detach().cpu().numpy(),
                    "pts_H": pointsB_o.detach().cpu().numpy(),
                    "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                    "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                    "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                    "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                })   # 图像坐标系
                img_pair = {}
                img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                f = self.output_dir / (nameAB + "_net_line.bmp")
                cv2.imwrite(str(f), img_pts)

            # 画Net配准图
            if require_image:
                Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)


            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if remove_border_r > 0:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)
            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis)  # p -> k
            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
            # print(siftnet_desc_A[0])
            # print(siftnet_desc_A[149])

            siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            # print(siftnet_desc_B[0])

            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_thr(siftnet_desc_A, siftnet_desc_B, sift_match_mask, thr1=30)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            # 画Sift配准图
            if require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if remove_border_r > 0:
                sift_descA = sift_descA[borderA, :]
                sift_descB = sift_descB[borderB, :]
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_match_hanming_dis, sift_nomatch_hanming_dis = self.get_des_hanmingdis_permute(sift_descA, sift_descB, sift_match_mask)
            sift_match_hanming_dis, sift_nomatch_hanming_dis = self.get_des_hanmingdis_thr(sift_descA, sift_descB, sift_match_mask, thr=60)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            # sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_match_mask == 1], sift_hanmingdist_AtoB[sift_match_mask == 0]
            
            if idx == 0:
                net_match_hanming_dis_all = net_match_hanming_dis
                net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        content = []
        net_match_max, net_match_min, net_match_mean = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match')
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch')
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])

        net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
        net_match_ones = torch.ones_like(net_match_hanming_dis_all)
        net_match_num = net_match_hanming_dis_all.shape[0]
        net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
            net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
            net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


"""命中率分析"""
class Hit_Ratio_Analysis_ALikeWithHard(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 300
        self.th2            = 300
        self.isrot          = True
        self.isrot91        = True
        self.remove_border_w = 0
        self.remove_border_h = 0 # 14
        self.test_netp_flag = False

        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_enhance_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front

    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        return pts, desc_front, desc_back

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min

    def get_match_point_pair(self, dis, dis_thre=-1):  # 获得双向匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre

        match_mask = torch.zeros_like(dis, device=self.device)    

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        b2a_min_id = torch.argmin(dis, dim=0)  # N X 1
        mid_mask = b2a_min_id[a2b_min_id]

        # 双向最近邻
        len_p = len(a2b_min_id)
        len_a = len(b2a_min_id)
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_bs = torch.tensor(list(range(len_a)), device=self.device)
        mask = reshape_as == mid_mask
        a_indexes = reshape_as[mask]
        b_indexes = a2b_min_id[mask]
        match_mask[reshape_as, a2b_min_id] = 1
        match_mask[b2a_min_id, reshape_bs] = 1

        ch = dis[a_indexes, b_indexes] < correspond
        a_s = a_indexes[ch]
        b_s = b_indexes[ch]
        d_k = dis[a_s, b_s]
     
        return a_s, b_s, d_k, match_mask

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        # match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        match_indicesA, match_indicesB, _ = self.get_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)    
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]
     
        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        return dis_max, dis_min, dis_mean

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def test_process(self, FPDT):
        count = 0

        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = True
        content = []
        eps = 1e-6
        remove_border_r = 2
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)

                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)
                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 
                
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis)  # p -> k
                assert pos_repeatA_mask.shape[0] > 0 

                net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_nn_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                # net_pointsA_norm = pointsA / pointsA.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
                # net_pointsB_norm = pointsB / pointsB.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_nn_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_nn_mask)
                # net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_nn_mask)
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1
                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = torch.sum(net_match_mask * net_nn_mask, (0, 1)) 
                net_hit_ratio = net_cand_num / (net_nn_num + eps)
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4
                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis)  # p -> k
            assert sift_pos_repeatA_mask.shape[0] > 0 

            sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
            sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
            sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1

            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            # sift_ptsA_norm = sift_ptsA_now / sift_ptsA_now.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
            # sift_ptsB_norm = sift_ptsB_now / sift_ptsB_now.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
            # _, _, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis(sift_ptsA_norm, sift_ptsB_norm, desc_A.unsqueeze(0), desc_B.unsqueeze(0), sift_match_mask)
            siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
            siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            if self.thr_mode:
                siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_nn_mask)
            # siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_nn_mask)
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1
            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_num / (sift_nn_num + eps)

            # siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            # siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)

            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]

            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                    sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                    sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                    sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                    sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                    sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1 * 2,  self.th2 * 2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)   
            sift_match_indicesA, sift_match_indicesB, _ = self.get_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[0] + 1)  # p -> k   
            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA, sift_match_indicesB] = 1
            sift_cand_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1))
            sift_hit_ratio = sift_cand_num / (sift_nn_num + eps)
            # sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            content.append([nameA, nameB, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_num.item(), siftnet_cand_num.item(), sift_cand_num.item()])


        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))

"""Top30对中match占比"""
class Candidate_Hit_Ratio_Analysis_ALikeWithHard(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w = 0
        self.remove_border_h = 0 # 14
        self.test_netp_flag = True

        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass
        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_enhance_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_enhance.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front

    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist
    
    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min

    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_match_point_pair(self, dis, dis_thre=-1):  # 获得双向匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre

        match_mask = torch.zeros_like(dis, device=self.device)    

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        b2a_min_id = torch.argmin(dis, dim=0)  # N X 1
        mid_mask = b2a_min_id[a2b_min_id]

        # 双向最近邻
        len_p = len(a2b_min_id)
        len_a = len(b2a_min_id)
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_bs = torch.tensor(list(range(len_a)), device=self.device)
        mask = reshape_as == mid_mask
        a_indexes = reshape_as[mask]
        b_indexes = a2b_min_id[mask]
        match_mask[reshape_as, a2b_min_id] = 1
        match_mask[b2a_min_id, reshape_bs] = 1

        ch = dis[a_indexes, b_indexes] < correspond
        a_s = a_indexes[ch]
        b_s = b_indexes[ch]
        d_k = dis[a_s, b_s]
     
        return a_s, b_s, d_k, match_mask

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        # match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        match_indicesA, match_indicesB, _ = self.get_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)    
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)

        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # Top30
        top30_mask = torch.topk(min_dis, 30, largest=False)      
        return match_indicesA[top30_mask.indices], match_indicesB[top30_mask.indices], hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * 10000) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * 10000) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        return dis_max, dis_min, dis_mean

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def test_process(self, FPDT):
        count = 0

        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = True
        content = []
        eps = 1e-6
        remove_border_r = 2
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())
            trans_angle = trans_obj.rotation * 180 / math.pi + 180 # [-180, 180]

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)

                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)

                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)
                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 
                
                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # print(nameA, nameB)
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)  
                    
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                assert pos_repeatA_mask.shape[0] > 0 

                net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_nn_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                # net_pointsA_norm = pointsA / pointsA.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
                # net_pointsB_norm = pointsB / pointsB.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_nn_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_nn_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_nn_mask)
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1
                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4
                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            assert sift_pos_repeatA_mask.shape[0] > 0 

            sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
            sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
            sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1


            if self.isdense:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                if self.has45:
                    siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                else:
                    siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                    siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
            else:
                siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)

            else:
                siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_nn_mask)
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1
            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num

            # siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            # siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)

            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]

            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                    sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                    sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                    sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                    sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                    sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)   
            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask] 

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num
            # sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            content.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item()])


        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Candidate_Hit_Ratio_Analysis_ALikeWithHard_Ext(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w = 0
        self.remove_border_h = 0 # 14
        self.test_netp_flag = True

        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6_7024/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass
        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front

    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist
    
    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min

    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_match_point_pair(self, dis, dis_thre=-1):  # 获得双向匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre

        match_mask = torch.zeros_like(dis, device=self.device)    

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        b2a_min_id = torch.argmin(dis, dim=0)  # N X 1
        mid_mask = b2a_min_id[a2b_min_id]

        # 双向最近邻
        len_p = len(a2b_min_id)
        len_a = len(b2a_min_id)
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_bs = torch.tensor(list(range(len_a)), device=self.device)
        mask = reshape_as == mid_mask
        a_indexes = reshape_as[mask]
        b_indexes = a2b_min_id[mask]
        match_mask[reshape_as, a2b_min_id] = 1
        match_mask[b2a_min_id, reshape_bs] = 1

        ch = dis[a_indexes, b_indexes] < correspond
        a_s = a_indexes[ch]
        b_s = b_indexes[ch]
        d_k = dis[a_s, b_s]
     
        return a_s, b_s, d_k, match_mask

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        # match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        match_indicesA, match_indicesB, _ = self.get_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)    
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)

        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # Top30
        top30_mask = torch.topk(min_dis, 30, largest=False)      
        return match_indicesA[top30_mask.indices], match_indicesB[top30_mask.indices], hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * 10000) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * 10000) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        return dis_max, dis_min, dis_mean

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def test_process(self, FPDT):
        count = 0

        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = True
        content = []
        eps = 1e-6
        remove_border_r = 2
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_t2s(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)

            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())
            trans_angle = trans_obj.rotation * 180 / math.pi + 180 # [-180, 180]

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)

                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)

                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)
                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 
                
                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # print(nameA, nameB)
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        # desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        # desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
 
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                assert pos_repeatA_mask.shape[0] > 0 

                net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_nn_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                # net_pointsA_norm = pointsA / pointsA.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1
                # net_pointsB_norm = pointsB / pointsB.new_tensor([W - 1, H - 1]).to(self.device) * 2 - 1

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_nn_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_nn_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_nn_mask)
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1
                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4
                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            assert sift_pos_repeatA_mask.shape[0] > 0 

            sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
            sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
            sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1


            if self.isdense:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                if self.has45:
                    # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                    siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                else:
                    siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                    siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
            else:
                siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)

            else:
                siftnet_match_indicesA, siftnet_match_indicesB, _, _ = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_nn_mask)
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1
            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num

            # siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            # siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)

            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]

            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                    sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                    sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                    sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                    sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                    sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)   
            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask] 

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num
            # sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            content.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item()])


        df = pd.DataFrame(content, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num'
                    ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_ALikeWithHard_Ext(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = False
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6_7024/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * 10000) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * 10000) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict)
        repeat_dis = 2
        require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_t2s(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())
            trans_angle = trans_obj.rotation * 180 / math.pi + 180 # [-180, 180]

            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)  
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        # desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        # desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                if pos_repeatA_mask.shape[0] > 0: 
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                    net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                    net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis  = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)
                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)

                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1
                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                # if not self.isDilation:
                #     net_pos_nnA[:, 0] += 2
                #     net_pos_nnB[:, 0] += 2
                if require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    f = self.output_dir / (nameAB + "_net_line.bmp")
                    cv2.imwrite(str(f), img_pts)

                # 画Net配准图
                if require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4
                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            

            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            if self.isdense:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                if self.has45:
                    # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                    siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                else:
                    siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                    siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

            else:
                siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            # 画Sift配准图
            if require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                    sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                    sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                    sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                    sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                    sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_match_mask == 1], sift_hanmingdist_AtoB[sift_match_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask] 

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num
            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item()])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num'
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_ALikeWithHard_Ext93(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = False
        self.siftq           = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9], mat_info[:, -5]
        desc2, desc6 = mat_info[:, -8], mat_info[:, -4]
        desc3, desc7 = mat_info[:, -7], mat_info[:, -3]
        desc4, desc8 = mat_info[:, -6], mat_info[:, -2]
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input


    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * 10000) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * 10000) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)  
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        # desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        # desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                if pos_repeatA_mask.shape[0] > 0: 
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                    net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                    net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis  = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)
                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)

                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1
                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                # if not self.isDilation:
                #     net_pos_nnA[:, 0] += 2
                #     net_pos_nnB[:, 0] += 2
                if require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    f = self.output_dir / (nameAB + "_net_line.bmp")
                    cv2.imwrite(str(f), img_pts)

                # 画Net配准图
                if require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            

            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            if self.isdense:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                if self.has45:
                    # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                    siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                else:
                    siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                    siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

            else:
                siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            # 画Sift配准图
            if require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask] 

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num
            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item()])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num'
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_oridiff_ALikeWithHard_Ext93(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True

        self.isdense        = False # False
        self.has45          = False # True

        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.slope          = 5000
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = False
        self.siftq           = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        
        # 6193_DK7_merge_test1_9800
        # 6193_DK7_XA_rot_test_9800
        # 6193_DK7_merge_test2_9800
        # 6193-DK7-160-8-normal-nocontrol_test_9800
        
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1_9800/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input


    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_des_hanmingdis_wht_permute_rec_sift_256(self, des_a, des_b):
        half_dim =  des_a.shape[1] // 2 
        desc_binary_a = des_a  # Nxdim
        desc_binary_b = des_b
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.sift_index] @ desc_binary_b[:, self.sift_index].t() - (1 - desc_binary_a[:, self.sift_index]) @ (1 - desc_binary_b[:, self.sift_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.sift_index==False] @ desc_binary_b[:, self.sift_index==False].t() - (1 - desc_binary_a[:, self.sift_index==False]) @ (1 - desc_binary_b[:, self.sift_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end


    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = desA[:, 128:].float(), desB[:, 128:].float()
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_rec_sift_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        net_require_image = False
        sift_require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)  
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        # desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        # desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                if pos_repeatA_mask.shape[0] > 0: 
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                    net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                    net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis  = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)
                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)

                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1
                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                # if not self.isDilation:
                #     net_pos_nnA[:, 0] += 2
                #     net_pos_nnB[:, 0] += 2
                if require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    f = self.output_dir / (nameAB + "_net_line.bmp")
                    cv2.imwrite(str(f), img_pts)

                # 画Net配准图
                if require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            
            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            # sift_ptsA_now = copy.deepcopy(sift_ptsA)
            # sift_ptsB_now = copy.deepcopy(sift_ptsB)
            # if not self.isDilation:
            #     sift_ptsA_now[:, 0] -= 2
            #     sift_ptsB_now[:, 0] -= 2

            # sift描述
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.isdense:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)
                
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                        siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                    else:
                        siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                        siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                    siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                    siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                else:
                    if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                        siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                        siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                    else:
                        if self.has45:
                            # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                        
                            # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                        
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                        else:
                            siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

            else:
                # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                # print(siftnet_desc_A_0[0, :]]
                # torch.set_printoptions(precision=10)
                # print(siftnet_desc_A_0[8, :])
                # print((siftnet_desc_A_0 * 100).int()[8, :])
                # print(sift_ptsA[8, :])
                # exit()
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                        siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                    else:
                        siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                        siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                    siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                    siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                else:
                    if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                        siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                        siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                    else:
                        if self.has45:
                            # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                        
                            # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                        
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                        else:
                            siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
            
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                    else:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                else:
                    siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            siftnet_pointdis_mean = torch.mean(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_var = torch.var(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_std = torch.std(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_oridiff_mean = torch.mean(sift_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_var = torch.var(sift_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_std = torch.std(sift_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_hamdis_top30_mean = torch.mean(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_var = torch.var(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_std = torch.std(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            if siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                siftnet_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
            else:
                siftnet_hamdis_top30max = torch.max(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])

            # 画Net配准图
            if net_require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8)
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_siftnet_match.bmp'), image_merge)
            
            # 画Sift配准图
            if sift_require_image:
                mat_H_modify = self.homography_centorcrop(mat_H, -3, -8) # 122x36 -> 128x52
                imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(mat_H_modify.cpu().numpy().astype(np.float32)))
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_2D_warped = imgA_o_warped.numpy().squeeze()
                b = np.zeros_like(img_2D_A)
                g = img_2D_warped * 255  #  旋转后的模板
                r = img_2D_B * 255    
                image_merge = cv2.merge([b, g, r])
                image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_sift_match.bmp'), image_merge)

            # 如果是长条形描述测试sift 0度 128维double
            if self.is_rec and self.test_sift128:
                sift_descA = torch.cat((sift_descA[:, :128], sift_descA[:, :128]), dim=1)
                sift_descB = torch.cat((sift_descB[:, :128], sift_descB[:, :128]), dim=1)

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask]
            # print(torch.sort(sift_min_dis[sift_top30_mask]), sift_pos_repeatA_indices, torch.sort(sift_pos_repeatB_mask))

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            # print(sift_match_indicesA, sift_match_indicesB, sift_min_dis)
            # print(sift_hanmingdist_AtoB[sift_match_mask == 1], torch.where(sift_match_mask == 1)[0], torch.where(sift_match_mask == 1)[1])
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num

            sift_pointdis_mean = torch.mean(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_var = torch.var(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_std = torch.std(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])

            sift_oridiff_mean = torch.mean(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_var = torch.var(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_std = torch.std(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])

            sift_hamdis_top30_mean = torch.mean(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_var = torch.var(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_std = torch.std(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            if sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                sift_hamdis_top30max = torch.mean(torch.tensor([])) 
            else:
                sift_hamdis_top30max  = torch.max(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])

            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item(), 
                                siftnet_pointdis_mean.item(), siftnet_pointdis_var.item(), siftnet_pointdis_std.item(), sift_pointdis_mean.item(), sift_pointdis_var.item(),sift_pointdis_std.item(),
                                siftnet_oridiff_mean.item(), siftnet_oridiff_var.item(), siftnet_oridiff_std.item(), sift_oridiff_mean.item(), sift_oridiff_var.item(), sift_oridiff_std.item(),
                                siftnet_hamdis_top30_mean.item(), siftnet_hamdis_top30_var.item(), siftnet_hamdis_top30_std.item(), siftnet_hamdis_top30max.item(), sift_hamdis_top30_mean.item(), sift_hamdis_top30_var.item(), sift_hamdis_top30_std.item(), sift_hamdis_top30max.item()])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max'
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_oridiff_ALikeWithHard_Ext93_Pnt(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 130
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.bord           = 2

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True

        self.isdense        = False # False
        self.has45          = False # True

        self.partial_filter = True
        self.use_netori     = True # True

        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.slope          = 5000
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        self.test_netdesc_flag = False # True
        self.siftq           = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        
        # 6193_DK7_merge_test1_9800
        # 6193_DK7_XA_rot_test_9800
        # 6193_DK7_merge_test2_9800
        # 6193-DK7-160-8-normal-nocontrol_test_9800
        # 6193-DK4-130-purple-suppress-SNR28_9800
        # 6193-DK4-110-tarnish-normal-SNR80_9800
        
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1_9800/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (2, 2, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt_debase(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        import torch.nn.functional as F
        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('_9800', '').replace('img_extend_data', 'img_ori_data').replace('_extend', ''))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)
        img_pnt_o_A = F.pad(img_pnt_o_A, (2, 2, 2, 2), "constant", 0)       # 118 x 32 -> 122 x 36

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('_9800', '').replace('img_extend_data', 'img_ori_data').replace('_extend', ''))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 118 x 32
        img_pnt_o_B = F.pad(img_pnt_o_B, (2, 2, 2, 2), "constant", 0)       # 118 x 32 -> 122 x 36

        if self.isDilation:
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (2, 2, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_des_hanmingdis_wht_permute_rec_sift_256(self, des_a, des_b):
        half_dim =  des_a.shape[1] // 2 
        desc_binary_a = des_a  # Nxdim
        desc_binary_b = des_b
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.sift_index] @ desc_binary_b[:, self.sift_index].t() - (1 - desc_binary_a[:, self.sift_index]) @ (1 - desc_binary_b[:, self.sift_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.sift_index==False] @ desc_binary_b[:, self.sift_index==False].t() - (1 - desc_binary_a[:, self.sift_index==False]) @ (1 - desc_binary_b[:, self.sift_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end


    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = desA[:, 128:].float(), desB[:, 128:].float()
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_rec_sift_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        net_require_image = False
        sift_require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s_pnt(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            imgA_pnt, imgB_pnt = sample["imgA_pnt"].unsqueeze(0), sample["imgB_pnt"].unsqueeze(0)
            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            # sift点
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isDilation:
                W_o -= 4            # 122 x 40 -> 122 x 36

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            
            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1

            # sift描述
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.test_netp_flag:
                '''Tradition'''
                heatmap, _ = FPDT.run_heatmap_alike_9800(imgA_pnt.to(self.device))   # 128 X 40
                heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB_pnt.to(self.device))
                
                # 有角度图
                intensityA, intensityB = None, None
                pnmapA, pnmapB = None, None
                if self.use_netori and heatmap.shape[0] == 3 and heatmap_B.shape[0] == 3:
                    intensityA = heatmap[1, 3:-3, 2:-2]
                    intensityB = heatmap_B[1, 3:-3, 2:-2]
                    pnmapA = heatmap[2, 3:-3, 2:-2]
                    pnmapB = heatmap_B[2, 3:-3, 2:-2]

                # print(heatmap.shape)
                heatmap = heatmap[0, 3:-3, 2:-2]           # 128 x 40 -> 122 x 36
                heatmap_B = heatmap_B[0, 3:-3, 2:-2]        
                
                if self.partial_filter:
                    heatmap = heatmap * imgA_partial_mask.to(heatmap.device)
                    heatmap_B = heatmap_B * imgB_partial_mask.to(heatmap_B.device)
                    
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 122 x 36
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2                        
                else:
                    imgA_pnt, imgB_pnt = sample['img_pnt_o_A'].unsqueeze(0), sample['img_pnt_o_B'].unsqueeze(0)
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    # W_o -= 4    # 40->36
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [122, 36]的中间（118，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]
                
                # predict angle
                net_oriA, net_oriB = None, None
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    pointsA_o_normalized = pointsA_o / pointsA_o.new_tensor([W_o - 1, H_o - 1]).to(pointsA_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    pointsB_o_normalized = pointsB_o / pointsB_o.new_tensor([W_o - 1, H_o - 1]).to(pointsB_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_kpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_kpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_kpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_kpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    pn_predictA = (pn_kpA_predict_all > 0.5).float().squeeze()   
                    pn_predictB = (pn_kpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    net_oriA = ori_kpA_predict_all[:, 0] * (2 * pn_predictA - 1) * 90      # 正负90度
                    net_oriB = ori_kpB_predict_all[:, 0] * (2 * pn_predictB - 1) * 90
                    # print(net_oriA.shape)

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                # net_nn_mask
                warped_net_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                net_point_disAB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2]) # N x M

                warped_net_pts_A, net_mask_points = filter_points(warped_net_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                net_key_disAtoB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2])
                net_pos_repeatA_mask, net_pos_repeatB_mask, _ = self.get_point_pair(net_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                if net_pos_repeatA_mask.shape[0] > 0:
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[net_mask_points][net_pos_repeatA_mask]
                    # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, net_pos_repeatB_mask] = 1

                if self.isdense:
                    net_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)  
                    net_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    if net_oriA is None and net_oriB is None:
                        net_desc_A_0, net_oriA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                        net_desc_B_0, net_oriB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                    else:
                        net_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                        net_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        
                        # # 测试网络点 + 网络角度 + 网络描述子的一致性
                        # torch.set_printoptions(precision=7)
                        # print(nameA, pointsA_o, net_desc_A_0)
                        # save_path = Path(self.output_dir, nameA)
                        # self.output_txt(save_path, nameA, net_desc_A_0.cpu().numpy(), None)
                        # exit()

                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                
                if self.thr_mode:
                    # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                    if self.is_rec:
                        if self.cat_sift:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                    else:
                        net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(net_desc_A, net_desc_B, net_match_mask)
            
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1

                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                
                net_pointdis_mean = torch.mean(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_var = torch.var(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_std = torch.std(net_point_disAB[(net_match_mask * net_nn_mask) == 1])

                net_ori_diff = self.get_ori_diff(net_oriA, net_oriB, trans_angle)
                net_oridiff_mean = torch.mean(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_var = torch.var(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_std = torch.std(net_ori_diff[(net_match_mask * net_nn_mask) == 1])

                net_hamdis_top30_mean = torch.mean(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_var = torch.var(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_std = torch.std(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
                if net_hanming_dis[(net_match_mask * net_nn_mask) == 1].shape[0] == 0:
                    net_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
                else:
                    net_hamdis_top30max = torch.max(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])

                # 画Net配准图
                if net_require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8) # 122 x 36 -> 128 x 52
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_net_match.bmp'), image_merge)
                
                # if require_image:
                #     pred = {}
                #     pred.update({
                #         "pts": pointsA_o.detach().cpu().numpy(),
                #         "pts_H": pointsB_o.detach().cpu().numpy(),
                #         "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                #         "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                #         "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                #         "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                #     })   # 图像坐标系
                #     img_pair = {}
                #     img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                #     img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                #     # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                #     f = self.output_dir / (nameAB + "_net_line.bmp")
                #     cv2.imwrite(str(f), img_pts)

            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            siftnet_oriA, siftnet_oriB = sift_oriA, sift_oriB
            if not self.test_netdesc_flag:
                siftnet_desc_A, siftnet_desc_B = torch.rand(sift_ptsA.shape[0], 256).to(self.device), torch.rand(sift_ptsB.shape[0], 256).to(self.device)
            else:
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    sift_ptsA_normalized = sift_ptsA / sift_ptsA.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsA.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    sift_ptsB_normalized = sift_ptsB / sift_ptsB.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsB.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_siftkpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_siftkpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_siftkpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_siftkpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    sift_pn_predictA = (pn_siftkpA_predict_all > 0.5).float().squeeze()   
                    sift_pn_predictB = (pn_siftkpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    siftnet_oriA = ori_siftkpA_predict_all[:, 0] * (2 * sift_pn_predictA - 1) * 90      # 正负90度
                    siftnet_oriB = ori_siftkpB_predict_all[:, 0] * (2 * sift_pn_predictB - 1) * 90
                    # print(net_oriA.shape)


                if self.isdense:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                    siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    siftnet_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=siftnet_oriA)
                    siftnet_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=siftnet_oriB)

                    # print(siftnet_desc_A_0[0, :]]
                    # torch.set_printoptions(precision=10)
                    # print(siftnet_desc_A_0[8, :])
                    # print((siftnet_desc_A_0 * 100).int()[8, :])
                    # print(sift_ptsA[8, :])
                    # exit()
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                    else:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                else:
                    siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            siftnet_pointdis_mean = torch.mean(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_var = torch.var(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_std = torch.std(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_ori_diff = self.get_ori_diff(siftnet_oriA, siftnet_oriB, trans_angle)
            siftnet_oridiff_mean = torch.mean(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_var = torch.var(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_std = torch.std(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_hamdis_top30_mean = torch.mean(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_var = torch.var(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_std = torch.std(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            if siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                siftnet_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
            else:
                siftnet_hamdis_top30max = torch.max(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])

            # 画Net配准图
            if net_require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8)
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_siftnet_match.bmp'), image_merge)
            
            # 画Sift配准图
            if sift_require_image:
                mat_H_modify = self.homography_centorcrop(mat_H, -3, -8) # 122x36 -> 128x52
                imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(mat_H_modify.cpu().numpy().astype(np.float32)))
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_2D_warped = imgA_o_warped.numpy().squeeze()
                b = np.zeros_like(img_2D_A)
                g = img_2D_warped * 255  #  旋转后的模板
                r = img_2D_B * 255    
                image_merge = cv2.merge([b, g, r])
                image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_sift_match.bmp'), image_merge)

            # 如果是长条形描述测试sift 0度 128维double
            if self.is_rec and self.test_sift128:
                sift_descA = torch.cat((sift_descA[:, :128], sift_descA[:, :128]), dim=1)
                sift_descB = torch.cat((sift_descB[:, :128], sift_descB[:, :128]), dim=1)

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask]
            # print(torch.sort(sift_min_dis[sift_top30_mask]), sift_pos_repeatA_indices, torch.sort(sift_pos_repeatB_mask))

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            # print(sift_match_indicesA, sift_match_indicesB, sift_min_dis)
            # print(sift_hanmingdist_AtoB[sift_match_mask == 1], torch.where(sift_match_mask == 1)[0], torch.where(sift_match_mask == 1)[1])
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num

            sift_pointdis_mean = torch.mean(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_var = torch.var(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_std = torch.std(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])

            sift_oridiff_mean = torch.mean(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_var = torch.var(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_std = torch.std(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])

            sift_hamdis_top30_mean = torch.mean(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_var = torch.var(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_std = torch.std(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            if sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                sift_hamdis_top30max = torch.mean(torch.tensor([])) 
            else:
                sift_hamdis_top30max  = torch.max(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])

            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item(), 
                                siftnet_pointdis_mean.item(), siftnet_pointdis_var.item(), siftnet_pointdis_std.item(), sift_pointdis_mean.item(), sift_pointdis_var.item(), sift_pointdis_std.item(), net_pointdis_mean.item(), net_pointdis_var.item(), net_pointdis_std.item(),
                                siftnet_oridiff_mean.item(), siftnet_oridiff_var.item(), siftnet_oridiff_std.item(), sift_oridiff_mean.item(), sift_oridiff_var.item(), sift_oridiff_std.item(), net_oridiff_mean.item(), net_oridiff_var.item(), net_oridiff_std.item(),
                                siftnet_hamdis_top30_mean.item(), siftnet_hamdis_top30_var.item(), siftnet_hamdis_top30_std.item(), siftnet_hamdis_top30max.item(), sift_hamdis_top30_mean.item(), sift_hamdis_top30_var.item(), sift_hamdis_top30_std.item(), sift_hamdis_top30max.item(),
                                net_hamdis_top30_mean.item(), net_hamdis_top30_var.item(), net_hamdis_top30_std.item(), net_hamdis_top30max.item()])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'net_pointdis_mean',
                    'net_pointdis_var',
                    'net_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'net_oridiff_mean',
                    'net_oridiff_var',
                    'net_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'net_hamdis_top30_mean',
                    'net_hamdis_top30_var',
                    'net_hamdis_top30_std',
                    'net_hamdis_top30max'
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_oridiff_ALikeWithHard_Ext95_Pnt(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 130
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.bord           = 2

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True
        self.is_imitate_95   = False

        self.isdense        = False # False
        self.has45          = False # True

        self.partial_filter = True
        self.use_netori     = True # True

        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.slope          = 5000
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        self.test_netdesc_flag = False # True
        self.siftq           = True

        self.use_wbpnt_mask = True
        self.enhance_version = '615' # '20577' '91' '583' '612'

        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        
        # 6193_DK7_merge_test1_9800
        # 6193_DK7_XA_rot_test_9800
        # 6193_DK7_merge_test2_9800
        # 6195_DK7_110_cd_auto
        # 6195_DK7-110-normal-mul_auto
        # 6195_DK7140_wet
        # 6193-DK7-160-8-normal-nocontrol_test_9800
        # 6193-DK4-130-purple-suppress-SNR28_9800
        # 6193-DK4-110-tarnish-normal-SNR80_9800
        
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/home/linwc/match/data/6195Test/6195_DK7-110-normal-mul_auto_' + self.enhance_version + '/SIFT_Trans/SIFT_transSucc_615.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 4:-4].unsqueeze(0)      # 128 x 48 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 4:-4].unsqueeze(0) 

            pad_size = (5, 5, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 30 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

            if self.is_imitate_95:
                homo_modify = self.homography_centorcrop(homo, 0, 3)    # trans从 122x36->122x30
            else:
                homo_modify = copy.deepcopy(homo)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
            homo_modify = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed, 'homo_modify': torch.inverse(homo_modify)})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt_debase(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        import torch.nn.functional as F
        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('_9800', '').replace('img_extend_data', 'img_ori_data').replace('_extend', ''))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)
        img_pnt_o_A = F.pad(img_pnt_o_A, (2, 2, 2, 2), "constant", 0)       # 118 x 32 -> 122 x 36

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('_9800', '').replace('img_extend_data', 'img_ori_data').replace('_extend', ''))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 118 x 32
        img_pnt_o_B = F.pad(img_pnt_o_B, (2, 2, 2, 2), "constant", 0)       # 118 x 32 -> 122 x 36

        if self.isDilation:
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (2, 2, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_des_hanmingdis_wht_permute_rec_sift_256(self, des_a, des_b):
        half_dim =  des_a.shape[1] // 2 
        desc_binary_a = des_a  # Nxdim
        desc_binary_b = des_b
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.sift_index] @ desc_binary_b[:, self.sift_index].t() - (1 - desc_binary_a[:, self.sift_index]) @ (1 - desc_binary_b[:, self.sift_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.sift_index==False] @ desc_binary_b[:, self.sift_index==False].t() - (1 - desc_binary_a[:, self.sift_index==False]) @ (1 - desc_binary_b[:, self.sift_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end


    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(self, desA, desB, wbpnt_maskA, wbpnt_maskB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesA_w], all_indexesA[~wbpnt_maskA][match_indicesA_b]), dim=0)
        match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        min_dis = torch.cat((min_dis_w, min_dis_b), dim=0)
        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = desA[:, 128:].float(), desB[:, 128:].float()
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_rec_sift_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        net_require_image = False
        sift_require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s_pnt(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H, mat_H_mod = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo'], sample['homo_modify']
            imgA_pnt, imgB_pnt = sample["imgA_pnt"].unsqueeze(0), sample["imgB_pnt"].unsqueeze(0)
            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            # sift点
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isDilation:
                W_o -= 4            # 122 x 40 -> 122 x 36

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            
            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            if warped_sift_pts_A.shape[0] == 0:
                warped_sift_pts_A = torch.ones_like(sift_ptsA) * (-10 * max(W_o, H_o))

            sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                # sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device)
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                
            # sift描述
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.test_netp_flag:
                '''Tradition'''
                heatmap, _ = FPDT.run_heatmap_alike_9800(imgA_pnt.to(self.device))   # 128 X 40
                heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB_pnt.to(self.device))
                
                # 有角度图
                intensityA, intensityB = None, None
                pnmapA, pnmapB = None, None
                if self.use_netori and heatmap.shape[0] == 3 and heatmap_B.shape[0] == 3:
                    intensityA = heatmap[1, 3:-3, 5:-5]
                    intensityB = heatmap_B[1, 3:-3, 5:-5]
                    pnmapA = heatmap[2, 3:-3, 5:-5]
                    pnmapB = heatmap_B[2, 3:-3, 5:-5]

                # print(heatmap.shape)
                heatmap = heatmap[0, 3:-3, 5:-5]           # 128 x 40 -> 122 x 30
                heatmap_B = heatmap_B[0, 3:-3, 5:-5]        
                
                if self.partial_filter:
                    heatmap = heatmap * imgA_partial_mask.to(heatmap.device)
                    heatmap_B = heatmap_B * imgB_partial_mask.to(heatmap_B.device)
                    
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 122 x 36
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2                        
                else:
                    imgA_pnt, imgB_pnt = sample['img_pnt_o_A'].unsqueeze(0), sample['img_pnt_o_B'].unsqueeze(0)
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 48
                    # W_o -= 4    # 40->36
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36

                W_pnt_o = W_o
                H_pnt_o = H_o
                
                W_pnt_o = W_o - 6
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_pnt_o, H_pnt_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_pnt_o, H_pnt_o)  # [122, 30]的中间（118，26）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]
                
                # predict angle
                net_oriA, net_oriB = None, None
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    pointsA_o_normalized = pointsA_o / pointsA_o.new_tensor([W_pnt_o - 1, H_pnt_o - 1]).to(pointsA_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    pointsB_o_normalized = pointsB_o / pointsB_o.new_tensor([W_pnt_o - 1, H_pnt_o - 1]).to(pointsB_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_kpB_predict_all = F.grid_sample(intensityB.view(-1, H_pnt_o, W_pnt_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_kpA_predict_all = F.grid_sample(intensityA.view(-1, H_pnt_o, W_pnt_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_kpB_predict_all = F.grid_sample(pnmapB.view(-1, H_pnt_o, W_pnt_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_kpA_predict_all = F.grid_sample(pnmapA.view(-1, H_pnt_o, W_pnt_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    pn_predictA = (pn_kpA_predict_all > 0.5).float().squeeze()   
                    pn_predictB = (pn_kpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    net_oriA = ori_kpA_predict_all[:, 0] * (2 * pn_predictA - 1) * 90      # 正负90度
                    net_oriB = ori_kpB_predict_all[:, 0] * (2 * pn_predictB - 1) * 90
                    # print(net_oriA.shape)

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                # net_nn_mask
                warped_net_pts_A = warp_points(pointsA_o, mat_H_mod.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                net_point_disAB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2]) # N x M

                warped_net_pts_A, net_mask_points = filter_points(warped_net_pts_A, torch.tensor([W_pnt_o, H_pnt_o], device=self.device), return_mask=True)

                net_key_disAtoB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2])
                net_pos_repeatA_mask, net_pos_repeatB_mask, _ = self.get_point_pair(net_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                if net_pos_repeatA_mask.shape[0] > 0:
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[net_mask_points][net_pos_repeatA_mask]
                    # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, net_pos_repeatB_mask] = 1

                if self.isdense:
                    net_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)  
                    net_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    if net_oriA is None and net_oriB is None:
                        net_desc_A_0, net_oriA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                        net_desc_B_0, net_oriB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                    else:
                        # 122 x 40, 122 x 48, 122 x 30
                        net_desc_A_0, _, wbpnt_maskA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext95(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                        net_desc_B_0, _, wbpnt_maskB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext95(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        
                        # # 测试网络点 + 网络角度 + 网络描述子的一致性
                        # torch.set_printoptions(precision=7)
                        # print(nameA, pointsA_o, net_desc_A_0)
                        # save_path = Path(self.output_dir, nameA)
                        # self.output_txt(save_path, nameA, net_desc_A_0.cpu().numpy(), None)
                        # exit()

                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                
                if self.thr_mode:
                    # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                    if self.is_rec:
                        if self.cat_sift:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                    else:
                        if self.use_wbpnt_mask:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(net_desc_A, net_desc_B, wbpnt_maskA, wbpnt_maskB, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(net_desc_A, net_desc_B, net_match_mask)
            
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1

                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                
                net_pointdis_mean = torch.mean(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_var = torch.var(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_std = torch.std(net_point_disAB[(net_match_mask * net_nn_mask) == 1])

                net_ori_diff = self.get_ori_diff(net_oriA, net_oriB, trans_angle)
                net_oridiff_mean = torch.mean(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_var = torch.var(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_std = torch.std(net_ori_diff[(net_match_mask * net_nn_mask) == 1])

                net_hamdis_top30_mean = torch.mean(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_var = torch.var(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_std = torch.std(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
                if net_hanming_dis[(net_match_mask * net_nn_mask) == 1].shape[0] == 0:
                    net_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
                else:
                    net_hamdis_top30max = torch.max(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])

                # 画Net配准图
                if net_require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8) # 122 x 36 -> 128 x 52
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_net_match.bmp'), image_merge)
                
                # if require_image:
                #     pred = {}
                #     pred.update({
                #         "pts": pointsA_o.detach().cpu().numpy(),
                #         "pts_H": pointsB_o.detach().cpu().numpy(),
                #         "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                #         "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                #         "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                #         "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                #     })   # 图像坐标系
                #     img_pair = {}
                #     img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                #     img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                #     # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                #     f = self.output_dir / (nameAB + "_net_line.bmp")
                #     cv2.imwrite(str(f), img_pts)

            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            siftnet_oriA, siftnet_oriB = sift_oriA, sift_oriB
            if not self.test_netdesc_flag:
                siftnet_desc_A, siftnet_desc_B = torch.rand(sift_ptsA.shape[0], 256).to(self.device), torch.rand(sift_ptsB.shape[0], 256).to(self.device)
            else:
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    sift_ptsA_normalized = sift_ptsA / sift_ptsA.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsA.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    sift_ptsB_normalized = sift_ptsB / sift_ptsB.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsB.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_siftkpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_siftkpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_siftkpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_siftkpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    sift_pn_predictA = (pn_siftkpA_predict_all > 0.5).float().squeeze()   
                    sift_pn_predictB = (pn_siftkpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    siftnet_oriA = ori_siftkpA_predict_all[:, 0] * (2 * sift_pn_predictA - 1) * 90      # 正负90度
                    siftnet_oriB = ori_siftkpB_predict_all[:, 0] * (2 * sift_pn_predictB - 1) * 90
                    # print(net_oriA.shape)


                if self.isdense:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                    siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    siftnet_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=siftnet_oriA)
                    siftnet_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=siftnet_oriB)

                    # print(siftnet_desc_A_0[0, :]]
                    # torch.set_printoptions(precision=10)
                    # print(siftnet_desc_A_0[8, :])
                    # print((siftnet_desc_A_0 * 100).int()[8, :])
                    # print(sift_ptsA[8, :])
                    # exit()
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                    else:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                else:
                    siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            siftnet_pointdis_mean = torch.mean(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_var = torch.var(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_std = torch.std(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_ori_diff = self.get_ori_diff(siftnet_oriA, siftnet_oriB, trans_angle)
            siftnet_oridiff_mean = torch.mean(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_var = torch.var(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_std = torch.std(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_hamdis_top30_mean = torch.mean(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_var = torch.var(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_std = torch.std(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            if siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                siftnet_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
            else:
                siftnet_hamdis_top30max = torch.max(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])

            # 画Net配准图
            if net_require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8)
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_siftnet_match.bmp'), image_merge)
            
            # 画Sift配准图
            if sift_require_image:
                mat_H_modify = self.homography_centorcrop(mat_H, -3, -8) # 122x36 -> 128x52
                imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(mat_H_modify.cpu().numpy().astype(np.float32)))
                img_2D_A = imgA.numpy().squeeze()
                img_2D_B = imgB.numpy().squeeze()
                img_2D_warped = imgA_o_warped.numpy().squeeze()
                b = np.zeros_like(img_2D_A)
                g = img_2D_warped * 255  #  旋转后的模板
                r = img_2D_B * 255    
                image_merge = cv2.merge([b, g, r])
                image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_sift_match.bmp'), image_merge)

            # 如果是长条形描述测试sift 0度 128维double
            if self.is_rec and self.test_sift128:
                sift_descA = torch.cat((sift_descA[:, :128], sift_descA[:, :128]), dim=1)
                sift_descB = torch.cat((sift_descB[:, :128], sift_descB[:, :128]), dim=1)

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask]
            # print(torch.sort(sift_min_dis[sift_top30_mask]), sift_pos_repeatA_indices, torch.sort(sift_pos_repeatB_mask))

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            # print(sift_match_indicesA, sift_match_indicesB, sift_min_dis)
            # print(sift_hanmingdist_AtoB[sift_match_mask == 1], torch.where(sift_match_mask == 1)[0], torch.where(sift_match_mask == 1)[1])
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num

            sift_pointdis_mean = torch.mean(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_var = torch.var(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_std = torch.std(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])

            sift_oridiff_mean = torch.mean(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_var = torch.var(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_std = torch.std(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])

            sift_hamdis_top30_mean = torch.mean(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_var = torch.var(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_std = torch.std(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            if sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                sift_hamdis_top30max = torch.mean(torch.tensor([])) 
            else:
                sift_hamdis_top30max  = torch.max(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])

            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item(), 
                                siftnet_pointdis_mean.item(), siftnet_pointdis_var.item(), siftnet_pointdis_std.item(), sift_pointdis_mean.item(), sift_pointdis_var.item(), sift_pointdis_std.item(), net_pointdis_mean.item(), net_pointdis_var.item(), net_pointdis_std.item(),
                                siftnet_oridiff_mean.item(), siftnet_oridiff_var.item(), siftnet_oridiff_std.item(), sift_oridiff_mean.item(), sift_oridiff_var.item(), sift_oridiff_std.item(), net_oridiff_mean.item(), net_oridiff_var.item(), net_oridiff_std.item(),
                                siftnet_hamdis_top30_mean.item(), siftnet_hamdis_top30_var.item(), siftnet_hamdis_top30_std.item(), siftnet_hamdis_top30max.item(), sift_hamdis_top30_mean.item(), sift_hamdis_top30_var.item(), sift_hamdis_top30_std.item(), sift_hamdis_top30max.item(),
                                net_hamdis_top30_mean.item(), net_hamdis_top30_var.item(), net_hamdis_top30_std.item(), net_hamdis_top30max.item()])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'net_pointdis_mean',
                    'net_pointdis_var',
                    'net_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'net_oridiff_mean',
                    'net_oridiff_var',
                    'net_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'net_hamdis_top30_mean',
                    'net_hamdis_top30_var',
                    'net_hamdis_top30_std',
                    'net_hamdis_top30max'
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))



class Distribution_Candidate_Hit_Ratio_oridiff_ALikeWithHard_Ext93_Pnt_Match(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 130
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.bord           = 2

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True

        self.isdense        = False # False
        self.has45          = False # True

        self.partial_filter = True
        self.use_netori     = True # True

        # match
        self.use_wbpnt_mask = True
        self.match_thr      = 0 # 0.2
        self.use_net_match  = True
        self.use_desc_all   = False # False
        self.use_post_process = True
        self.dbvalue_slope  = -3500

        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.slope          = 5000
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        self.test_netdesc_flag = False # True
        self.siftq           = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        
        # 6193_DK7_merge_test1_9800
        # 6193_DK7_XA_rot_test_9800
        # 6193_DK7_merge_test2_9800
        # 6193-DK7-160-8-normal-nocontrol_test_9800
        # 6193-DK4-130-purple-suppress-SNR28_9800
        # 6193-DK4-110-tarnish-normal-SNR80_9800
        # 6193-DK7-scene_normal_9800
        
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1_9800/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (2, 2, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input


    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_des_hanmingdis_wht_permute_rec_sift_256(self, des_a, des_b):
        half_dim =  des_a.shape[1] // 2 
        desc_binary_a = des_a  # Nxdim
        desc_binary_b = des_b
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.sift_index] @ desc_binary_b[:, self.sift_index].t() - (1 - desc_binary_a[:, self.sift_index]) @ (1 - desc_binary_b[:, self.sift_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.sift_index==False] @ desc_binary_b[:, self.sift_index==False].t() - (1 - desc_binary_a[:, self.sift_index==False]) @ (1 - desc_binary_b[:, self.sift_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end


    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(self, desA, desB, wbpnt_maskA, wbpnt_maskB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesA_w], all_indexesA[~wbpnt_maskA][match_indicesA_b]), dim=0)
        match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        min_dis = torch.cat((min_dis_w, min_dis_b), dim=0)
        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask(self, FPDT, desA, desB, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, repeat_mask):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        # mask1 = hanming_distAtoB_part_begin < th1
        # mask2 = hadamadist_AtoB < th2
        # hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        # hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        pred_match_w = FPDT.detector_net.get_match_predict(data_match_w).unsqueeze(0)
        pred_match_b = FPDT.detector_net.get_match_predict(data_match_b).unsqueeze(0)

        match_mess_w =self.find_match_idx_score(pred_match_w, match_thr=self.match_thr)
        match_mess_b =self.find_match_idx_score(pred_match_b, match_thr=self.match_thr)
        match_indicesB_w_all = match_mess_w['matches0'].squeeze()
        match_indicesB_b_all = match_mess_b['matches0'].squeeze()
        match_scoresB_w_all = match_mess_w['matching_scores0'].squeeze()
        match_scoresB_b_all = match_mess_b['matching_scores0'].squeeze()
        match_indicesB_w = match_indicesB_w_all[match_indicesB_w_all != -1]
        match_indicesB_b = match_indicesB_b_all[match_indicesB_b_all != -1]
        
        # match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        # match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1], all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1]), dim=0)
        match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)

        max_sim = torch.cat((match_scoresB_w_all[match_indicesB_w_all != -1], match_scoresB_b_all[match_indicesB_b_all != -1]), dim=0)
        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if max_sim.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(max_sim, 30, largest=True).indices  # 取相似度最大前三十个点对
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = desA[:, 128:].float(), desB[:, 128:].float()
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_rec_sift_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()

    def generate_int_desc(self, desc):
        desc_select_thr = self.thresholding_desc(torch.round(desc * self.slope) + 5000)
        # 量化
        desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())
        # desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)
        return desc_select_quantize

    def get_quant_desc(self, desc):
        # hadama量化
        descs_quant_0 = self.generate_int_desc(desc[:, :128])
        descs_quant_45 = self.generate_int_desc(desc[:, 128:])
        descs_quant_0_same, descs_quant_0_neg = descs_quant_0[:, self.net_index], descs_quant_0[:, self.net_index==False]
        descs_quant_45_same, descs_quant_45_neg = descs_quant_45[:, self.net_index], descs_quant_45[:, self.net_index==False]

        if self.use_desc_all:
            descs_quant = torch.cat((descs_quant_0_same, descs_quant_45_same, descs_quant_0_neg, descs_quant_45_neg), dim=-1)
        else:
            descs_quant = torch.cat((descs_quant_0_same, descs_quant_45_same, descs_quant_0_same, descs_quant_45_same), dim=-1)
        
        if self.use_post_process:
            descs_quant = descs_quant * 2 - 1

        return descs_quant.float()

    def arange_like(self, x, dim: int):
        return x.new_ones(x.shape[dim]).cumsum(0) - 1  # traceable in 1.1

    def find_match_idx_score(self, scores, match_thr=0.2):
        # Get the matches with score above "match_threshold".
        max0, max1 = scores.max(2), scores.max(1)
        indices0, indices1 = max0.indices, max1.indices
        mutual0 = self.arange_like(indices0, 1)[None] == indices1.gather(1, indices0)  #当二者互为匹配点时才认为是匹配点
        mutual1 = self.arange_like(indices1, 1)[None] == indices0.gather(1, indices1)
        zero = scores.new_tensor(0)
        mscores0 = torch.where(mutual0, max0.values.exp(), zero)
        mscores1 = torch.where(mutual1, mscores0.gather(1, indices1), zero)
        valid0 = mutual0 & (mscores0 > match_thr)
        valid1 = mutual1 & valid0.gather(1, indices1)
        indices0 = torch.where(valid0, indices0, indices0.new_tensor(-1))
        indices1 = torch.where(valid1, indices1, indices1.new_tensor(-1))
        
        return {
                'matches0': indices0, # use -1 for invalid match
                'matches1': indices1, # use -1 for invalid match
                'matching_scores0': mscores0,
                'matching_scores1': mscores1
            }

    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        net_require_image = False   # True
        sift_require_image = False
        siftnet_require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s_pnt(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            imgA_pnt, imgB_pnt = sample["imgA_pnt"].unsqueeze(0), sample["imgB_pnt"].unsqueeze(0)
            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            # sift点
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isDilation:
                W_o -= 4            # 122 x 40 -> 122 x 36

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            
            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1

            # sift描述
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.test_netp_flag:
                '''Tradition'''
                heatmap, _ = FPDT.run_heatmap_alike_9800(imgA_pnt.to(self.device))   # 128 X 40
                heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB_pnt.to(self.device))
                
                # 有角度图
                intensityA, intensityB = None, None
                pnmapA, pnmapB = None, None
                if self.use_netori and heatmap.shape[0] == 3 and heatmap_B.shape[0] == 3:
                    intensityA = heatmap[1, 3:-3, 2:-2]
                    intensityB = heatmap_B[1, 3:-3, 2:-2]
                    pnmapA = heatmap[2, 3:-3, 2:-2]
                    pnmapB = heatmap_B[2, 3:-3, 2:-2]

                # print(heatmap.shape)
                heatmap = heatmap[0, 3:-3, 2:-2]           # 128 x 40 -> 122 x 36
                heatmap_B = heatmap_B[0, 3:-3, 2:-2]        
                
                if self.partial_filter:
                    heatmap = heatmap * imgA_partial_mask.to(heatmap.device)
                    heatmap_B = heatmap_B * imgB_partial_mask.to(heatmap_B.device)
                    
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 122 x 36
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点
                
                from Match_component import sample_desc_from_points
                # pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                # pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsA = torch.tensor(pts_A.transpose(1, 0), device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0), device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2                        
                else:
                    imgA_pnt, imgB_pnt = sample['img_pnt_o_A'].unsqueeze(0), sample['img_pnt_o_B'].unsqueeze(0)
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    # W_o -= 4    # 40->36
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [122, 36]的中间（118，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]
                
                pntsA_score = pointsA_o[:, 2].float()
                pntsB_score = pointsB_o[:, 2].float()
                pointsA_o = pointsA_o[:, :2].float()
                pointsB_o = pointsB_o[:, :2].float()

                # predict angle
                net_oriA, net_oriB = None, None
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    pointsA_o_normalized = pointsA_o / pointsA_o.new_tensor([W_o - 1, H_o - 1]).to(pointsA_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    pointsB_o_normalized = pointsB_o / pointsB_o.new_tensor([W_o - 1, H_o - 1]).to(pointsB_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_kpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_kpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_kpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_kpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    pn_predictA = (pn_kpA_predict_all > 0.5).float().squeeze()   
                    pn_predictB = (pn_kpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    net_oriA = ori_kpA_predict_all[:, 0] * (2 * pn_predictA - 1) * 90      # 正负90度
                    net_oriB = ori_kpB_predict_all[:, 0] * (2 * pn_predictB - 1) * 90
                    # print(net_oriA.shape)

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                # net_nn_mask
                warped_net_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                net_point_disAB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2]) # N x M

                warped_net_pts_A, net_mask_points = filter_points(warped_net_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                net_key_disAtoB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2])
                net_pos_repeatA_mask, net_pos_repeatB_mask, _ = self.get_point_pair(net_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                if net_pos_repeatA_mask.shape[0] > 0:
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[net_mask_points][net_pos_repeatA_mask]
                    # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, net_pos_repeatB_mask] = 1

                    net_pos_repeatA = pointsA_o[net_mask_points, :][net_pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[net_pos_repeatB_mask, :]

                if self.isdense:
                    net_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)  
                    net_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    if self.use_wbpnt_mask:
                        if net_oriA is None and net_oriB is None:
                            net_desc_A_0, net_oriA, wbpnt_maskA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, net_oriB, wbpnt_maskB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        else:
                            net_desc_A_0, _, wbpnt_maskA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, _, wbpnt_maskB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)                
                    else:
                        wbpnt_maskA, wbpnt_maskB = None, None
                        if net_oriA is None and net_oriB is None:
                            net_desc_A_0, net_oriA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, net_oriB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        else:
                            net_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        
                        # # 测试网络点 + 网络角度 + 网络描述子的一致性
                        # torch.set_printoptions(precision=7)
                        # print(nameA, pointsA_o, net_desc_A_0)
                        # save_path = Path(self.output_dir, nameA)
                        # self.output_txt(save_path, nameA, net_desc_A_0.cpu().numpy(), None)
                        # exit()

                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                
                if self.thr_mode:
                    # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                    if self.is_rec:
                        if self.cat_sift:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                    else:
                        if self.use_wbpnt_mask:
                            if self.use_net_match: 
                                pai_coeff = 3.1415926
                                net_descs_quantA = self.get_quant_desc(net_desc_A)
                                net_descs_quantB = self.get_quant_desc(net_desc_B)
                                net_oriA_short = ((net_oriA + 90) / 180 * pai_coeff * 4096).int().float()     
                                net_oriA_rad = net_oriA_short / 4096
                                net_oriB_short = ((net_oriB + 90) / 180 * pai_coeff * 4096).int().float()     
                                net_oriB_rad = net_oriB_short / 4096
                                dbvalueA = (pntsA_score * self.dbvalue_slope).int().float() / self.dbvalue_slope
                                dbvalueB = (pntsB_score * self.dbvalue_slope).int().float() / self.dbvalue_slope
                                
                                data_match_w = {
                                    'descriptors0_same': net_descs_quantA[wbpnt_maskA, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors1_same': net_descs_quantB[wbpnt_maskB, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors0': net_descs_quantA[wbpnt_maskA, :].transpose(0, 1).unsqueeze(0),
                                    'descriptors1': net_descs_quantB[wbpnt_maskB, :].transpose(0, 1).unsqueeze(0),
                                    'keypoints0': pointsA_o[wbpnt_maskA, :].unsqueeze(0),
                                    'keypoints1': pointsB_o[wbpnt_maskB, :].unsqueeze(0),
                                    'angles0': net_oriA_rad[wbpnt_maskA].unsqueeze(0),
                                    'angles1': net_oriB_rad[wbpnt_maskB].unsqueeze(0),
                                    'dbvalue0': dbvalueA[wbpnt_maskA].unsqueeze(0),
                                    'dbvalue1': dbvalueB[wbpnt_maskB].unsqueeze(0),
                                    'H': H_o,
                                    'W': W_o,
                                }

                                data_match_b = {
                                    'descriptors0_same': net_descs_quantA[~wbpnt_maskA, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors1_same': net_descs_quantB[~wbpnt_maskB, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors0': net_descs_quantA[~wbpnt_maskA, :].transpose(0, 1).unsqueeze(0),
                                    'descriptors1': net_descs_quantB[~wbpnt_maskB, :].transpose(0, 1).unsqueeze(0),
                                    'keypoints0': pointsA_o[~wbpnt_maskA, :].unsqueeze(0),
                                    'keypoints1': pointsB_o[~wbpnt_maskB, :].unsqueeze(0),
                                    'angles0': net_oriA_rad[~wbpnt_maskA].unsqueeze(0),
                                    'angles1': net_oriB_rad[~wbpnt_maskB].unsqueeze(0),
                                    'dbvalue0': dbvalueA[~wbpnt_maskA].unsqueeze(0),
                                    'dbvalue1': dbvalueB[~wbpnt_maskB].unsqueeze(0),
                                    'H': H_o,
                                    'W': W_o,
                                }
                                
                                net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask(FPDT, net_desc_A, net_desc_B, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, net_nn_mask)

                            else:
                                net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(net_desc_A, net_desc_B, wbpnt_maskA, wbpnt_maskB, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(net_desc_A, net_desc_B, net_match_mask)
            
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1

                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                
                net_pointdis_mean = torch.mean(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_var = torch.var(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_std = torch.std(net_point_disAB[(net_match_mask * net_nn_mask) == 1])

                net_ori_diff = self.get_ori_diff(net_oriA, net_oriB, trans_angle)
                net_oridiff_mean = torch.mean(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_var = torch.var(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_std = torch.std(net_ori_diff[(net_match_mask * net_nn_mask) == 1])

                net_hamdis_top30_mean = torch.mean(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_var = torch.var(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_std = torch.std(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
                if net_hanming_dis[(net_match_mask * net_nn_mask) == 1].shape[0] == 0:
                    net_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
                else:
                    net_hamdis_top30max = torch.max(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])

                # 画Net配准图
                if net_require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        # Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8) # 122 x 36 -> 128 x 52
                        Homography_modify = torch.tensor(Homography.astype(np.float32))
                        imgA_o_warped = inv_warp_image(imgA_pnt.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                        img_2D_A = imgA_pnt.numpy().squeeze()
                        img_2D_B = imgB_pnt.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_net_match.bmp'), image_merge)
                
                if net_require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA_pnt.numpy().squeeze(), 'img_H': imgB_pnt.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=3, s=3)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    f = self.output_dir / (nameAB + "_net_line.bmp")
                    cv2.imwrite(str(f), img_pts)

            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            siftnet_oriA, siftnet_oriB = sift_oriA, sift_oriB
            if not self.test_netdesc_flag:
                siftnet_desc_A, siftnet_desc_B = torch.rand(sift_ptsA.shape[0], 256).to(self.device), torch.rand(sift_ptsB.shape[0], 256).to(self.device)
            else:
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    sift_ptsA_normalized = sift_ptsA / sift_ptsA.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsA.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    sift_ptsB_normalized = sift_ptsB / sift_ptsB.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsB.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_siftkpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_siftkpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_siftkpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_siftkpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    sift_pn_predictA = (pn_siftkpA_predict_all > 0.5).float().squeeze()   
                    sift_pn_predictB = (pn_siftkpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    siftnet_oriA = ori_siftkpA_predict_all[:, 0] * (2 * sift_pn_predictA - 1) * 90      # 正负90度
                    siftnet_oriB = ori_siftkpB_predict_all[:, 0] * (2 * sift_pn_predictB - 1) * 90
                    # print(net_oriA.shape)


                if self.isdense:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                    siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    siftnet_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=siftnet_oriA)
                    siftnet_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=siftnet_oriB)

                    # print(siftnet_desc_A_0[0, :]]
                    # torch.set_printoptions(precision=10)
                    # print(siftnet_desc_A_0[8, :])
                    # print((siftnet_desc_A_0 * 100).int()[8, :])
                    # print(sift_ptsA[8, :])
                    # exit()
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                    else:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                else:
                    siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            siftnet_pointdis_mean = torch.mean(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_var = torch.var(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_std = torch.std(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_ori_diff = self.get_ori_diff(siftnet_oriA, siftnet_oriB, trans_angle)
            siftnet_oridiff_mean = torch.mean(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_var = torch.var(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_std = torch.std(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_hamdis_top30_mean = torch.mean(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_var = torch.var(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_std = torch.std(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            if siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                siftnet_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
            else:
                siftnet_hamdis_top30max = torch.max(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])

            # 画Net配准图
            if siftnet_require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8)
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_siftnet_match.bmp'), image_merge)
            
            # 画Sift配准图
            if sift_require_image:
                # mat_H_modify = self.homography_centorcrop(mat_H, -3, -8) # 122x36 -> 128x52
                imgA_o_warped = inv_warp_image(imgA_pnt.squeeze(), torch.from_numpy(mat_H.cpu().numpy().astype(np.float32)))
                img_2D_A = imgA_pnt.numpy().squeeze()
                img_2D_B = imgB_pnt.numpy().squeeze()
                img_2D_warped = imgA_o_warped.numpy().squeeze()
                b = np.zeros_like(img_2D_A)
                g = img_2D_warped * 255  #  旋转后的模板
                r = img_2D_B * 255    
                image_merge = cv2.merge([b, g, r])
                image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_sift_match.bmp'), image_merge)

            # 如果是长条形描述测试sift 0度 128维double
            if self.is_rec and self.test_sift128:
                sift_descA = torch.cat((sift_descA[:, :128], sift_descA[:, :128]), dim=1)
                sift_descB = torch.cat((sift_descB[:, :128], sift_descB[:, :128]), dim=1)

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask]
            # print(torch.sort(sift_min_dis[sift_top30_mask]), sift_pos_repeatA_indices, torch.sort(sift_pos_repeatB_mask))

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            # print(sift_match_indicesA, sift_match_indicesB, sift_min_dis)
            # print(sift_hanmingdist_AtoB[sift_match_mask == 1], torch.where(sift_match_mask == 1)[0], torch.where(sift_match_mask == 1)[1])
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num

            sift_pointdis_mean = torch.mean(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_var = torch.var(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_std = torch.std(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])

            sift_oridiff_mean = torch.mean(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_var = torch.var(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_std = torch.std(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])

            sift_hamdis_top30_mean = torch.mean(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_var = torch.var(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_std = torch.std(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            if sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                sift_hamdis_top30max = torch.mean(torch.tensor([])) 
            else:
                sift_hamdis_top30max  = torch.max(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])

            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item(), 
                                siftnet_pointdis_mean.item(), siftnet_pointdis_var.item(), siftnet_pointdis_std.item(), sift_pointdis_mean.item(), sift_pointdis_var.item(), sift_pointdis_std.item(), net_pointdis_mean.item(), net_pointdis_var.item(), net_pointdis_std.item(),
                                siftnet_oridiff_mean.item(), siftnet_oridiff_var.item(), siftnet_oridiff_std.item(), sift_oridiff_mean.item(), sift_oridiff_var.item(), sift_oridiff_std.item(), net_oridiff_mean.item(), net_oridiff_var.item(), net_oridiff_std.item(),
                                siftnet_hamdis_top30_mean.item(), siftnet_hamdis_top30_var.item(), siftnet_hamdis_top30_std.item(), siftnet_hamdis_top30max.item(), sift_hamdis_top30_mean.item(), sift_hamdis_top30_var.item(), sift_hamdis_top30_std.item(), sift_hamdis_top30max.item(),
                                net_hamdis_top30_mean.item(), net_hamdis_top30_var.item(), net_hamdis_top30_std.item(), net_hamdis_top30max.item()])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'net_pointdis_mean',
                    'net_pointdis_var',
                    'net_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'net_oridiff_mean',
                    'net_oridiff_var',
                    'net_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'net_hamdis_top30_mean',
                    'net_hamdis_top30_var',
                    'net_hamdis_top30_std',
                    'net_hamdis_top30max'
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_oridiff_ALikeWithHard_Ext93_Pnt_Match_wbSimi(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 130
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.bord           = 2

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.want_cand_num  = 30
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True

        self.isdense        = False # False
        self.has45          = False # True

        self.partial_filter = True
        self.use_netori     = True # True

        # match
        self.use_wbpnt_mask = True
        self.match_thr      = 0 # 0.2
        self.use_net_match  = True
        self.use_desc_all   = True # False
        self.use_post_process = True
        self.use_old_ransac = False
        self.dbvalue_slope  = -3500
        self.printImg_wb_thr = 0.6

        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.slope          = 5000
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        self.test_netdesc_flag = False # True
        self.siftq           = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        
        # 6193_DK7_merge_test1_9800
        # 6193_DK7_XA_rot_test_9800
        # 6193_DK7_merge_test2_9800
        # 6193-DK7-160-8-normal-nocontrol_test_9800
        # 6193-DK4-130-purple-suppress-SNR28_9800
        # 6193-DK4-110-tarnish-normal-SNR80_9800
        # 6193-DK7-scene_normal_9800
        
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1_9800/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (2, 2, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input


    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans    
        half_dim =  wht_desc_a.shape[1] // 2
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_des_hanmingdis_wht_permute_rec_sift_256(self, des_a, des_b):
        half_dim =  des_a.shape[1] // 2 
        desc_binary_a = des_a  # Nxdim
        desc_binary_b = des_b
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.sift_index] @ desc_binary_b[:, self.sift_index].t() - (1 - desc_binary_a[:, self.sift_index]) @ (1 - desc_binary_b[:, self.sift_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.sift_index==False] @ desc_binary_b[:, self.sift_index==False].t() - (1 - desc_binary_a[:, self.sift_index==False]) @ (1 - desc_binary_b[:, self.sift_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end


    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(self, desA, desB, wbpnt_maskA, wbpnt_maskB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        
        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesA_w], all_indexesA[~wbpnt_maskA][match_indicesA_b]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # min_dis = torch.cat((min_dis_w, min_dis_b), dim=0)
        
        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesA_b], all_indexesA[wbpnt_maskA][match_indicesA_w]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        min_dis = torch.cat((min_dis_b, min_dis_w), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= self.want_cand_num:
            # Top30
            # 保证topk后的indexes是从小到大的
            min_dis_new = min_dis + torch.tensor(range(min_dis.shape[0])).to(min_dis.device) / min_dis.shape[0]
            top30_mask = torch.topk(min_dis_new, self.want_cand_num, largest=False).indices  
    
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    

        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask(self, FPDT, desA, desB, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, repeat_mask):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        # mask1 = hanming_distAtoB_part_begin < th1
        # mask2 = hadamadist_AtoB < th2
        # hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        # hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        pred_match_w = FPDT.detector_net.get_match_predict(data_match_w).unsqueeze(0)
        pred_match_b = FPDT.detector_net.get_match_predict(data_match_b).unsqueeze(0)
        match_mess_w =self.find_match_idx_score(pred_match_w, match_thr=self.match_thr)
        match_mess_b =self.find_match_idx_score(pred_match_b, match_thr=self.match_thr)
        match_indicesB_w_all = match_mess_w['matches0'].squeeze()
        match_indicesB_b_all = match_mess_b['matches0'].squeeze()
        match_scoresB_w_all = match_mess_w['matching_scores0'].squeeze()
        match_scoresB_b_all = match_mess_b['matching_scores0'].squeeze()
        match_indicesB_w = match_indicesB_w_all[match_indicesB_w_all != -1]
        match_indicesB_b = match_indicesB_b_all[match_indicesB_b_all != -1]
        
        # match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        # match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)

        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1], all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # max_sim = torch.cat((match_scoresB_w_all[match_indicesB_w_all != -1], match_scoresB_b_all[match_indicesB_b_all != -1]), dim=0)

        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1], all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        max_sim = torch.cat((match_scoresB_b_all[match_indicesB_b_all != -1], match_scoresB_w_all[match_indicesB_w_all != -1]), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)

        if max_sim.shape[0] >= self.want_cand_num:
            # Top30
            # # 保证topk后的indexes是从小到大的(只限于整形的距离)
            # max_sim_new = max_sim - torch.tensor(range(max_sim.shape[0])).to(max_sim.device) / max_sim.shape[0]
            # top30_mask = torch.topk(max_sim_new, 30, largest=True).indices  # 取相似度最大前三十个点对
            
            # 使用numpy argsort里稳定的归并排序(默认从小到大)
            top30_mask = np.argsort(-max_sim.cpu().numpy(), kind='mergesort')[:self.want_cand_num]
            top30_mask = torch.from_numpy(top30_mask).to(max_sim.device)
            
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = desA[:, 128:].float(), desB[:, 128:].float()
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_rec_sift_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                # print(model.params[:2])
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None


    def check_inlier_angle(self, nearest_matches, nearest_matches_angle, model, inliers):
        
        if inliers is None:
            return np.array([]), np.array([])
        
        inliers_matches = nearest_matches[inliers.nonzero()[0]]   #内点对
        inliers_matches_angle = nearest_matches_angle[inliers.nonzero()[0]]  #内点对的角度
        
        pi_coef = 3.1415926
        diff = np.zeros([inliers_matches_angle.shape[0], 4])
        
        diff[:,0] = -model.rotation - inliers_matches_angle[:,0] + inliers_matches_angle[:,1] #[-2pi,2pi]
        #print(model.rotation, inliers_matches_angle[:,0] - inliers_matches_angle[:,1])
        mask = diff[:,0] < 0
        diff[mask,0] = diff[mask,0] + 2*pi_coef  #[0,2pi]
        diff[:,1] = 2*pi_coef - diff[:,0]  #[0,2pi]
        
        diff[:,2] = -model.rotation - inliers_matches_angle[:,0] + inliers_matches_angle[:,1] + pi_coef #避免0,180干扰 [-pi,3pi]
        mask = diff[:,2] < 0
        diff[mask,2] = diff[mask,2] + 2*pi_coef  #[0,3pi]
        mask = diff[:,2] > 2*pi_coef
        diff[mask,2] = diff[mask,2] - 2*pi_coef  #[0,2pi]
        diff[:,3] = 2*pi_coef - diff[:,2]  #[0,2pi]
        
        diff_c = np.min(diff,axis = 1)  #  [0, pi]
        #print(diff_c)
        inliers_matches_mask = diff_c < pi_coef/12  #误差不超过+-15度(model.rotation [-pi,pi])
        #print(np.sum(inliers),np.sum(inliers_matches_mask),  model.rotation)
        #print(np.int32(inliers_matches_mask))
        return inliers_matches[inliers_matches_mask], inliers_matches_mask

    def ransac_project(self, matched_left, matched_right, dist_list_return, angle_left, angle_right, residual_threshold=2.5):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac_project import slransac_project
            # model, inliers = slransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            model, inliers, best_inlier_residuals_sum = slransac_project((matched_left, matched_right), AffineTransform, min_samples=3, residual_threshold=residual_threshold,
                          max_trials=500, stop_sample_num= 22, random_state=2000, weight_pairs=None, angles = (angle_left, angle_right))

            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None

            # 重拟合,避免有3个内点计算的trans导致计
            data_inliers = [matched_left[inliers], matched_right[inliers]]
            model.estimate(*data_inliers)
            # mid_model_residuals = np.abs(model.residuals(*data_inliers))
            # # consensus set / inliers
            # mid_inliers = mid_model_residuals < residual_threshold

            nearest_matches = np.concatenate(((matched_right, matched_left)), axis=-1)
            nearest_matches_angle = np.concatenate(((angle_right[:, None], angle_left[:, None])), axis=-1)
            # 方向一致性过滤
            inliers_matches, inliers_matches_mask = self.check_inlier_angle(nearest_matches, nearest_matches_angle, model, inliers)
            final_inliers = inliers_matches_mask

            # matches_idx = np.array([])
            # if inliers is not None and inliers_matches_mask.shape[0] > 0:
            #     matches_idx = nearest_topk_idx[inliers][inliers_matches_mask]    # 通过角度卡控后的内点索引 top_k

            # estimate final model using all inliers
            if inliers_matches.shape[0] >= 4 and best_inlier_residuals_sum > 16384./256/256 * np.sum(inliers):    #用所有点重新拟合
                # select inliers for each data array
                data_inliers = [inliers_matches[:,2:4], inliers_matches[:,0:2]]
                model.estimate(*data_inliers)

                final_model_residuals = np.abs(model.residuals(*data_inliers))
                # consensus set / inliers
                final_inliers = final_model_residuals < residual_threshold

            if model is None:
                return None, None
            else:
                # print(model.params[:2])
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, final_inliers
        else:
            return None, None


    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()

    def generate_int_desc(self, desc):
        desc_select_thr = self.thresholding_desc(torch.round(desc * self.slope) + 5000)
        # 量化
        desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())
        # desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)
        return desc_select_quantize

    def get_quant_desc(self, desc):
        # hadama量化
        descs_quant_0 = self.generate_int_desc(desc[:, :128])
        descs_quant_45 = self.generate_int_desc(desc[:, 128:])
        descs_quant_0_same, descs_quant_0_neg = descs_quant_0[:, self.net_index], descs_quant_0[:, self.net_index==False]
        descs_quant_45_same, descs_quant_45_neg = descs_quant_45[:, self.net_index], descs_quant_45[:, self.net_index==False]

        if self.use_desc_all:
            descs_quant = torch.cat((descs_quant_0_same, descs_quant_45_same, descs_quant_0_neg, descs_quant_45_neg), dim=-1)
        else:
            descs_quant = torch.cat((descs_quant_0_same, descs_quant_45_same, descs_quant_0_same, descs_quant_45_same), dim=-1)
        
        if self.use_post_process:
            descs_quant = descs_quant * 2 - 1

        return descs_quant.float()

    def arange_like(self, x, dim: int):
        return x.new_ones(x.shape[dim]).cumsum(0) - 1  # traceable in 1.1

    def find_match_idx_score(self, scores, match_thr=0.2):
        # Get the matches with score above "match_threshold".
        max0, max1 = scores.max(2), scores.max(1)
        indices0, indices1 = max0.indices, max1.indices
        mutual0 = self.arange_like(indices0, 1)[None] == indices1.gather(1, indices0)  #当二者互为匹配点时才认为是匹配点
        mutual1 = self.arange_like(indices1, 1)[None] == indices0.gather(1, indices1)
        zero = scores.new_tensor(0)
        mscores0 = torch.where(mutual0, max0.values.exp(), zero)
        mscores1 = torch.where(mutual1, mscores0.gather(1, indices1), zero)
        valid0 = mutual0 & (mscores0 > match_thr)
        valid1 = mutual1 & valid0.gather(1, indices1)
        indices0 = torch.where(valid0, indices0, indices0.new_tensor(-1))
        indices1 = torch.where(valid1, indices1, indices1.new_tensor(-1))
        
        return {
                'matches0': indices0, # use -1 for invalid match
                'matches1': indices1, # use -1 for invalid match
                'matching_scores0': mscores0,
                'matching_scores1': mscores1
            }

    def compute_simility_bin(self, imgA, imgB, imgA_pm, imgB_pm, model, w=36, h=122, bin_thr=-1):
        # # 计算黑白相似度
        if bin_thr <= 0:
            # 均值卡控
            img_p_bin = (np.array(imgB) > np.mean(np.array(imgB)))
        else:
            # 阈值卡控
            img_p_bin = (np.array(imgB) > bin_thr)
        # print(h, w)
        np_a = cv2.warpAffine(np.array(imgA), model[:2, :], (w, h), flags=cv2.INTER_CUBIC)
        img_a_bin = (np.array(np_a) > np.mean(np.array(imgA))) if bin_thr <= 0 else (np.array(np_a) > bin_thr)
        
        np_flag = np.ones(img_a_bin.shape)
        np_flag_a = cv2.warpAffine(np_flag, model[:2, :], (w, h), flags=cv2.INTER_CUBIC)

        valid_mask = cv2.warpAffine(np.array(imgA_pm), model[:2, :], (w, h), flags=cv2.INTER_CUBIC) * 250 * np.array(imgB_pm)
        np_flag_a = np_flag_a*250
        sam_num = 0
        total_num=0
        total_area=w*h
        # # 原版本
        # for i in range(h):
        #     for j in range(w):
        #         if valid_mask[i, j] > 10: # np_flag_a[i,j]>10:
        #             total_num+=1
        #         if valid_mask[i, j] > 10: # np_flag_a[i,j]>10:
        #             if img_a_bin[i,j]==img_p_bin[i,j]:
        #                 sam_num+=1   
        # print(sam_num, total_num)
        # 并行
        total_num = np.sum(valid_mask > 10)
        sam_num = np.sum((valid_mask > 10) * (img_a_bin == img_p_bin))

        if total_num!=0:
            simility = sam_num / total_num
        else:
            simility = 0
        return simility, sam_num, total_num, total_num / total_area

    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        net_require_image = True   # True
        sift_require_image = True
        siftnet_require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s_pnt(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            imgA_pnt, imgB_pnt = sample["imgA_pnt"].unsqueeze(0), sample["imgB_pnt"].unsqueeze(0)
            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            # sift点
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isDilation:
                W_o -= 4            # 122 x 40 -> 122 x 36

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            
            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1

            # sift描述
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.test_netp_flag:
                '''Tradition'''
                heatmap, _ = FPDT.run_heatmap_alike_9800(imgA_pnt.to(self.device))   # 128 X 40
                heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB_pnt.to(self.device))
                
                # 有角度图
                intensityA, intensityB = None, None
                pnmapA, pnmapB = None, None
                if self.use_netori and heatmap.shape[0] == 3 and heatmap_B.shape[0] == 3:
                    intensityA = heatmap[1, 3:-3, 2:-2]
                    intensityB = heatmap_B[1, 3:-3, 2:-2]
                    pnmapA = heatmap[2, 3:-3, 2:-2]
                    pnmapB = heatmap_B[2, 3:-3, 2:-2]

                # print(heatmap.shape)
                heatmap = heatmap[0, 3:-3, 2:-2]           # 128 x 40 -> 122 x 36
                heatmap_B = heatmap_B[0, 3:-3, 2:-2]        
                
                if self.partial_filter:
                    heatmap = heatmap * imgA_partial_mask.to(heatmap.device)
                    heatmap_B = heatmap_B * imgB_partial_mask.to(heatmap_B.device)
                    
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 122 x 36
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点
                
                from Match_component import sample_desc_from_points
                # pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                # pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsA = torch.tensor(pts_A.transpose(1, 0), device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0), device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2                        
                else:
                    imgA_pnt, imgB_pnt = sample['img_pnt_o_A'].unsqueeze(0), sample['img_pnt_o_B'].unsqueeze(0)
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    # W_o -= 4    # 40->36
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [122, 36]的中间（118，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]
                
                pntsA_score = pointsA_o[:, 2].float()
                pntsB_score = pointsB_o[:, 2].float()
                pointsA_o = pointsA_o[:, :2].float()
                pointsB_o = pointsB_o[:, :2].float()

                # predict angle
                net_oriA, net_oriB = None, None
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    pointsA_o_normalized = pointsA_o / pointsA_o.new_tensor([W_o - 1, H_o - 1]).to(pointsA_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    pointsB_o_normalized = pointsB_o / pointsB_o.new_tensor([W_o - 1, H_o - 1]).to(pointsB_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_kpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_kpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_kpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_kpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    pn_predictA = (pn_kpA_predict_all > 0.5).float().squeeze()   
                    pn_predictB = (pn_kpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    net_oriA = ori_kpA_predict_all[:, 0] * (2 * pn_predictA - 1) * 90      # 正负90度
                    net_oriB = ori_kpB_predict_all[:, 0] * (2 * pn_predictB - 1) * 90
                    # print(net_oriA.shape)

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                # net_nn_mask
                warped_net_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                net_point_disAB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2]) # N x M

                warped_net_pts_A, net_mask_points = filter_points(warped_net_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                net_key_disAtoB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2])
                net_pos_repeatA_mask, net_pos_repeatB_mask, _ = self.get_point_pair(net_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                if net_pos_repeatA_mask.shape[0] > 0:
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[net_mask_points][net_pos_repeatA_mask]
                    # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, net_pos_repeatB_mask] = 1

                    net_pos_repeatA = pointsA_o[net_mask_points, :][net_pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[net_pos_repeatB_mask, :]

                if self.isdense:
                    net_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)  
                    net_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    if self.use_wbpnt_mask:
                        if net_oriA is None and net_oriB is None:
                            net_desc_A_0, net_oriA, wbpnt_maskA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, net_oriB, wbpnt_maskB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        else:
                            net_desc_A_0, _, wbpnt_maskA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, _, wbpnt_maskB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)                
                    else:
                        wbpnt_maskA, wbpnt_maskB = None, None
                        if net_oriA is None and net_oriB is None:
                            net_desc_A_0, net_oriA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, net_oriB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        else:
                            net_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        
                        # # 测试网络点 + 网络角度 + 网络描述子的一致性
                        # torch.set_printoptions(precision=7)
                        # print(nameA, pointsA_o, net_desc_A_0)
                        # save_path = Path(self.output_dir, nameA)
                        # self.output_txt(save_path, nameA, net_desc_A_0.cpu().numpy(), None)
                        # exit()

                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                
                pai_coeff = 3.1415926
                
                net_oriA_short = ((net_oriA + 90) / 180 * pai_coeff * 4096).int().float()     
                net_oriA_rad = net_oriA_short / 4096

                net_oriB_short = ((net_oriB + 90) / 180 * pai_coeff * 4096).int().float()     
                net_oriB_rad = net_oriB_short / 4096

                if self.thr_mode:
                    # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                    if self.is_rec:
                        if self.cat_sift:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                    else:
                        if self.use_wbpnt_mask:
                            if self.use_net_match: 

                                net_descs_quantA = self.get_quant_desc(net_desc_A)
                                net_descs_quantB = self.get_quant_desc(net_desc_B)

                                dbvalueA = (pntsA_score * self.dbvalue_slope).int().float() / self.dbvalue_slope
                                dbvalueB = (pntsB_score * self.dbvalue_slope).int().float() / self.dbvalue_slope
                                
                                data_match_w = {
                                    'descriptors0_same': net_descs_quantA[wbpnt_maskA, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors1_same': net_descs_quantB[wbpnt_maskB, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors0': net_descs_quantA[wbpnt_maskA, :].transpose(0, 1).unsqueeze(0),
                                    'descriptors1': net_descs_quantB[wbpnt_maskB, :].transpose(0, 1).unsqueeze(0),
                                    'keypoints0': pointsA_o[wbpnt_maskA, :].unsqueeze(0),
                                    'keypoints1': pointsB_o[wbpnt_maskB, :].unsqueeze(0),
                                    'angles0': net_oriA_rad[wbpnt_maskA].unsqueeze(0),
                                    'angles1': net_oriB_rad[wbpnt_maskB].unsqueeze(0),
                                    'dbvalue0': dbvalueA[wbpnt_maskA].unsqueeze(0),
                                    'dbvalue1': dbvalueB[wbpnt_maskB].unsqueeze(0),
                                    'H': H_o,
                                    'W': W_o,
                                }

                                data_match_b = {
                                    'descriptors0_same': net_descs_quantA[~wbpnt_maskA, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors1_same': net_descs_quantB[~wbpnt_maskB, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors0': net_descs_quantA[~wbpnt_maskA, :].transpose(0, 1).unsqueeze(0),
                                    'descriptors1': net_descs_quantB[~wbpnt_maskB, :].transpose(0, 1).unsqueeze(0),
                                    'keypoints0': pointsA_o[~wbpnt_maskA, :].unsqueeze(0),
                                    'keypoints1': pointsB_o[~wbpnt_maskB, :].unsqueeze(0),
                                    'angles0': net_oriA_rad[~wbpnt_maskA].unsqueeze(0),
                                    'angles1': net_oriB_rad[~wbpnt_maskB].unsqueeze(0),
                                    'dbvalue0': dbvalueA[~wbpnt_maskA].unsqueeze(0),
                                    'dbvalue1': dbvalueB[~wbpnt_maskB].unsqueeze(0),
                                    'H': H_o,
                                    'W': W_o,
                                }

                                net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask(FPDT, net_desc_A, net_desc_B, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, net_nn_mask)

                            else:
                                net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(net_desc_A, net_desc_B, wbpnt_maskA, wbpnt_maskB, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(net_desc_A, net_desc_B, net_match_mask)
            
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                net_oriA_ransac = (net_oriA_rad - pai_coeff / 2)[net_match_indicesA].cpu()
                net_oriB_ransac = (net_oriB_rad - pai_coeff / 2)[net_match_indicesB].cpu()
                
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1

                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                
                net_pointdis_mean = torch.mean(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_var = torch.var(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_std = torch.std(net_point_disAB[(net_match_mask * net_nn_mask) == 1])

                net_ori_diff = self.get_ori_diff(net_oriA, net_oriB, trans_angle)
                net_oridiff_mean = torch.mean(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_var = torch.var(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_std = torch.std(net_ori_diff[(net_match_mask * net_nn_mask) == 1])

                net_hamdis_top30_mean = torch.mean(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_var = torch.var(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_std = torch.std(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
                if net_hanming_dis[(net_match_mask * net_nn_mask) == 1].shape[0] == 0:
                    net_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
                else:
                    net_hamdis_top30max = torch.max(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])

                net_wb_simility, net_wb_sam_num, net_repeat_area_num, net_repeat_area = 0, 0, 0, 0
                net_inliers_num, net_inliers_ratio = 0, 0
                # 画Net配准图
                if net_require_image:
                    if self.use_old_ransac:
                        Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    else:
                        Homography, inliers = self.ransac_project(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None, 
                                                angle_left=net_oriA_ransac, angle_right=net_oriB_ransac)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        # Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8) # 122 x 36 -> 128 x 52
                        Homography_modify = torch.tensor(Homography.astype(np.float32))
                        imgA_o_warped = inv_warp_image(imgA_pnt.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                        img_2D_A = imgA_pnt.numpy().squeeze()
                        img_2D_B = imgB_pnt.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        # cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_net_match.bmp'), image_merge)

                        # print(imgA_simi.shape)
                        net_wb_simility, net_wb_sam_num, net_repeat_area_num, net_repeat_area = self.compute_simility_bin(imgA_pnt.squeeze(), imgB_pnt.squeeze(), 
                                                                                                                        imgA_partial_mask.squeeze(), imgB_partial_mask.squeeze(), 
                                                                                                                        Homography, 
                                                                                                                        w=imgA_pnt.squeeze().shape[1], h=imgA_pnt.squeeze().shape[0], 
                                                                                                                        bin_thr=-1)
                        if net_wb_simility < self.printImg_wb_thr:
                            cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_net_match_' + "{:.4f}".format(net_wb_simility) + '.bmp'), image_merge)
                                                                                                                                        

                        net_inliers_num = np.sum(inliers)
                        net_inliers_ratio = net_inliers_num / net_cand_num
                
                if net_require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA_pnt.numpy().squeeze(), 'img_H': imgB_pnt.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=3, s=3)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    if net_wb_simility < self.printImg_wb_thr:
                        f = self.output_dir / (nameAB + "_net_line_" + "{:.4f}".format(net_wb_simility) +  ".bmp")
                        cv2.imwrite(str(f), img_pts)
                    pass

            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            siftnet_oriA, siftnet_oriB = sift_oriA, sift_oriB
            if not self.test_netdesc_flag:
                siftnet_desc_A, siftnet_desc_B = torch.rand(sift_ptsA.shape[0], 256).to(self.device), torch.rand(sift_ptsB.shape[0], 256).to(self.device)
            else:
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    sift_ptsA_normalized = sift_ptsA / sift_ptsA.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsA.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    sift_ptsB_normalized = sift_ptsB / sift_ptsB.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsB.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_siftkpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_siftkpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_siftkpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_siftkpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    sift_pn_predictA = (pn_siftkpA_predict_all > 0.5).float().squeeze()   
                    sift_pn_predictB = (pn_siftkpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    siftnet_oriA = ori_siftkpA_predict_all[:, 0] * (2 * sift_pn_predictA - 1) * 90      # 正负90度
                    siftnet_oriB = ori_siftkpB_predict_all[:, 0] * (2 * sift_pn_predictB - 1) * 90
                    # print(net_oriA.shape)


                if self.isdense:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                    siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    siftnet_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=siftnet_oriA)
                    siftnet_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=siftnet_oriB)

                    # print(siftnet_desc_A_0[0, :]]
                    # torch.set_printoptions(precision=10)
                    # print(siftnet_desc_A_0[8, :])
                    # print((siftnet_desc_A_0 * 100).int()[8, :])
                    # print(sift_ptsA[8, :])
                    # exit()
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                    else:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                else:
                    siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            siftnet_pointdis_mean = torch.mean(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_var = torch.var(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_std = torch.std(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_ori_diff = self.get_ori_diff(siftnet_oriA, siftnet_oriB, trans_angle)
            siftnet_oridiff_mean = torch.mean(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_var = torch.var(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_std = torch.std(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_hamdis_top30_mean = torch.mean(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_var = torch.var(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_std = torch.std(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            if siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                siftnet_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
            else:
                siftnet_hamdis_top30max = torch.max(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])

            # 画Net配准图
            if siftnet_require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8)
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_siftnet_match.bmp'), image_merge)
            
            # 画Sift配准图
            if sift_require_image:
                # mat_H_modify = self.homography_centorcrop(mat_H, -3, -8) # 122x36 -> 128x52
                imgA_o_warped = inv_warp_image(imgA_pnt.squeeze(), torch.from_numpy(mat_H.cpu().numpy().astype(np.float32)))
                img_2D_A = imgA_pnt.numpy().squeeze()
                img_2D_B = imgB_pnt.numpy().squeeze()
                img_2D_warped = imgA_o_warped.numpy().squeeze()
                b = np.zeros_like(img_2D_A)
                g = img_2D_warped * 255  #  旋转后的模板
                r = img_2D_B * 255    
                image_merge = cv2.merge([b, g, r])
                image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                if net_wb_simility < self.printImg_wb_thr:
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_sift_match.bmp'), image_merge)

            # 如果是长条形描述测试sift 0度 128维double
            if self.is_rec and self.test_sift128:
                sift_descA = torch.cat((sift_descA[:, :128], sift_descA[:, :128]), dim=1)
                sift_descB = torch.cat((sift_descB[:, :128], sift_descB[:, :128]), dim=1)

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask]
            # print(torch.sort(sift_min_dis[sift_top30_mask]), sift_pos_repeatA_indices, torch.sort(sift_pos_repeatB_mask))

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            # print(sift_match_indicesA, sift_match_indicesB, sift_min_dis)
            # print(sift_hanmingdist_AtoB[sift_match_mask == 1], torch.where(sift_match_mask == 1)[0], torch.where(sift_match_mask == 1)[1])
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num

            sift_pointdis_mean = torch.mean(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_var = torch.var(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_std = torch.std(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])

            sift_oridiff_mean = torch.mean(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_var = torch.var(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_std = torch.std(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])

            sift_hamdis_top30_mean = torch.mean(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_var = torch.var(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_std = torch.std(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            if sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                sift_hamdis_top30max = torch.mean(torch.tensor([])) 
            else:
                sift_hamdis_top30max  = torch.max(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])

            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item(), 
                                siftnet_pointdis_mean.item(), siftnet_pointdis_var.item(), siftnet_pointdis_std.item(), sift_pointdis_mean.item(), sift_pointdis_var.item(), sift_pointdis_std.item(), net_pointdis_mean.item(), net_pointdis_var.item(), net_pointdis_std.item(),
                                siftnet_oridiff_mean.item(), siftnet_oridiff_var.item(), siftnet_oridiff_std.item(), sift_oridiff_mean.item(), sift_oridiff_var.item(), sift_oridiff_std.item(), net_oridiff_mean.item(), net_oridiff_var.item(), net_oridiff_std.item(),
                                siftnet_hamdis_top30_mean.item(), siftnet_hamdis_top30_var.item(), siftnet_hamdis_top30_std.item(), siftnet_hamdis_top30max.item(), sift_hamdis_top30_mean.item(), sift_hamdis_top30_var.item(), sift_hamdis_top30_std.item(), sift_hamdis_top30max.item(),
                                net_hamdis_top30_mean.item(), net_hamdis_top30_var.item(), net_hamdis_top30_std.item(), net_hamdis_top30max.item(), net_wb_sam_num, net_wb_simility, net_repeat_area_num, net_repeat_area, net_inliers_num, net_inliers_ratio])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'net_pointdis_mean',
                    'net_pointdis_var',
                    'net_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'net_oridiff_mean',
                    'net_oridiff_var',
                    'net_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'net_hamdis_top30_mean',
                    'net_hamdis_top30_var',
                    'net_hamdis_top30_std',
                    'net_hamdis_top30max',
                    'net_wb_sam_num',
                    'net_wb_simility', 
                    'net_repeat_area_num', 
                    'net_repeat_area',
                    'net_inliers_num', 
                    'net_inliers_ratio',
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_oridiff_ALikeWithHard_Ext93_Pnt_Match_wbSimi_score(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 130
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.bord           = 2

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.want_cand_num  = 30
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True

        self.isdense        = False # False
        self.has45          = False # True

        self.partial_filter = True
        self.use_netori     = True # True

        # match
        self.use_wbpnt_mask = True
        self.match_thr      = 0 # 0.2
        self.use_net_match  = True
        self.use_desc_all   = True # False
        self.use_post_process = True
        self.use_old_ransac = False
        self.dbvalue_slope  = -3500
        self.printImg_wb_thr = 0.6

        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.slope          = 5000
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        self.test_netdesc_flag = False # True
        self.siftq           = True
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        
        # 6193_DK7_merge_test1_9800
        # 6193_DK7_XA_rot_test_9800
        # 6193_DK7_merge_test2_9800
        # 6193-DK7-160-8-normal-nocontrol_test_9800
        # 6193-DK4-130-purple-suppress-SNR28_9800
        # 6193-DK4-110-tarnish-normal-SNR80_9800
        # 6193-DK7-scene_normal_9800
        
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193_DK7_merge_test1_9800/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%.7f', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (2, 2, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input


    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans    
        half_dim =  wht_desc_a.shape[1] // 2
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_des_hanmingdis_wht_permute_rec_sift_256(self, des_a, des_b):
        half_dim =  des_a.shape[1] // 2 
        desc_binary_a = des_a  # Nxdim
        desc_binary_b = des_b
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.sift_index] @ desc_binary_b[:, self.sift_index].t() - (1 - desc_binary_a[:, self.sift_index]) @ (1 - desc_binary_b[:, self.sift_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.sift_index==False] @ desc_binary_b[:, self.sift_index==False].t() - (1 - desc_binary_a[:, self.sift_index==False]) @ (1 - desc_binary_b[:, self.sift_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end


    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(self, desA, desB, wbpnt_maskA, wbpnt_maskB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        
        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesA_w], all_indexesA[~wbpnt_maskA][match_indicesA_b]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # min_dis = torch.cat((min_dis_w, min_dis_b), dim=0)
        
        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesA_b], all_indexesA[wbpnt_maskA][match_indicesA_w]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        min_dis = torch.cat((min_dis_b, min_dis_w), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= self.want_cand_num:
            # Top30
            # 保证topk后的indexes是从小到大的
            min_dis_new = min_dis + torch.tensor(range(min_dis.shape[0])).to(min_dis.device) / min_dis.shape[0]
            top30_mask = torch.topk(min_dis_new, self.want_cand_num, largest=False).indices  
    
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    

        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask(self, FPDT, desA, desB, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, repeat_mask):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        # mask1 = hanming_distAtoB_part_begin < th1
        # mask2 = hadamadist_AtoB < th2
        # hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        # hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        pred_match_w = FPDT.detector_net.get_match_predict(data_match_w).unsqueeze(0)
        pred_match_b = FPDT.detector_net.get_match_predict(data_match_b).unsqueeze(0)
        match_mess_w =self.find_match_idx_score(pred_match_w, match_thr=self.match_thr)
        match_mess_b =self.find_match_idx_score(pred_match_b, match_thr=self.match_thr)
        match_indicesB_w_all = match_mess_w['matches0'].squeeze()
        match_indicesB_b_all = match_mess_b['matches0'].squeeze()
        match_scoresB_w_all = match_mess_w['matching_scores0'].squeeze()
        match_scoresB_b_all = match_mess_b['matching_scores0'].squeeze()
        match_indicesB_w = match_indicesB_w_all[match_indicesB_w_all != -1]
        match_indicesB_b = match_indicesB_b_all[match_indicesB_b_all != -1]
        
        # match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        # match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)

        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1], all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # max_sim = torch.cat((match_scoresB_w_all[match_indicesB_w_all != -1], match_scoresB_b_all[match_indicesB_b_all != -1]), dim=0)

        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1], all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        max_sim = torch.cat((match_scoresB_b_all[match_indicesB_b_all != -1], match_scoresB_w_all[match_indicesB_w_all != -1]), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)

        if max_sim.shape[0] >= self.want_cand_num:
            # Top30
            # # 保证topk后的indexes是从小到大的(只限于整形的距离)
            # max_sim_new = max_sim - torch.tensor(range(max_sim.shape[0])).to(max_sim.device) / max_sim.shape[0]
            # top30_mask = torch.topk(max_sim_new, 30, largest=True).indices  # 取相似度最大前三十个点对
            
            # 使用numpy argsort里稳定的归并排序(默认从小到大)
            top30_mask = np.argsort(-max_sim.cpu().numpy(), kind='mergesort')[:self.want_cand_num]
            top30_mask = torch.from_numpy(top30_mask).to(max_sim.device)
            
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores(self, FPDT, desA, desB, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, repeat_mask):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        # mask1 = hanming_distAtoB_part_begin < th1
        # mask2 = hadamadist_AtoB < th2
        # hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        # hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        # pred_match_w = FPDT.detector_net.get_match_predict(data_match_w).unsqueeze(0)
        # pred_match_b = FPDT.detector_net.get_match_predict(data_match_b).unsqueeze(0)
        pred_match_w_all = FPDT.detector_net.get_match_predict_all(data_match_w)
        pred_match_b_all = FPDT.detector_net.get_match_predict_all(data_match_b)
        pred_match_w = pred_match_w_all[:-1, :-1].unsqueeze(0)
        pred_match_b = pred_match_b_all[:-1, :-1].unsqueeze(0)
        match_mess_w =self.find_match_idx_score(pred_match_w, match_thr=self.match_thr)
        match_mess_b =self.find_match_idx_score(pred_match_b, match_thr=self.match_thr)
        match_indicesB_w_all = match_mess_w['matches0'].squeeze()
        match_indicesB_b_all = match_mess_b['matches0'].squeeze()
        match_scoresB_w_all = match_mess_w['matching_scores0'].squeeze()
        match_scoresB_b_all = match_mess_b['matching_scores0'].squeeze()
        match_indicesB_w = match_indicesB_w_all[match_indicesB_w_all != -1]
        match_indicesB_b = match_indicesB_b_all[match_indicesB_b_all != -1]

        # match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        # match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)

        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1], all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # max_sim = torch.cat((match_scoresB_w_all[match_indicesB_w_all != -1], match_scoresB_b_all[match_indicesB_b_all != -1]), dim=0)

        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1], all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        max_sim = torch.cat((match_scoresB_b_all[match_indicesB_b_all != -1], match_scoresB_w_all[match_indicesB_w_all != -1]), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)

        if max_sim.shape[0] >= self.want_cand_num:
            # Top30
            # # 保证topk后的indexes是从小到大的(只限于整形的距离)
            # max_sim_new = max_sim - torch.tensor(range(max_sim.shape[0])).to(max_sim.device) / max_sim.shape[0]
            # top30_mask = torch.topk(max_sim_new, 30, largest=True).indices  # 取相似度最大前三十个点对
            
            # 使用numpy argsort里稳定的归并排序(默认从小到大)
            top30_mask = np.argsort(-max_sim.cpu().numpy(), kind='mergesort')[:self.want_cand_num]
            top30_mask = torch.from_numpy(top30_mask).to(max_sim.device)
            
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        
        # 计算match点bin mask
        repeat_bin_maskA = torch.sum(repeat_mask, dim=-1) > 0
        repeat_bin_maskB = torch.sum(repeat_mask, dim=0) > 0
        exp_pred_match_w_all = torch.exp(pred_match_w_all)
        exp_pred_match_b_all = torch.exp(pred_match_b_all)

        match_binA_w = exp_pred_match_w_all[:-1, -1][repeat_bin_maskA[wbpnt_maskA].bool()]
        match_binA_b = exp_pred_match_b_all[:-1, -1][repeat_bin_maskA[~wbpnt_maskA].bool()]
        unmatch_binA_w = exp_pred_match_w_all[:-1, -1][~(repeat_bin_maskA[wbpnt_maskA].bool())]
        unmatch_binA_b = exp_pred_match_b_all[:-1, -1][~(repeat_bin_maskA[~wbpnt_maskA].bool())]
        match_binB_w = exp_pred_match_w_all[-1, :-1][repeat_bin_maskB[wbpnt_maskB].bool()]
        match_binB_b = exp_pred_match_b_all[-1, :-1][repeat_bin_maskB[~wbpnt_maskB].bool()]
        unmatch_binB_w = exp_pred_match_w_all[-1, :-1][~(repeat_bin_maskB[wbpnt_maskB].bool())]
        unmatch_binB_b = exp_pred_match_b_all[-1, :-1][~(repeat_bin_maskB[~wbpnt_maskB].bool())]

        hadamadist_AtoB_m_nothr = torch.cat((match_binA_w, match_binA_b, match_binB_w, match_binB_b), dim=-1)
        hadamadist_AtoB_nm_nothr = torch.cat((unmatch_binA_w, unmatch_binA_b, unmatch_binB_w, unmatch_binB_b), dim=-1)
        
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores_token(self, FPDT, desA, desB, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, repeat_mask):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        # mask1 = hanming_distAtoB_part_begin < th1
        # mask2 = hadamadist_AtoB < th2
        # hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        # hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        # pred_match_w = FPDT.detector_net.get_match_predict(data_match_w).unsqueeze(0)
        # pred_match_b = FPDT.detector_net.get_match_predict(data_match_b).unsqueeze(0)
        pred_match_w_dict = FPDT.detector_net.get_match_predict_dict(data_match_w)
        pred_match_b_dict = FPDT.detector_net.get_match_predict_dict(data_match_b)
        pred_match_w_all = pred_match_w_dict['scores_final'].squeeze()
        pred_match_b_all = pred_match_b_dict['scores_final'].squeeze()
        layer_final_index_w = pred_match_w_dict['layer_final']
        layer_final_index_b = pred_match_b_dict['layer_final']
        layer_final_index = [layer_final_index_w, layer_final_index_b]
       
        pred_match_w = pred_match_w_all[:-1, :-1].unsqueeze(0)
        pred_match_b = pred_match_b_all[:-1, :-1].unsqueeze(0)
        match_mess_w =self.find_match_idx_score(pred_match_w, match_thr=self.match_thr)
        match_mess_b =self.find_match_idx_score(pred_match_b, match_thr=self.match_thr)
        match_indicesB_w_all = match_mess_w['matches0'].squeeze()
        match_indicesB_b_all = match_mess_b['matches0'].squeeze()
        match_scoresB_w_all = match_mess_w['matching_scores0'].squeeze()
        match_scoresB_b_all = match_mess_b['matching_scores0'].squeeze()
        match_indicesB_w = match_indicesB_w_all[match_indicesB_w_all != -1]
        match_indicesB_b = match_indicesB_b_all[match_indicesB_b_all != -1]

        # match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        # match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)

        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1], all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # max_sim = torch.cat((match_scoresB_w_all[match_indicesB_w_all != -1], match_scoresB_b_all[match_indicesB_b_all != -1]), dim=0)

        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1], all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        max_sim = torch.cat((match_scoresB_b_all[match_indicesB_b_all != -1], match_scoresB_w_all[match_indicesB_w_all != -1]), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)

        if max_sim.shape[0] >= self.want_cand_num:
            # Top30
            # # 保证topk后的indexes是从小到大的(只限于整形的距离)
            # max_sim_new = max_sim - torch.tensor(range(max_sim.shape[0])).to(max_sim.device) / max_sim.shape[0]
            # top30_mask = torch.topk(max_sim_new, 30, largest=True).indices  # 取相似度最大前三十个点对
            
            # 使用numpy argsort里稳定的归并排序(默认从小到大)
            top30_mask = np.argsort(-max_sim.cpu().numpy(), kind='mergesort')[:self.want_cand_num]
            top30_mask = torch.from_numpy(top30_mask).to(max_sim.device)
            
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        
        # 计算match点bin mask
        repeat_bin_maskA = torch.sum(repeat_mask, dim=-1) > 0
        repeat_bin_maskB = torch.sum(repeat_mask, dim=0) > 0
        exp_pred_match_w_all = torch.exp(pred_match_w_all)
        exp_pred_match_b_all = torch.exp(pred_match_b_all)

        match_binA_w = exp_pred_match_w_all[:-1, -1][repeat_bin_maskA[wbpnt_maskA].bool()]
        match_binA_b = exp_pred_match_b_all[:-1, -1][repeat_bin_maskA[~wbpnt_maskA].bool()]
        unmatch_binA_w = exp_pred_match_w_all[:-1, -1][~(repeat_bin_maskA[wbpnt_maskA].bool())]
        unmatch_binA_b = exp_pred_match_b_all[:-1, -1][~(repeat_bin_maskA[~wbpnt_maskA].bool())]
        match_binB_w = exp_pred_match_w_all[-1, :-1][repeat_bin_maskB[wbpnt_maskB].bool()]
        match_binB_b = exp_pred_match_b_all[-1, :-1][repeat_bin_maskB[~wbpnt_maskB].bool()]
        unmatch_binB_w = exp_pred_match_w_all[-1, :-1][~(repeat_bin_maskB[wbpnt_maskB].bool())]
        unmatch_binB_b = exp_pred_match_b_all[-1, :-1][~(repeat_bin_maskB[~wbpnt_maskB].bool())]

        hadamadist_AtoB_m_nothr = torch.cat((match_binA_w, match_binA_b, match_binB_w, match_binB_b), dim=-1)
        hadamadist_AtoB_nm_nothr = torch.cat((unmatch_binA_w, unmatch_binA_b, unmatch_binB_w, unmatch_binB_b), dim=-1)
        
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr, layer_final_index


    def get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores_prune(self, FPDT, desA, desB, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, repeat_mask):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        # mask1 = hanming_distAtoB_part_begin < th1
        # mask2 = hadamadist_AtoB < th2
        # hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        # hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        # pred_match_w = FPDT.detector_net.get_match_predict(data_match_w).unsqueeze(0)
        # pred_match_b = FPDT.detector_net.get_match_predict(data_match_b).unsqueeze(0)
        pred_match_w_all = FPDT.detector_net.get_match_predict_all(data_match_w)
        pred_match_w_pruneA = pred_match_w_all[:-1, 0] == pred_match_w_all[:-1, -2]
        pred_match_w_pruneB = pred_match_w_all[0, :-1] == pred_match_w_all[-2, :-1]
        pred_match_w_pruneA_ratio = pred_match_w_pruneA.sum() / (pred_match_w_all.shape[0] - 1)
        pred_match_w_pruneB_ratio = pred_match_w_pruneB.sum() / (pred_match_w_all.shape[-1] - 1)

        pred_match_b_all = FPDT.detector_net.get_match_predict_all(data_match_b)
        pred_match_b_pruneA = pred_match_b_all[:-1, 0] == pred_match_b_all[:-1, -2]
        pred_match_b_pruneB = pred_match_b_all[0, :-1] == pred_match_b_all[-2, :-1]
        pred_match_b_pruneA_ratio = pred_match_b_pruneA.sum() / (pred_match_b_all.shape[0] - 1)
        pred_match_b_pruneB_ratio = pred_match_b_pruneB.sum() / (pred_match_b_all.shape[-1] - 1)
        pred_match_prune_ratio = [pred_match_b_pruneA_ratio.item(), pred_match_b_pruneB_ratio.item(), pred_match_w_pruneA_ratio.item(), pred_match_w_pruneB_ratio.item()]

        pred_match_w = pred_match_w_all[:-1, :-1].unsqueeze(0)
        pred_match_b = pred_match_b_all[:-1, :-1].unsqueeze(0)
        match_mess_w =self.find_match_idx_score(pred_match_w, match_thr=self.match_thr)
        match_mess_b =self.find_match_idx_score(pred_match_b, match_thr=self.match_thr)
        match_indicesB_w_all = match_mess_w['matches0'].squeeze()
        match_indicesB_b_all = match_mess_b['matches0'].squeeze()
        match_scoresB_w_all = match_mess_w['matching_scores0'].squeeze()
        match_scoresB_b_all = match_mess_b['matching_scores0'].squeeze()
        match_indicesB_w = match_indicesB_w_all[match_indicesB_w_all != -1]
        match_indicesB_b = match_indicesB_b_all[match_indicesB_b_all != -1]

        # match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        # match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)

        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1], all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # max_sim = torch.cat((match_scoresB_w_all[match_indicesB_w_all != -1], match_scoresB_b_all[match_indicesB_b_all != -1]), dim=0)

        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1], all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        max_sim = torch.cat((match_scoresB_b_all[match_indicesB_b_all != -1], match_scoresB_w_all[match_indicesB_w_all != -1]), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)

        if max_sim.shape[0] >= self.want_cand_num:
            # Top30
            # # 保证topk后的indexes是从小到大的(只限于整形的距离)
            # max_sim_new = max_sim - torch.tensor(range(max_sim.shape[0])).to(max_sim.device) / max_sim.shape[0]
            # top30_mask = torch.topk(max_sim_new, 30, largest=True).indices  # 取相似度最大前三十个点对
            
            # 使用numpy argsort里稳定的归并排序(默认从小到大)
            top30_mask = np.argsort(-max_sim.cpu().numpy(), kind='mergesort')[:self.want_cand_num]
            top30_mask = torch.from_numpy(top30_mask).to(max_sim.device)
            
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        
        # 计算match点bin mask
        repeat_bin_maskA = torch.sum(repeat_mask, dim=-1) > 0
        repeat_bin_maskB = torch.sum(repeat_mask, dim=0) > 0
        exp_pred_match_w_all = torch.exp(pred_match_w_all)
        exp_pred_match_b_all = torch.exp(pred_match_b_all)

        match_binA_w = exp_pred_match_w_all[:-1, -1][repeat_bin_maskA[wbpnt_maskA].bool()]
        match_binA_b = exp_pred_match_b_all[:-1, -1][repeat_bin_maskA[~wbpnt_maskA].bool()]
        unmatch_binA_w = exp_pred_match_w_all[:-1, -1][~(repeat_bin_maskA[wbpnt_maskA].bool())]
        unmatch_binA_b = exp_pred_match_b_all[:-1, -1][~(repeat_bin_maskA[~wbpnt_maskA].bool())]
        match_binB_w = exp_pred_match_w_all[-1, :-1][repeat_bin_maskB[wbpnt_maskB].bool()]
        match_binB_b = exp_pred_match_b_all[-1, :-1][repeat_bin_maskB[~wbpnt_maskB].bool()]
        unmatch_binB_w = exp_pred_match_w_all[-1, :-1][~(repeat_bin_maskB[wbpnt_maskB].bool())]
        unmatch_binB_b = exp_pred_match_b_all[-1, :-1][~(repeat_bin_maskB[~wbpnt_maskB].bool())]

        hadamadist_AtoB_m_nothr = torch.cat((match_binA_w, match_binA_b, match_binB_w, match_binB_b), dim=-1)
        hadamadist_AtoB_nm_nothr = torch.cat((unmatch_binA_w, unmatch_binA_b, unmatch_binB_w, unmatch_binB_b), dim=-1)
        
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr, pred_match_prune_ratio


    def get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores_prune_token(self, FPDT, desA, desB, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, repeat_mask):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)  # NA x NB
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        # mask1 = hanming_distAtoB_part_begin < th1
        # mask2 = hadamadist_AtoB < th2
        # hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        # hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1

        # 黑点匹配/白点匹配分支
        all_indexesA = torch.tensor(list(range(hadamadist_AtoB.shape[0])), device=hadamadist_AtoB.device)
        all_indexesB = torch.tensor(list(range(hadamadist_AtoB.shape[1])), device=hadamadist_AtoB.device)
        # pred_match_w = FPDT.detector_net.get_match_predict(data_match_w).unsqueeze(0)
        # pred_match_b = FPDT.detector_net.get_match_predict(data_match_b).unsqueeze(0)

        # pred_match_w_all = FPDT.detector_net.get_match_predict_all(data_match_w)
        pred_match_w_dict = FPDT.detector_net.get_match_predict_dict(data_match_w)
        pred_match_w_all = pred_match_w_dict['scores_final'].squeeze()
        layer_final_index_w = pred_match_w_dict['layer_final']
        pred_match_w_pruneA = pred_match_w_all[:-1, 0] == pred_match_w_all[:-1, -2]
        pred_match_w_pruneB = pred_match_w_all[0, :-1] == pred_match_w_all[-2, :-1]
        pred_match_w_pruneA_ratio = pred_match_w_pruneA.sum() / (pred_match_w_all.shape[0] - 1)
        pred_match_w_pruneB_ratio = pred_match_w_pruneB.sum() / (pred_match_w_all.shape[-1] - 1)

        # pred_match_b_all = FPDT.detector_net.get_match_predict_all(data_match_b)
        pred_match_b_dict = FPDT.detector_net.get_match_predict_dict(data_match_b)
        pred_match_b_all = pred_match_b_dict['scores_final'].squeeze()
        layer_final_index_b = pred_match_b_dict['layer_final']
        layer_final_index = [layer_final_index_w, layer_final_index_b]
        pred_match_b_pruneA = pred_match_b_all[:-1, 0] == pred_match_b_all[:-1, -2]
        pred_match_b_pruneB = pred_match_b_all[0, :-1] == pred_match_b_all[-2, :-1]
        pred_match_b_pruneA_ratio = pred_match_b_pruneA.sum() / (pred_match_b_all.shape[0] - 1)
        pred_match_b_pruneB_ratio = pred_match_b_pruneB.sum() / (pred_match_b_all.shape[-1] - 1)
        pred_match_prune_ratio = [pred_match_b_pruneA_ratio.item(), pred_match_b_pruneB_ratio.item(), pred_match_w_pruneA_ratio.item(), pred_match_w_pruneB_ratio.item()]

        pred_match_w = pred_match_w_all[:-1, :-1].unsqueeze(0)
        pred_match_b = pred_match_b_all[:-1, :-1].unsqueeze(0)
        match_mess_w =self.find_match_idx_score(pred_match_w, match_thr=self.match_thr)
        match_mess_b =self.find_match_idx_score(pred_match_b, match_thr=self.match_thr)
        match_indicesB_w_all = match_mess_w['matches0'].squeeze()
        match_indicesB_b_all = match_mess_b['matches0'].squeeze()
        match_scoresB_w_all = match_mess_w['matching_scores0'].squeeze()
        match_scoresB_b_all = match_mess_b['matching_scores0'].squeeze()
        match_indicesB_w = match_indicesB_w_all[match_indicesB_w_all != -1]
        match_indicesB_b = match_indicesB_b_all[match_indicesB_b_all != -1]

        # match_indicesA_w, match_indicesB_w, min_dis_w = self.get_point_pair(hadamadist_AtoB[wbpnt_maskA, :][:, wbpnt_maskB], dis_thre=desA.shape[-1] + 1)
        # match_indicesA_b, match_indicesB_b, min_dis_b = self.get_point_pair(hadamadist_AtoB[~wbpnt_maskA, :][:, ~wbpnt_maskB], dis_thre=desA.shape[-1] + 1)

        # # 白点在前 黑点在后
        # match_indicesA = torch.cat((all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1], all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1]), dim=0)
        # match_indicesB = torch.cat((all_indexesB[wbpnt_maskB][match_indicesB_w], all_indexesB[~wbpnt_maskB][match_indicesB_b]), dim=0)
        # max_sim = torch.cat((match_scoresB_w_all[match_indicesB_w_all != -1], match_scoresB_b_all[match_indicesB_b_all != -1]), dim=0)

        # 黑点在前 白点在后
        match_indicesA = torch.cat((all_indexesA[~wbpnt_maskA][match_indicesB_b_all != -1], all_indexesA[wbpnt_maskA][match_indicesB_w_all != -1]), dim=0)
        match_indicesB = torch.cat((all_indexesB[~wbpnt_maskB][match_indicesB_b], all_indexesB[wbpnt_maskB][match_indicesB_w]), dim=0)
        max_sim = torch.cat((match_scoresB_b_all[match_indicesB_b_all != -1], match_scoresB_w_all[match_indicesB_w_all != -1]), dim=0)

        # # 单向最近邻
        # match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)

        if max_sim.shape[0] >= self.want_cand_num:
            # Top30
            # # 保证topk后的indexes是从小到大的(只限于整形的距离)
            # max_sim_new = max_sim - torch.tensor(range(max_sim.shape[0])).to(max_sim.device) / max_sim.shape[0]
            # top30_mask = torch.topk(max_sim_new, 30, largest=True).indices  # 取相似度最大前三十个点对
            
            # 使用numpy argsort里稳定的归并排序(默认从小到大)
            top30_mask = np.argsort(-max_sim.cpu().numpy(), kind='mergesort')[:self.want_cand_num]
            top30_mask = torch.from_numpy(top30_mask).to(max_sim.device)
            
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        
        # 计算match点bin mask
        repeat_bin_maskA = torch.sum(repeat_mask, dim=-1) > 0
        repeat_bin_maskB = torch.sum(repeat_mask, dim=0) > 0
        exp_pred_match_w_all = torch.exp(pred_match_w_all)
        exp_pred_match_b_all = torch.exp(pred_match_b_all)

        match_binA_w = exp_pred_match_w_all[:-1, -1][repeat_bin_maskA[wbpnt_maskA].bool()]
        match_binA_b = exp_pred_match_b_all[:-1, -1][repeat_bin_maskA[~wbpnt_maskA].bool()]
        unmatch_binA_w = exp_pred_match_w_all[:-1, -1][~(repeat_bin_maskA[wbpnt_maskA].bool())]
        unmatch_binA_b = exp_pred_match_b_all[:-1, -1][~(repeat_bin_maskA[~wbpnt_maskA].bool())]
        match_binB_w = exp_pred_match_w_all[-1, :-1][repeat_bin_maskB[wbpnt_maskB].bool()]
        match_binB_b = exp_pred_match_b_all[-1, :-1][repeat_bin_maskB[~wbpnt_maskB].bool()]
        unmatch_binB_w = exp_pred_match_w_all[-1, :-1][~(repeat_bin_maskB[wbpnt_maskB].bool())]
        unmatch_binB_b = exp_pred_match_b_all[-1, :-1][~(repeat_bin_maskB[~wbpnt_maskB].bool())]

        hadamadist_AtoB_m_nothr = torch.cat((match_binA_w, match_binA_b, match_binB_w, match_binB_b), dim=-1)
        hadamadist_AtoB_nm_nothr = torch.cat((unmatch_binA_w, unmatch_binA_b, unmatch_binB_w, unmatch_binB_b), dim=-1)
        
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr, pred_match_prune_ratio, layer_final_index


    def get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = desA[:, 128:].float(), desB[:, 128:].float()
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_rec_sift_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                # print(model.params[:2])
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None


    def check_inlier_angle(self, nearest_matches, nearest_matches_angle, model, inliers):
        
        if inliers is None:
            return np.array([]), np.array([])
        
        inliers_matches = nearest_matches[inliers.nonzero()[0]]   #内点对
        inliers_matches_angle = nearest_matches_angle[inliers.nonzero()[0]]  #内点对的角度
        
        pi_coef = 3.1415926
        diff = np.zeros([inliers_matches_angle.shape[0], 4])
        
        diff[:,0] = -model.rotation - inliers_matches_angle[:,0] + inliers_matches_angle[:,1] #[-2pi,2pi]
        #print(model.rotation, inliers_matches_angle[:,0] - inliers_matches_angle[:,1])
        mask = diff[:,0] < 0
        diff[mask,0] = diff[mask,0] + 2*pi_coef  #[0,2pi]
        diff[:,1] = 2*pi_coef - diff[:,0]  #[0,2pi]
        
        diff[:,2] = -model.rotation - inliers_matches_angle[:,0] + inliers_matches_angle[:,1] + pi_coef #避免0,180干扰 [-pi,3pi]
        mask = diff[:,2] < 0
        diff[mask,2] = diff[mask,2] + 2*pi_coef  #[0,3pi]
        mask = diff[:,2] > 2*pi_coef
        diff[mask,2] = diff[mask,2] - 2*pi_coef  #[0,2pi]
        diff[:,3] = 2*pi_coef - diff[:,2]  #[0,2pi]
        
        diff_c = np.min(diff,axis = 1)  #  [0, pi]
        #print(diff_c)
        inliers_matches_mask = diff_c < pi_coef/12  #误差不超过+-15度(model.rotation [-pi,pi])
        #print(np.sum(inliers),np.sum(inliers_matches_mask),  model.rotation)
        #print(np.int32(inliers_matches_mask))
        return inliers_matches[inliers_matches_mask], inliers_matches_mask

    def ransac_project(self, matched_left, matched_right, dist_list_return, angle_left, angle_right, residual_threshold=2.5):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac_project import slransac_project
            # model, inliers = slransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            model, inliers, best_inlier_residuals_sum = slransac_project((matched_left, matched_right), AffineTransform, min_samples=3, residual_threshold=residual_threshold,
                          max_trials=500, stop_sample_num= 22, random_state=2000, weight_pairs=None, angles = (angle_left, angle_right))

            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None

            # 重拟合,避免有3个内点计算的trans导致计
            data_inliers = [matched_left[inliers], matched_right[inliers]]
            model.estimate(*data_inliers)
            # mid_model_residuals = np.abs(model.residuals(*data_inliers))
            # # consensus set / inliers
            # mid_inliers = mid_model_residuals < residual_threshold

            nearest_matches = np.concatenate(((matched_right, matched_left)), axis=-1)
            nearest_matches_angle = np.concatenate(((angle_right[:, None], angle_left[:, None])), axis=-1)
            # 方向一致性过滤
            inliers_matches, inliers_matches_mask = self.check_inlier_angle(nearest_matches, nearest_matches_angle, model, inliers)
            final_inliers = inliers_matches_mask

            # matches_idx = np.array([])
            # if inliers is not None and inliers_matches_mask.shape[0] > 0:
            #     matches_idx = nearest_topk_idx[inliers][inliers_matches_mask]    # 通过角度卡控后的内点索引 top_k

            # estimate final model using all inliers
            if inliers_matches.shape[0] >= 4 and best_inlier_residuals_sum > 16384./256/256 * np.sum(inliers):    #用所有点重新拟合
                # select inliers for each data array
                data_inliers = [inliers_matches[:,2:4], inliers_matches[:,0:2]]
                model.estimate(*data_inliers)

                final_model_residuals = np.abs(model.residuals(*data_inliers))
                # consensus set / inliers
                final_inliers = final_model_residuals < residual_threshold

            if model is None:
                return None, None
            else:
                # print(model.params[:2])
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, final_inliers
        else:
            return None, None


    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()

    def generate_int_desc(self, desc):
        desc_select_thr = self.thresholding_desc(torch.round(desc * self.slope) + 5000)
        # 量化
        desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())
        # desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)
        return desc_select_quantize

    def get_quant_desc(self, desc):
        # hadama量化
        descs_quant_0 = self.generate_int_desc(desc[:, :128])
        descs_quant_45 = self.generate_int_desc(desc[:, 128:])
        descs_quant_0_same, descs_quant_0_neg = descs_quant_0[:, self.net_index], descs_quant_0[:, self.net_index==False]
        descs_quant_45_same, descs_quant_45_neg = descs_quant_45[:, self.net_index], descs_quant_45[:, self.net_index==False]

        if self.use_desc_all:
            descs_quant = torch.cat((descs_quant_0_same, descs_quant_45_same, descs_quant_0_neg, descs_quant_45_neg), dim=-1)
        else:
            descs_quant = torch.cat((descs_quant_0_same, descs_quant_45_same, descs_quant_0_same, descs_quant_45_same), dim=-1)
        
        if self.use_post_process:
            descs_quant = descs_quant * 2 - 1

        return descs_quant.float()

    def arange_like(self, x, dim: int):
        return x.new_ones(x.shape[dim]).cumsum(0) - 1  # traceable in 1.1

    def find_match_idx_score(self, scores, match_thr=0.2):
        # Get the matches with score above "match_threshold".
        max0, max1 = scores.max(2), scores.max(1)
        indices0, indices1 = max0.indices, max1.indices
        mutual0 = self.arange_like(indices0, 1)[None] == indices1.gather(1, indices0)  #当二者互为匹配点时才认为是匹配点
        mutual1 = self.arange_like(indices1, 1)[None] == indices0.gather(1, indices1)
        zero = scores.new_tensor(0)
        mscores0 = torch.where(mutual0, max0.values.exp(), zero)
        mscores1 = torch.where(mutual1, mscores0.gather(1, indices1), zero)
        valid0 = mutual0 & (mscores0 > match_thr)
        valid1 = mutual1 & valid0.gather(1, indices1)
        indices0 = torch.where(valid0, indices0, indices0.new_tensor(-1))
        indices1 = torch.where(valid1, indices1, indices1.new_tensor(-1))
        
        return {
                'matches0': indices0, # use -1 for invalid match
                'matches1': indices1, # use -1 for invalid match
                'matching_scores0': mscores0,
                'matching_scores1': mscores1
            }

    def compute_simility_bin(self, imgA, imgB, imgA_pm, imgB_pm, model, w=36, h=122, bin_thr=-1):
        # # 计算黑白相似度
        if bin_thr <= 0:
            # 均值卡控
            img_p_bin = (np.array(imgB) > np.mean(np.array(imgB)))
        else:
            # 阈值卡控
            img_p_bin = (np.array(imgB) > bin_thr)
        # print(h, w)
        np_a = cv2.warpAffine(np.array(imgA), model[:2, :], (w, h), flags=cv2.INTER_CUBIC)
        img_a_bin = (np.array(np_a) > np.mean(np.array(imgA))) if bin_thr <= 0 else (np.array(np_a) > bin_thr)
        
        np_flag = np.ones(img_a_bin.shape)
        np_flag_a = cv2.warpAffine(np_flag, model[:2, :], (w, h), flags=cv2.INTER_CUBIC)

        valid_mask = cv2.warpAffine(np.array(imgA_pm), model[:2, :], (w, h), flags=cv2.INTER_CUBIC) * 250 * np.array(imgB_pm)
        np_flag_a = np_flag_a*250
        sam_num = 0
        total_num=0
        total_area=w*h
        # # 原版本
        # for i in range(h):
        #     for j in range(w):
        #         if valid_mask[i, j] > 10: # np_flag_a[i,j]>10:
        #             total_num+=1
        #         if valid_mask[i, j] > 10: # np_flag_a[i,j]>10:
        #             if img_a_bin[i,j]==img_p_bin[i,j]:
        #                 sam_num+=1   
        # print(sam_num, total_num)
        # 并行
        total_num = np.sum(valid_mask > 10)
        sam_num = np.sum((valid_mask > 10) * (img_a_bin == img_p_bin))

        if total_num!=0:
            simility = sam_num / total_num
        else:
            simility = 0
        return simility, sam_num, total_num, total_num / total_area

    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        net_require_image = True   # True
        sift_require_image = True
        siftnet_require_image = False
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s_pnt(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            imgA_pnt, imgB_pnt = sample["imgA_pnt"].unsqueeze(0), sample["imgB_pnt"].unsqueeze(0)
            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            # sift点
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isDilation:
                W_o -= 4            # 122 x 40 -> 122 x 36

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            
            # if remove_border_r > 0:
            #     sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, W_o, H_o)
            #     sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, W_o, H_o)

            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1

            # sift描述
            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.test_netp_flag:
                '''Tradition'''
                heatmap, _ = FPDT.run_heatmap_alike_9800(imgA_pnt.to(self.device))   # 128 X 40
                heatmap_B, _ = FPDT.run_heatmap_alike_9800(imgB_pnt.to(self.device))
                
                # 有角度图
                intensityA, intensityB = None, None
                pnmapA, pnmapB = None, None
                if self.use_netori and heatmap.shape[0] == 3 and heatmap_B.shape[0] == 3:
                    intensityA = heatmap[1, 3:-3, 2:-2]
                    intensityB = heatmap_B[1, 3:-3, 2:-2]
                    pnmapA = heatmap[2, 3:-3, 2:-2]
                    pnmapB = heatmap_B[2, 3:-3, 2:-2]

                # print(heatmap.shape)
                heatmap = heatmap[0, 3:-3, 2:-2]           # 128 x 40 -> 122 x 36
                heatmap_B = heatmap_B[0, 3:-3, 2:-2]        
                
                if self.partial_filter:
                    heatmap = heatmap * imgA_partial_mask.to(heatmap.device)
                    heatmap_B = heatmap_B * imgB_partial_mask.to(heatmap_B.device)
                    
                pts_A = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 122 x 36
                pts_B = getPtsFromHeatmap(heatmap_B.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)  # pts_B 只包含匹配区域的预测点
                
                from Match_component import sample_desc_from_points
                # pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                # pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)
                pointsA = torch.tensor(pts_A.transpose(1, 0), device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0), device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2                        
                else:
                    imgA_pnt, imgB_pnt = sample['img_pnt_o_A'].unsqueeze(0), sample['img_pnt_o_B'].unsqueeze(0)
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    # W_o -= 4    # 40->36
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [122, 36]的中间（118，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]
                
                pntsA_score = pointsA_o[:, 2].float()
                pntsB_score = pointsB_o[:, 2].float()
                pointsA_o = pointsA_o[:, :2].float()
                pointsB_o = pointsB_o[:, :2].float()

                # predict angle
                net_oriA, net_oriB = None, None
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    pointsA_o_normalized = pointsA_o / pointsA_o.new_tensor([W_o - 1, H_o - 1]).to(pointsA_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    pointsB_o_normalized = pointsB_o / pointsB_o.new_tensor([W_o - 1, H_o - 1]).to(pointsB_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_kpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_kpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_kpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsB_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_kpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            pointsA_o_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    pn_predictA = (pn_kpA_predict_all > 0.5).float().squeeze()   
                    pn_predictB = (pn_kpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    net_oriA = ori_kpA_predict_all[:, 0] * (2 * pn_predictA - 1) * 90      # 正负90度
                    net_oriB = ori_kpB_predict_all[:, 0] * (2 * pn_predictB - 1) * 90
                    # print(net_oriA.shape)

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                # net_nn_mask
                warped_net_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                net_point_disAB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2]) # N x M

                warped_net_pts_A, net_mask_points = filter_points(warped_net_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                net_key_disAtoB = self.get_dis(warped_net_pts_A[:, :2], pointsB_o[:, :2])
                net_pos_repeatA_mask, net_pos_repeatB_mask, _ = self.get_point_pair(net_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                if net_pos_repeatA_mask.shape[0] > 0:
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[net_mask_points][net_pos_repeatA_mask]
                    # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, net_pos_repeatB_mask] = 1

                    net_pos_repeatA = pointsA_o[net_mask_points, :][net_pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[net_pos_repeatB_mask, :]

                if self.isdense:
                    net_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)  
                    net_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    if self.use_wbpnt_mask:
                        if net_oriA is None and net_oriB is None:
                            net_desc_A_0, net_oriA, wbpnt_maskA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, net_oriB, wbpnt_maskB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        else:
                            net_desc_A_0, _, wbpnt_maskA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, _, wbpnt_maskB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_wbmask(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)                
                    else:
                        wbpnt_maskA, wbpnt_maskB = None, None
                        if net_oriA is None and net_oriB is None:
                            net_desc_A_0, net_oriA = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, net_oriB = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        else:
                            net_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)
                            net_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)
                        
                        # # 测试网络点 + 网络角度 + 网络描述子的一致性
                        # torch.set_printoptions(precision=7)
                        # print(nameA, pointsA_o, net_desc_A_0)
                        # save_path = Path(self.output_dir, nameA)
                        # self.output_txt(save_path, nameA, net_desc_A_0.cpu().numpy(), None)
                        # exit()

                    if self.is_rec:
                        if self.cat_sift:
                            net_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            net_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, sift_ori=net_oriA)[:, :128]
                            net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, sift_ori=net_oriB)[:, :128]

                        net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                        net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                    else:
                        if net_desc_A_0.shape[-1] == 256 and net_desc_B_0.shape[-1] == 256:
                            net_desc_A = copy.deepcopy(net_desc_A_0)
                            net_desc_B = copy.deepcopy(net_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                net_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriA)  
                            
                                net_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45, sift_ori=net_oriB)
                            else:
                                net_desc_A_45 = copy.deepcopy(net_desc_A_0)
                                net_desc_B_45 = copy.deepcopy(net_desc_B_0)
                            net_desc_A = torch.cat((net_desc_A_0, net_desc_A_45), dim=1).to(self.device)
                            net_desc_B = torch.cat((net_desc_B_0, net_desc_B_45), dim=1).to(self.device)
                
                pai_coeff = 3.1415926
                
                net_oriA_short = ((net_oriA + 90) / 180 * pai_coeff * 4096).int().float()     
                net_oriA_rad = net_oriA_short / 4096

                net_oriB_short = ((net_oriB + 90) / 180 * pai_coeff * 4096).int().float()     
                net_oriB_rad = net_oriB_short / 4096
                
                pred_match_prune_ratio = [0, 0, 0, 0]
                layer_final_index = [4, 4, 4, 4]

                if self.thr_mode:
                    # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                    if self.is_rec:
                        if self.cat_sift:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                    else:
                        if self.use_wbpnt_mask:
                            if self.use_net_match: 

                                net_descs_quantA = self.get_quant_desc(net_desc_A)
                                net_descs_quantB = self.get_quant_desc(net_desc_B)

                                dbvalueA = (pntsA_score * self.dbvalue_slope).int().float() / self.dbvalue_slope
                                dbvalueB = (pntsB_score * self.dbvalue_slope).int().float() / self.dbvalue_slope
                                
                                data_match_w = {
                                    'descriptors0_same': net_descs_quantA[wbpnt_maskA, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors1_same': net_descs_quantB[wbpnt_maskB, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors0': net_descs_quantA[wbpnt_maskA, :].transpose(0, 1).unsqueeze(0),
                                    'descriptors1': net_descs_quantB[wbpnt_maskB, :].transpose(0, 1).unsqueeze(0),
                                    'keypoints0': pointsA_o[wbpnt_maskA, :].unsqueeze(0),
                                    'keypoints1': pointsB_o[wbpnt_maskB, :].unsqueeze(0),
                                    'angles0': net_oriA_rad[wbpnt_maskA].unsqueeze(0),
                                    'angles1': net_oriB_rad[wbpnt_maskB].unsqueeze(0),
                                    'dbvalue0': dbvalueA[wbpnt_maskA].unsqueeze(0),
                                    'dbvalue1': dbvalueB[wbpnt_maskB].unsqueeze(0),
                                    'H': H_o,
                                    'W': W_o,
                                }

                                data_match_b = {
                                    'descriptors0_same': net_descs_quantA[~wbpnt_maskA, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors1_same': net_descs_quantB[~wbpnt_maskB, :][:, :128].transpose(0, 1).unsqueeze(0),
                                    'descriptors0': net_descs_quantA[~wbpnt_maskA, :].transpose(0, 1).unsqueeze(0),
                                    'descriptors1': net_descs_quantB[~wbpnt_maskB, :].transpose(0, 1).unsqueeze(0),
                                    'keypoints0': pointsA_o[~wbpnt_maskA, :].unsqueeze(0),
                                    'keypoints1': pointsB_o[~wbpnt_maskB, :].unsqueeze(0),
                                    'angles0': net_oriA_rad[~wbpnt_maskA].unsqueeze(0),
                                    'angles1': net_oriB_rad[~wbpnt_maskB].unsqueeze(0),
                                    'dbvalue0': dbvalueA[~wbpnt_maskA].unsqueeze(0),
                                    'dbvalue1': dbvalueB[~wbpnt_maskB].unsqueeze(0),
                                    'H': H_o,
                                    'W': W_o,
                                }

                                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores(FPDT, net_desc_A, net_desc_B, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, net_nn_mask)
                                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis, pred_match_prune_ratio = self.get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores_prune(FPDT, net_desc_A, net_desc_B, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, net_nn_mask)
                                net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis, layer_final_index = self.get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores_token(FPDT, net_desc_A, net_desc_B, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, net_nn_mask)
                                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis, pred_match_prune_ratio, layer_final_index = self.get_match_nomatch_dis_nosampler_hadama_permute_256_net_wbmask_scores_prune_token(FPDT, net_desc_A, net_desc_B, data_match_w, data_match_b, wbpnt_maskA, wbpnt_maskB, net_nn_mask)

                            else:
                                net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256_wbmask(net_desc_A, net_desc_B, wbpnt_maskA, wbpnt_maskB, net_nn_mask, self.th1, self.th2)
                        else:
                            net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis, net_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(net_desc_A, net_desc_B, net_nn_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(net_desc_A, net_desc_B, net_match_mask)
            
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                net_oriA_ransac = (net_oriA_rad - pai_coeff / 2)[net_match_indicesA].cpu()
                net_oriB_ransac = (net_oriB_rad - pai_coeff / 2)[net_match_indicesB].cpu()
                
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1

                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                
                net_pointdis_mean = torch.mean(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_var = torch.var(net_point_disAB[(net_match_mask * net_nn_mask) == 1])
                net_pointdis_std = torch.std(net_point_disAB[(net_match_mask * net_nn_mask) == 1])

                net_ori_diff = self.get_ori_diff(net_oriA, net_oriB, trans_angle)
                net_oridiff_mean = torch.mean(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_var = torch.var(net_ori_diff[(net_match_mask * net_nn_mask) == 1])
                net_oridiff_std = torch.std(net_ori_diff[(net_match_mask * net_nn_mask) == 1])

                net_hamdis_top30_mean = torch.mean(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_var = torch.var(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                net_hamdis_top30_std = torch.std(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])
                # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
                if net_hanming_dis[(net_match_mask * net_nn_mask) == 1].shape[0] == 0:
                    net_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
                else:
                    net_hamdis_top30max = torch.max(net_hanming_dis[(net_match_mask * net_nn_mask) == 1])

                net_wb_simility, net_wb_sam_num, net_repeat_area_num, net_repeat_area = 0, 0, 0, 0
                net_inliers_num, net_inliers_ratio = 0, 0
                # 画Net配准图
                if net_require_image:
                    if self.use_old_ransac:
                        Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    else:
                        Homography, inliers = self.ransac_project(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None, 
                                                angle_left=net_oriA_ransac, angle_right=net_oriB_ransac)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        # Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8) # 122 x 36 -> 128 x 52
                        Homography_modify = torch.tensor(Homography.astype(np.float32))
                        imgA_o_warped = inv_warp_image(imgA_pnt.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                        img_2D_A = imgA_pnt.numpy().squeeze()
                        img_2D_B = imgB_pnt.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        # cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_net_match.bmp'), image_merge)

                        # print(imgA_simi.shape)
                        net_wb_simility, net_wb_sam_num, net_repeat_area_num, net_repeat_area = self.compute_simility_bin(imgA_pnt.squeeze(), imgB_pnt.squeeze(), 
                                                                                                                        imgA_partial_mask.squeeze(), imgB_partial_mask.squeeze(), 
                                                                                                                        Homography, 
                                                                                                                        w=imgA_pnt.squeeze().shape[1], h=imgA_pnt.squeeze().shape[0], 
                                                                                                                        bin_thr=-1)
                        if net_wb_simility < self.printImg_wb_thr:
                            cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_net_match_' + "{:.4f}".format(net_wb_simility) + '.bmp'), image_merge)
                                                                                                                                        

                        net_inliers_num = np.sum(inliers)
                        net_inliers_ratio = net_inliers_num / net_cand_num
                
                if net_require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA_pnt.numpy().squeeze(), 'img_H': imgB_pnt.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=3, s=3)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    if net_wb_simility < self.printImg_wb_thr:
                        f = self.output_dir / (nameAB + "_net_line_" + "{:.4f}".format(net_wb_simility) +  ".bmp")
                        cv2.imwrite(str(f), img_pts)
                    pass

            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            siftnet_oriA, siftnet_oriB = sift_oriA, sift_oriB
            if not self.test_netdesc_flag:
                siftnet_desc_A, siftnet_desc_B = torch.rand(sift_ptsA.shape[0], 256).to(self.device), torch.rand(sift_ptsB.shape[0], 256).to(self.device)
            else:
                if self.use_netori and intensityA is not None and pnmapA is not None:
                    sift_ptsA_normalized = sift_ptsA / sift_ptsA.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsA.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)
                    sift_ptsB_normalized = sift_ptsB / sift_ptsB.new_tensor([W_o - 1, H_o - 1]).to(sift_ptsB.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                    ori_siftkpB_predict_all = F.grid_sample(intensityB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    ori_siftkpA_predict_all = F.grid_sample(intensityA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    pn_siftkpB_predict_all = F.grid_sample(pnmapB.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsB_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTB x 2
                    pn_siftkpA_predict_all = F.grid_sample(pnmapA.view(-1, H_o, W_o).unsqueeze(0),
                                            sift_ptsA_normalized.float().view(1, 1, -1, 2),
                                            mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                    # 正负号mask
                    sift_pn_predictA = (pn_siftkpA_predict_all > 0.5).float().squeeze()   
                    sift_pn_predictB = (pn_siftkpB_predict_all > 0.5).float().squeeze()
                    # print(pn_predictA.shape, ori_kpA_predict_all[:, 0])
                    # 强度loss
                    siftnet_oriA = ori_siftkpA_predict_all[:, 0] * (2 * sift_pn_predictA - 1) * 90      # 正负90度
                    siftnet_oriB = ori_siftkpB_predict_all[:, 0] * (2 * sift_pn_predictB - 1) * 90
                    # print(net_oriA.shape)


                if self.isdense:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)         # 136x36
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)

                    # siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                    siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                    
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)
                    
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

                else:
                    # siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                    # siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
                    siftnet_desc_A_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=siftnet_oriA)
                    siftnet_desc_B_0, _ = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=siftnet_oriB)

                    # print(siftnet_desc_A_0[0, :]]
                    # torch.set_printoptions(precision=10)
                    # print(siftnet_desc_A_0[8, :])
                    # print((siftnet_desc_A_0 * 100).int()[8, :])
                    # print(sift_ptsA[8, :])
                    # exit()
                    if self.is_rec:
                        if self.cat_sift:
                            siftnet_desc_A_45 = copy.deepcopy(sift_descA[:, :128])
                            siftnet_desc_B_45 = copy.deepcopy(sift_descB[:, :128])
                        else:
                            siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)[:, :128]
                            siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)[:, :128]

                        siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                        siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                    else:
                        if siftnet_desc_A_0.shape[-1] == 256 and siftnet_desc_B_0.shape[-1] == 256:
                            siftnet_desc_A = copy.deepcopy(siftnet_desc_A_0)
                            siftnet_desc_B = copy.deepcopy(siftnet_desc_B_0)
                        else:
                            if self.has45:
                                # siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                # siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                                siftnet_desc_A_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                            
                                siftnet_desc_B_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                            else:
                                siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                                siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                            siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                            siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)
                
            if self.thr_mode:
                # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(siftnet_desc_A, siftnet_desc_B, sift_match_mask, self.th1, self.th2)
                if self.is_rec:
                    if self.cat_sift:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                    else:
                        siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
                else:
                    siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            # siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num = torch.sum(siftnet_match_mask * sift_nn_mask, (0, 1))
            siftnet_hit_ratio = siftnet_cand_top30_num / siftnet_cand_num
            
            siftnet_pointdis_mean = torch.mean(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_var = torch.var(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_pointdis_std = torch.std(sift_point_disAB[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_ori_diff = self.get_ori_diff(siftnet_oriA, siftnet_oriB, trans_angle)
            siftnet_oridiff_mean = torch.mean(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_var = torch.var(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_oridiff_std = torch.std(siftnet_ori_diff[(siftnet_match_mask * sift_nn_mask) == 1])

            siftnet_hamdis_top30_mean = torch.mean(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_var = torch.var(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            siftnet_hamdis_top30_std = torch.std(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            # print(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])
            if siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                siftnet_hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
            else:
                siftnet_hamdis_top30max = torch.max(siftnet_hanming_dis[(siftnet_match_mask * sift_nn_mask) == 1])

            # 画Net配准图
            if siftnet_require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    Homography_modify = self.homography_centorcrop(torch.tensor(Homography.astype(np.float32)), -3, -8)
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography_modify.cpu().numpy().astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_siftnet_match.bmp'), image_merge)
            
            # 画Sift配准图
            if sift_require_image:
                # mat_H_modify = self.homography_centorcrop(mat_H, -3, -8) # 122x36 -> 128x52
                imgA_o_warped = inv_warp_image(imgA_pnt.squeeze(), torch.from_numpy(mat_H.cpu().numpy().astype(np.float32)))
                img_2D_A = imgA_pnt.numpy().squeeze()
                img_2D_B = imgB_pnt.numpy().squeeze()
                img_2D_warped = imgA_o_warped.numpy().squeeze()
                b = np.zeros_like(img_2D_A)
                g = img_2D_warped * 255  #  旋转后的模板
                r = img_2D_B * 255    
                image_merge = cv2.merge([b, g, r])
                image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                if net_wb_simility < self.printImg_wb_thr:
                    cv2.imwrite(os.path.join(self.output_dir, str(count - 1) + '_' + nameAB + '_sift_match.bmp'), image_merge)

            # 如果是长条形描述测试sift 0度 128维double
            if self.is_rec and self.test_sift128:
                sift_descA = torch.cat((sift_descA[:, :128], sift_descA[:, :128]), dim=1)
                sift_descB = torch.cat((sift_descB[:, :128], sift_descB[:, :128]), dim=1)

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
            # _, _, _, sift_match_mask = self.get_match_point_pair(sift_hanmingdist_AtoB, dis_thre=sift_descA.shape[-1] + 1)      
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask]
            # print(torch.sort(sift_min_dis[sift_top30_mask]), sift_pos_repeatA_indices, torch.sort(sift_pos_repeatB_mask))

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            # print(sift_match_indicesA, sift_match_indicesB, sift_min_dis)
            # print(sift_hanmingdist_AtoB[sift_match_mask == 1], torch.where(sift_match_mask == 1)[0], torch.where(sift_match_mask == 1)[1])
            sift_cand_top30_num = torch.sum(sift_match_mask * sift_nn_mask, (0, 1)) 
            sift_hit_ratio = sift_cand_top30_num / sift_cand_num

            sift_pointdis_mean = torch.mean(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_var = torch.var(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])
            sift_pointdis_std = torch.std(sift_point_disAB[(sift_match_mask * sift_nn_mask) == 1])

            sift_oridiff_mean = torch.mean(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_var = torch.var(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])
            sift_oridiff_std = torch.std(sift_ori_diff[(sift_match_mask * sift_nn_mask) == 1])

            sift_hamdis_top30_mean = torch.mean(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_var = torch.var(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            sift_hamdis_top30_std = torch.std(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])
            if sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1].shape[0] == 0:
                sift_hamdis_top30max = torch.mean(torch.tensor([])) 
            else:
                sift_hamdis_top30max  = torch.max(sift_hanmingdist_AtoB[(sift_match_mask * sift_nn_mask) == 1])

            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item(), 
                                siftnet_pointdis_mean.item(), siftnet_pointdis_var.item(), siftnet_pointdis_std.item(), sift_pointdis_mean.item(), sift_pointdis_var.item(), sift_pointdis_std.item(), net_pointdis_mean.item(), net_pointdis_var.item(), net_pointdis_std.item(),
                                siftnet_oridiff_mean.item(), siftnet_oridiff_var.item(), siftnet_oridiff_std.item(), sift_oridiff_mean.item(), sift_oridiff_var.item(), sift_oridiff_std.item(), net_oridiff_mean.item(), net_oridiff_var.item(), net_oridiff_std.item(),
                                siftnet_hamdis_top30_mean.item(), siftnet_hamdis_top30_var.item(), siftnet_hamdis_top30_std.item(), siftnet_hamdis_top30max.item(), sift_hamdis_top30_mean.item(), sift_hamdis_top30_var.item(), sift_hamdis_top30_std.item(), sift_hamdis_top30max.item(),
                                net_hamdis_top30_mean.item(), net_hamdis_top30_var.item(), net_hamdis_top30_std.item(), net_hamdis_top30max.item(), net_wb_sam_num, net_wb_simility, net_repeat_area_num, net_repeat_area, net_inliers_num, net_inliers_ratio, 
                                pred_match_prune_ratio[0], pred_match_prune_ratio[1], pred_match_prune_ratio[2], pred_match_prune_ratio[3], layer_final_index[0], layer_final_index[1]])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)

        df_hit = pd.DataFrame(content_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'net_pointdis_mean',
                    'net_pointdis_var',
                    'net_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'net_oridiff_mean',
                    'net_oridiff_var',
                    'net_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'net_hamdis_top30_mean',
                    'net_hamdis_top30_var',
                    'net_hamdis_top30_std',
                    'net_hamdis_top30max',
                    'net_wb_sam_num',
                    'net_wb_simility', 
                    'net_repeat_area_num', 
                    'net_repeat_area',
                    'net_inliers_num', 
                    'net_inliers_ratio',
                    'pred_match_b_pruneA_ratio', 
                    'pred_match_b_pruneB_ratio', 
                    'pred_match_w_pruneA_ratio', 
                    'pred_match_w_pruneB_ratio',
                    'layer_final_index_w',
                    'layer_final_index_b'
                    ])
        df_hit.to_csv(os.path.join(self.output_dir, 'results_hit.csv'))


        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Distribution_Candidate_Hit_Ratio_oridiff_ALikeWithHard_Ext93FA(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 150
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = False
        self.siftq           = True
        self.root_dir        = '/hdd/file-input/linwc/Descriptor/data/6193Test/'
        self.category        = '6193_DK7_XA_rot_test'
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            
            sift_succ_path = Path(self.root_dir + self.category + '/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')
            sift_FAsucc_path = Path(self.root_dir + self.category + '/SIFT_Trans/SIFT_transSuccFA.csv')
            self.sift_FAsucc_dict = self.Decode_Succ_Fail_Csv_rotFA(sift_FAsucc_path, 'pnt_desc_data')
            self.sift_inliers_path = self.root_dir + self.category + '/sift_inliers.npy'
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF,
            'OneScore': df['score_flag'].to_list()
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF, OneScore) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF'],
            files['OneScore']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF, 'OneScore': OneScore}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rotFA(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        # enr_vefy = df['enrollverify'].to_list()

        # enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_enrollverify = df['enrollverify'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = ['/'.join(e.split(' verify:')[0].replace('_extend', '').split('/')[-3:]) for e in tmp_enrollverify]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        
        # tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF,
            'OneScore': df['score_flag'].to_list()
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF, OneScore) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF'],
            files['OneScore']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF, 'OneScore': OneScore}
            sequence_set.append(sample)

        return sequence_set


    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9], mat_info[:, -5]
        desc2, desc6 = mat_info[:, -8], mat_info[:, -4]
        desc3, desc7 = mat_info[:, -7], mat_info[:, -3]
        desc4, desc8 = mat_info[:, -6], mat_info[:, -2]
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'onescore': samples[index]['OneScore']})

        return input


    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * 10000) + 5000), self.thresholding_desc(torch.round(desB * 10000) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * 10000) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * 10000) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * 10000) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()

    def cal_mean_var_std(self, dis, mask=None):
        dis_mean = torch.mean(dis[mask])
        dis_var = torch.var(dis[mask])
        dis_std = torch.std(dis[mask])

        return dis_mean, dis_var, dis_std

    def cal_hit_ratio_messure(self, point_disAB, ori_diff, hanming_dis, nn_mask, match_nn_mask, cand_num):
        nn_num = torch.sum(nn_mask, (0, 1))
        cand_top30_num = torch.sum(match_nn_mask, (0, 1))
        hit_ratio = cand_top30_num / cand_num

        # 点距离 
        dis_mean, dis_var, dis_std = self.cal_mean_var_std(point_disAB, match_nn_mask)
        diff_mean, diff_var, diff_std = self.cal_mean_var_std(ori_diff, match_nn_mask)
        hamdis_top30_mean, hamdis_top30_var, hamdis_top30_std = self.cal_mean_var_std(hanming_dis, match_nn_mask)

        if hanming_dis[match_nn_mask].shape[0] == 0:
            hamdis_top30max = torch.mean(torch.tensor([]))                          # tensor(nan)
        else:
            hamdis_top30max = torch.max(hanming_dis[match_nn_mask])
        
        return cand_top30_num, hit_ratio, dis_mean, dis_var, dis_std, diff_mean, diff_var, diff_std, hamdis_top30_mean, hamdis_top30_var, hamdis_top30_std, hamdis_top30max

    def get_hit_matchDis(self, FPDT, sift_succ_dict, repeat_dis, phase='FR', remove_border_r=2, require_image=False, sift_in_mask_all=None, test_num=-1):
        content_hit = []
        content_hit_in = []
        total_count = len(sift_succ_dict) if test_num < 0 else test_num

        print('Mode:'+ phase + '\n')
        count = 0
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s(idx, sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            if self.test_netp_flag:
                '''Tradition'''
                pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
                pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
                
                from Match_component import sample_desc_from_points
                pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
                pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

                pointsA_o = copy.deepcopy(pointsA)
                pointsB_o = copy.deepcopy(pointsB)

                '''后续计算均还原到原图136*36上'''
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36
                    pointsA_o[:, 0] += 2
                    pointsB_o[:, 0] += 2
                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o -= 4    # 40->36
                    mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                    pointsA_o = pointsA_o[mask_ptsA, :]
                    mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                    pointsB_o = pointsB_o[mask_ptsB, :]
                    pointsA_o[:, 0] -= 2
                    pointsB_o[:, 0] -= 2
                # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if remove_border_r > 0:
                    pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                    pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
                
                '''截断150'''
                if self.top_k_net:
                    if pointsA_o.shape[0] > self.top_k_net:
                        pointsA_o = pointsA_o[:self.top_k_net, :]
                    if pointsB_o.shape[0] > self.top_k_net:
                        pointsB_o = pointsB_o[:self.top_k_net, :]

                if self.isrot and not self.isrot91:
                    pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                    pointsB_o[:, 1] += 12
                    H_o += 24    # 136->160 

                if self.isdense:
                    # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)  
                    desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    # desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
                    desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                    if self.has45:
                        # desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        # desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                        desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgA_m.to(self.device), imgA.to(self.device), pointsA_o.unsqueeze(0), False, False, cal_angle=45)  
                    
                        desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(imgB_m.to(self.device), imgB.to(self.device), pointsB_o.unsqueeze(0), False, False, cal_angle=45)
                    else:
                        desc_A_45 = copy.deepcopy(desc_A_0)
                        desc_B_45 = copy.deepcopy(desc_B_0)
                    desc_A = torch.cat((desc_A_0, desc_A_45), dim=1).to(self.device)
                    desc_B = torch.cat((desc_B_0, desc_B_45), dim=1).to(self.device)
                else:
                    desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                    desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                warped_pts_A = warp_points(pointsA_o, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
                warped_pts_A, mask_points = filter_points(warped_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

                key_disAtoB = self.get_dis(warped_pts_A[:, :2], pointsB_o[:, :2])
                pos_repeatA_mask, pos_repeatB_mask, _ = self.get_point_pair(key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                if pos_repeatA_mask.shape[0] > 0: 
                    net_A_all_indexes = torch.tensor(list(range(pointsA_o.shape[0])), device=self.device)
                    net_pos_repeatA_indices = net_A_all_indexes[mask_points][pos_repeatA_mask]
                    net_match_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1
                    net_pos_repeatA = pointsA_o[mask_points, :][pos_repeatA_mask, :]
                    net_pos_repeatB = pointsB_o[pos_repeatB_mask, :]
                    net_nn_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                    net_nn_mask[net_pos_repeatA_indices, pos_repeatB_mask] = 1

                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis(net_pointsA_norm.float(), net_pointsB_norm.float(), desc_A.unsqueeze(0), desc_B.unsqueeze(0), net_match_mask)
                if self.thr_mode:
                    # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis  = self.get_match_nomatch_dis_nosampler_hadama_permute_256(desc_A, desc_B, net_match_mask, self.th1, self.th2)
                else:
                    net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)
                # net_match_indicesA, net_match_indicesB, net_match_hanming_dis, net_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(desc_A, desc_B, net_match_mask)

                net_pos_nnA = pointsA_o[net_match_indicesA, :]
                net_pos_nnB = pointsB_o[net_match_indicesB, :]
                net_match_mask = torch.zeros((pointsA_o.shape[0], pointsB_o.shape[0]), device=self.device) 
                net_match_mask[net_match_indicesA, net_match_indicesB] = 1
                net_nn_num = torch.sum(net_nn_mask, (0, 1))
                net_cand_num = net_match_indicesA.shape[0] if net_match_indicesA.shape[0] < 30 else 30
                net_cand_top30_num = torch.sum(net_match_mask * net_nn_mask, (0, 1))
                net_hit_ratio = net_cand_top30_num / net_cand_num
                # if not self.isDilation:
                #     net_pos_nnA[:, 0] += 2
                #     net_pos_nnB[:, 0] += 2
                if require_image:
                    pred = {}
                    pred.update({
                        "pts": pointsA_o.detach().cpu().numpy(),
                        "pts_H": pointsB_o.detach().cpu().numpy(),
                        "pts_repeatA": net_pos_repeatA.detach().cpu().numpy(),
                        "pts_repeatB": net_pos_repeatB.detach().cpu().numpy(),
                        "pts_nnA": net_pos_nnA.detach().cpu().numpy(),
                        "pts_nnB": net_pos_nnB.detach().cpu().numpy()
                    })   # 图像坐标系
                    img_pair = {}
                    img_pair.update({'img': imgA.numpy().squeeze(), 'img_H': imgB.numpy().squeeze()})
                    img_pts = draw_match_pair_hanming(img_pair, pred, None, radius=1, s=1)
                    # label_pts = fe.getPtsFromLabels2D(label_2d.cpu().squeeze())
                    f = self.output_dir / (nameAB + "_net_line.bmp")
                    cv2.imwrite(str(f), img_pts)

                # 画Net配准图
                if require_image:
                    Homography, inliers = self.ransac_new(net_pos_nnA.detach().cpu().numpy().astype(np.int32), net_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                    if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                        imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                        img_2D_A = imgA.numpy().squeeze()
                        img_2D_B = imgB.numpy().squeeze()
                        img_2D_warped = imgA_o_warped.numpy().squeeze()
                        b = np.zeros_like(img_2D_A)
                        g = img_2D_warped * 255  #  旋转后的模板
                        r = img_2D_B * 255    
                        image_merge = cv2.merge([b, g, r])
                        image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                        cv2.imwrite(os.path.join(self.output_dir, nameAB + '_net_match.bmp'), image_merge)
            else:
                if self.isrot and not self.isrot91:
                    H_o += 24
                if not self.isDilation:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                    W_o += 4            # 32->36                        
                else:
                    imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
                    W_o -= 4        # 36

                net_nn_num = torch.zeros(1).to(self.device)
                net_cand_num = torch.zeros(1).to(self.device)
                net_cand_top30_num = torch.zeros(1).to(self.device)
                net_hit_ratio = torch.zeros(1).to(self.device)

            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k]
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]

            # sift_nn_mask 
            # print(mat_H)
            warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            if sift_pos_repeatA_mask.shape[0] > 0:
                sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
                sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
                sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
                sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1

            sift_nn_num = torch.sum(sift_nn_mask, (0, 1))

            if sift_in_mask_all is None:
                sift_in_mask = copy.deepcopy(sift_nn_mask)
            else:
                sift_in_mask = torch.tensor(sift_in_mask_all[idx], device=self.device)

            sift_in_num = torch.sum(sift_in_mask, (0, 1))

            if self.isdense:

                siftnet_desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, sift_ori=sift_oriA)  
                
                siftnet_desc_B_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, sift_ori=sift_oriB)

                if self.has45:
                    siftnet_desc_A_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgA_m.to(self.device), imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriA)  
                
                    siftnet_desc_B_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(imgB_m.to(self.device), imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False, cal_angle=45, sift_ori=sift_oriB)
                else:
                    siftnet_desc_A_45 = copy.deepcopy(siftnet_desc_A_0)
                    siftnet_desc_B_45 = copy.deepcopy(siftnet_desc_B_0)
                siftnet_desc_A = torch.cat((siftnet_desc_A_0, siftnet_desc_A_45), dim=1).to(self.device)
                siftnet_desc_B = torch.cat((siftnet_desc_B_0, siftnet_desc_B_45), dim=1).to(self.device)

            else:
                siftnet_desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), sift_ptsA.unsqueeze(0), False, False)
                siftnet_desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), sift_ptsB.unsqueeze(0), False, False)
            if self.thr_mode:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis, siftnet_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama_permute_256(siftnet_desc_A, siftnet_desc_B, sift_nn_mask, self.th1, self.th2)
            else:
                siftnet_match_indicesA, siftnet_match_indicesB, siftnet_match_hanming_dis, siftnet_nomatch_hanming_dis = self.get_match_nomatch_dis_nosampler_hadama(siftnet_desc_A, siftnet_desc_B, sift_match_mask)
            
            siftnet_pos_nnA = sift_ptsA[siftnet_match_indicesA, :]
            siftnet_pos_nnB = sift_ptsB[siftnet_match_indicesB, :]
            
            # siftnet_match_mask
            siftnet_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            siftnet_match_mask[siftnet_match_indicesA, siftnet_match_indicesB] = 1

            # siftnet_match_nn_mask
            siftnet_match_nn_mask = (siftnet_match_mask * sift_nn_mask) == 1
            siftnet_cand_num = siftnet_match_indicesA.shape[0] if siftnet_match_indicesA.shape[0] < 30 else 30
            siftnet_cand_top30_num, siftnet_hit_ratio, siftnet_pointdis_mean, siftnet_pointdis_var, siftnet_pointdis_std, siftnet_oridiff_mean, siftnet_oridiff_var, siftnet_oridiff_std, siftnet_hamdis_top30_mean, siftnet_hamdis_top30_var, siftnet_hamdis_top30_std, siftnet_hamdis_top30max = self.cal_hit_ratio_messure(sift_point_disAB, 
                                                                                                                                                                                                                                                                                                    sift_ori_diff, siftnet_hanming_dis, sift_nn_mask, siftnet_match_nn_mask, siftnet_cand_num)
            
            # siftnet_match_in_mask
            siftnet_match_in_mask = (siftnet_match_mask * sift_in_mask) == 1
            siftnet_in_cand_top30_num, siftnet_in_hit_ratio, siftnet_in_pointdis_mean, siftnet_in_pointdis_var, siftnet_in_pointdis_std, siftnet_in_oridiff_mean, siftnet_in_oridiff_var, siftnet_in_oridiff_std, siftnet_in_hamdis_top30_mean, siftnet_in_hamdis_top30_var, siftnet_in_hamdis_top30_std, siftnet_in_hamdis_top30max = self.cal_hit_ratio_messure(sift_point_disAB, 
                                                                                                                                                                                                                                                                                                    sift_ori_diff, siftnet_hanming_dis, sift_in_mask, siftnet_match_in_mask, siftnet_cand_num)
            # 画Sift配准图
            if require_image:
                Homography, inliers = self.ransac_new(siftnet_pos_nnA.detach().cpu().numpy().astype(np.int32), siftnet_pos_nnB.detach().cpu().numpy().astype(np.int32), None)
                if Homography is not None and inliers is not None and np.linalg.det(Homography) != 0:
                    imgA_o_warped = inv_warp_image(imgA.squeeze(), torch.from_numpy(Homography.astype(np.float32)))
                    img_2D_A = imgA.numpy().squeeze()
                    img_2D_B = imgB.numpy().squeeze()
                    img_2D_warped = imgA_o_warped.numpy().squeeze()
                    b = np.zeros_like(img_2D_A)
                    g = img_2D_warped * 255  #  旋转后的模板
                    r = img_2D_B * 255    
                    image_merge = cv2.merge([b, g, r])
                    image_merge = cv2.resize(image_merge, None, fx=1, fy=1)
                    cv2.imwrite(os.path.join(self.output_dir, nameAB + '_siftnet_match.bmp'), image_merge)

            sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            if self.isrot and not self.isrot91: 
                if self.remove_border_w > 0 or self.remove_border_h > 0: 
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            else:
                if remove_border_r > 0:
                    sift_descA = sift_descA[borderA, :]
                    sift_descB = sift_descB[borderB, :]
            if self.top_k:
                if sift_descA.shape[0] > self.top_k:
                    sift_descA = sift_descA[:self.top_k, :]
                if sift_descB.shape[0] > self.top_k:
                    sift_descB = sift_descB[:self.top_k, :]
            
            if self.isrot:
                if self.isrot91:
                    if not self.siftq:
                        sift_descA_front_thr, sift_descA_back_thr = self.thresholding_desc(sift_descA[:, :128]), self.thresholding_desc(sift_descA[:, 128:])
                        sift_descA_front_quan, sift_descA_back_quan = self.quantize_desc_hadama(sift_descA_front_thr.float()), self.quantize_desc_hadama(sift_descA_back_thr.float())
                        sift_descA = torch.cat((sift_descA_front_quan, sift_descA_back_quan), dim=-1).float()

                        sift_descB_front_thr, sift_descB_back_thr = self.thresholding_desc(sift_descB[:, :128]), self.thresholding_desc(sift_descB[:, 128:])
                        sift_descB_front_quan, sift_descB_back_quan = self.quantize_desc_hadama(sift_descB_front_thr.float()), self.quantize_desc_hadama(sift_descB_back_thr.float())
                        sift_descB = torch.cat((sift_descB_front_quan, sift_descB_back_quan), dim=-1).float()
                else:
                    sift_descA_thr = self.thresholding_desc(sift_descA[:, :128])
                    sift_descA_quan = self.quantize_desc_hadama(sift_descA_thr.float())
                    sift_descA = torch.cat((sift_descA_quan, sift_descA_quan), dim=-1).float()

                    sift_descB_thr = self.thresholding_desc(sift_descB[:, :128])
                    sift_descB_quan = self.quantize_desc_hadama(sift_descB_thr.float())
                    sift_descB = torch.cat((sift_descB_quan, sift_descB_quan), dim=-1).float()

            if self.thr_mode:
                sift_hanmingdist_AtoB_thr, sift_hanmingdist_AtoB = self.get_des_hanmingdis_permute(sift_descA, sift_descB, self.th1, self.th2)
            else:
                sift_hanmingdist_AtoB = self.get_des_hanmingdis(sift_descA, sift_descB)
     
            sift_match_hanming_dis, sift_nomatch_hanming_dis = sift_hanmingdist_AtoB[sift_nn_mask == 1], sift_hanmingdist_AtoB[sift_nn_mask == 0]

            sift_match_indicesA, sift_match_indicesB, sift_min_dis = self.get_point_pair(sift_hanmingdist_AtoB_thr, dis_thre=sift_descA.shape[0] + 1)  # p -> k 
            if sift_min_dis.shape[0] >= 30:
                # Top30
                sift_top30_mask = torch.topk(sift_min_dis, 30, largest=False).indices 
                sift_cand_num = 30 
            else:
                sift_top30_mask = torch.ones_like(sift_match_indicesA).to(self.device).bool()    
                sift_cand_num = sift_min_dis.shape[0]

            # sift_match_mask
            sift_match_indicesA_top30, sift_match_indicesB_top30 = sift_match_indicesA[sift_top30_mask], sift_match_indicesB[sift_top30_mask]

            sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            sift_match_mask[sift_match_indicesA_top30, sift_match_indicesB_top30] = 1
            
            # sift_match_nn_mask
            sift_match_nn_mask = (sift_match_mask * sift_nn_mask) == 1
            sift_cand_top30_num, sift_hit_ratio, sift_pointdis_mean, sift_pointdis_var, sift_pointdis_std, sift_oridiff_mean, sift_oridiff_var, sift_oridiff_std, sift_hamdis_top30_mean, sift_hamdis_top30_var, sift_hamdis_top30_std, sift_hamdis_top30max = self.cal_hit_ratio_messure(sift_point_disAB, 
                                                                                                                                                                                                                                                                                        sift_ori_diff, sift_hanmingdist_AtoB, sift_nn_mask, sift_match_nn_mask, sift_cand_num)

            # sift_match_in_mask
            sift_match_in_mask = (sift_match_mask * sift_in_mask) == 1
            sift_in_cand_top30_num, sift_in_hit_ratio, sift_in_pointdis_mean, sift_in_pointdis_var, sift_in_pointdis_std, sift_in_oridiff_mean, sift_in_oridiff_var, sift_in_oridiff_std, sift_in_hamdis_top30_mean, sift_in_hamdis_top30_var, sift_in_hamdis_top30_std, sift_in_hamdis_top30max = self.cal_hit_ratio_messure(sift_point_disAB, 
                                                                                                                                                                                                                                                                                        sift_ori_diff, sift_hanmingdist_AtoB, sift_in_mask, sift_match_in_mask, sift_cand_num)


            content_hit.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_hit_ratio.item(), sift_hit_ratio.item(), net_nn_num.item(), sift_nn_num.item(), net_cand_top30_num.item(), siftnet_cand_top30_num.item(), sift_cand_top30_num.item(), 
                                siftnet_pointdis_mean.item(), siftnet_pointdis_var.item(), siftnet_pointdis_std.item(), sift_pointdis_mean.item(), sift_pointdis_var.item(),sift_pointdis_std.item(),
                                siftnet_oridiff_mean.item(), siftnet_oridiff_var.item(), siftnet_oridiff_std.item(), sift_oridiff_mean.item(), sift_oridiff_var.item(), sift_oridiff_std.item(),
                                siftnet_hamdis_top30_mean.item(), siftnet_hamdis_top30_var.item(), siftnet_hamdis_top30_std.item(), siftnet_hamdis_top30max.item(), sift_hamdis_top30_mean.item(), sift_hamdis_top30_var.item(), sift_hamdis_top30_std.item(), sift_hamdis_top30max.item(), int(sample['onescore'])])

            content_hit_in.append([nameA, nameB, trans_angle, net_hit_ratio.item(), siftnet_in_hit_ratio.item(), sift_in_hit_ratio.item(), net_nn_num.item(), sift_in_num.item(), net_cand_top30_num.item(), siftnet_in_cand_top30_num.item(), sift_in_cand_top30_num.item(), 
                                siftnet_in_pointdis_mean.item(), siftnet_in_pointdis_var.item(), siftnet_in_pointdis_std.item(), sift_in_pointdis_mean.item(), sift_in_pointdis_var.item(),sift_in_pointdis_std.item(),
                                siftnet_in_oridiff_mean.item(), siftnet_in_oridiff_var.item(), siftnet_in_oridiff_std.item(), sift_in_oridiff_mean.item(), sift_in_oridiff_var.item(), sift_in_oridiff_std.item(),
                                siftnet_in_hamdis_top30_mean.item(), siftnet_in_hamdis_top30_var.item(), siftnet_in_hamdis_top30_std.item(), siftnet_in_hamdis_top30max.item(), sift_in_hamdis_top30_mean.item(), sift_in_hamdis_top30_var.item(), sift_in_hamdis_top30_std.item(), sift_in_hamdis_top30max.item(), int(sample['onescore'])])

            if idx == 0:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = net_match_hanming_dis
                    net_nomatch_hanming_dis_all = net_nomatch_hanming_dis
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = siftnet_match_hanming_dis
                siftnet_nomatch_hanming_dis_all = siftnet_nomatch_hanming_dis
                sift_match_hanming_dis_all = sift_match_hanming_dis
                sift_nomatch_hanming_dis_all = sift_nomatch_hanming_dis
            else:
                if self.test_netp_flag:
                    net_match_hanming_dis_all = torch.cat((net_match_hanming_dis_all, net_match_hanming_dis), dim=-1)
                    net_nomatch_hanming_dis_all = torch.cat((net_nomatch_hanming_dis_all, net_nomatch_hanming_dis), dim=-1)
                else:
                    net_match_hanming_dis_all = None
                    net_nomatch_hanming_dis_all = None
                siftnet_match_hanming_dis_all = torch.cat((siftnet_match_hanming_dis_all, siftnet_match_hanming_dis), dim=-1)
                siftnet_nomatch_hanming_dis_all = torch.cat((siftnet_nomatch_hanming_dis_all, siftnet_nomatch_hanming_dis), dim=-1)
                sift_match_hanming_dis_all = torch.cat((sift_match_hanming_dis_all, sift_match_hanming_dis), dim=-1)
                sift_nomatch_hanming_dis_all = torch.cat((sift_nomatch_hanming_dis_all, sift_nomatch_hanming_dis), dim=-1)
            
        return siftnet_match_hanming_dis_all, sift_match_hanming_dis_all, content_hit, content_hit_in

    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)
        
        # FR:

        siftnet_match_hanming_dis_all, sift_match_hanming_dis_all, content_FR_hit, content_FR_hit_in = self.get_hit_matchDis(FPDT, self.sift_succ_dict, repeat_dis=3, phase='FR', remove_border_r=2, require_image=False, sift_in_mask_all=None)

        
        df_FR_hit = pd.DataFrame(content_FR_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'sift_onescore'
                    ])
        df_FR_hit.to_csv(os.path.join(self.output_dir, 'results_FR_hit.csv'))
        print('\n')

        # FA 
        # 内点对
        sift_inliers_message = np.load(self.sift_inliers_path, allow_pickle=True).item()
        sift_inliers_mask_all = sift_inliers_message['inliers_mask']

        siftnet_nomatch_hanming_dis_all, sift_nomatch_hanming_dis_all, content_FA_hit, content_FA_hit_in = self.get_hit_matchDis(FPDT, self.sift_FAsucc_dict, repeat_dis=3, phase='FA', remove_border_r=2, require_image=False, sift_in_mask_all=sift_inliers_mask_all)

        df_FA_hit = pd.DataFrame(content_FA_hit, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'sift_onescore'
                    ])
        df_FA_hit.to_csv(os.path.join(self.output_dir, 'results_FA_hit.csv'))

        df_FA_hit_in = pd.DataFrame(content_FA_hit_in, 
                columns=[
                    'nameA',
                    'nameB',
                    'trans_angle',
                    'net_hit_ratio',
                    'siftnet_hit_ratio',
                    'sift_hit_ratio',
                    'net_nn_num',
                    'sift_nn_num',
                    'net_cand_num',
                    'siftnet_cand_num',
                    'sift_cand_num',
                    'siftnet_pointdis_mean',
                    'siftnet_pointdis_var',
                    'siftnet_pointdis_std',
                    'sift_pointdis_mean',
                    'sift_pointdis_var',
                    'sift_pointdis_std',
                    'siftnet_oridiff_mean',
                    'siftnet_oridiff_var',
                    'siftnet_oridiff_std',
                    'sift_oridiff_mean',
                    'sift_oridiff_var',
                    'sift_oridiff_std',
                    'siftnet_hamdis_top30_mean',
                    'siftnet_hamdis_top30_var',
                    'siftnet_hamdis_top30_std',
                    'siftnet_hamdis_top30max',
                    'sift_hamdis_top30_mean',
                    'sift_hamdis_top30_var',
                    'sift_hamdis_top30_std',
                    'sift_hamdis_top30max',
                    'sift_onescore'
                    ])
        df_FA_hit_in.to_csv(os.path.join(self.output_dir, 'results_FA_hit_in.csv'))
        
        net_match_hanming_dis_all = None
        net_nomatch_hanming_dis_all = None

        content = []
        net_match_max, net_match_min, net_match_mean, net_match_std = self.cal_max_min_mean(net_match_hanming_dis_all, phase='net_match') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        net_nomatch_max, net_nomatch_min, net_nomatch_mean, net_nomatch_std = self.cal_max_min_mean(net_nomatch_hanming_dis_all, phase='net_nomatch') if self.test_netp_flag else (0, 0, 0, 0)
        print('-------------------------------')
        sift_match_max, sift_match_min, sift_match_mean, sift_match_std = self.cal_max_min_mean(sift_match_hanming_dis_all, phase='sift_match')
        print('-------------------------------')
        sift_nomatch_max, sift_nomatch_min, sift_nomatch_mean, sift_nomatch_std = self.cal_max_min_mean(sift_nomatch_hanming_dis_all, phase='sift_nomatch')
        print('-------------------------------')
        siftnet_match_max, siftnet_match_min, siftnet_match_mean, siftnet_match_std = self.cal_max_min_mean(siftnet_match_hanming_dis_all, phase='siftnet_match')
        print('-------------------------------')
        siftnet_nomatch_max, siftnet_nomatch_min, siftnet_nomatch_mean, siftnet_nomatch_std = self.cal_max_min_mean(siftnet_nomatch_hanming_dis_all, phase='siftnet_nomatch')

        content.append(['Max', net_nomatch_max, net_match_max, sift_nomatch_max, sift_match_max, siftnet_nomatch_max, siftnet_match_max])
        content.append(['Min', net_nomatch_min, net_match_min, sift_nomatch_min, sift_match_min, siftnet_nomatch_min, siftnet_match_min])
        content.append(['Mean', net_nomatch_mean, net_match_mean, sift_nomatch_mean, sift_match_mean, siftnet_nomatch_mean, siftnet_match_mean])
        content.append(['Std', net_nomatch_std, net_match_std, sift_nomatch_std, sift_match_std, siftnet_nomatch_std, siftnet_match_std])

        if self.test_netp_flag:
            net_nomatch_hanming_dis_sort, _ = torch.sort(net_nomatch_hanming_dis_all)
            net_match_ones = torch.ones_like(net_match_hanming_dis_all)
            net_match_num = net_match_hanming_dis_all.shape[0]
            net_nomatch_num = net_nomatch_hanming_dis_all.shape[0]

        sift_nomatch_hanming_dis_sort, _ = torch.sort(sift_nomatch_hanming_dis_all)
        sift_match_ones = torch.ones_like(sift_match_hanming_dis_all)
        sift_match_num = sift_match_hanming_dis_all.shape[0]
        sift_nomatch_num = sift_nomatch_hanming_dis_all.shape[0]

        siftnet_nomatch_hanming_dis_sort, _ = torch.sort(siftnet_nomatch_hanming_dis_all)
        siftnet_match_ones = torch.ones_like(siftnet_match_hanming_dis_all)
        siftnet_match_num = siftnet_match_hanming_dis_all.shape[0]
        siftnet_nomatch_num = siftnet_nomatch_hanming_dis_all.shape[0]
        begin = 5
        stop = 100
        step = 5
        for ratio in range(begin, stop + step, step):
            if self.test_netp_flag:
                net_current_thres_index = int(ratio * net_nomatch_num / 100) - 1
                net_current_thres = net_nomatch_hanming_dis_sort[net_current_thres_index]
                net_match_ratio = torch.sum(net_match_ones[net_match_hanming_dis_all < net_current_thres]) / net_match_num * 100
            else:
                net_current_thres_index, net_current_thres, net_match_ratio = 0, torch.tensor(0).to(siftnet_match_hanming_dis_all.device), torch.tensor(0).to(siftnet_match_hanming_dis_all.device)
            
            sift_current_thres_index = int(ratio * sift_nomatch_num / 100) - 1
            sift_current_thres = sift_nomatch_hanming_dis_sort[sift_current_thres_index]
            sift_match_ratio = torch.sum(sift_match_ones[sift_match_hanming_dis_all < sift_current_thres]) / sift_match_num * 100

            siftnet_current_thres_index = int(ratio * siftnet_nomatch_num / 100) - 1
            siftnet_current_thres = siftnet_nomatch_hanming_dis_sort[siftnet_current_thres_index]
            siftnet_match_ratio = torch.sum(siftnet_match_ones[siftnet_match_hanming_dis_all < siftnet_current_thres]) / siftnet_match_num * 100
    
            content.append([ratio, 
                            net_current_thres.detach().cpu().numpy(), 
                            net_match_ratio.detach().cpu().numpy(),
                            sift_current_thres.detach().cpu().numpy(), 
                            sift_match_ratio.detach().cpu().numpy(), 
                            siftnet_current_thres.detach().cpu().numpy(),
                            siftnet_match_ratio.detach().cpu().numpy()])

        # 固定阈值下区分度

        df = pd.DataFrame(content, 
                        columns=[
                            'nomatch_ratio',
                            'net_nomatch_thres',
                            'net_match',
                            'sift_nomatch_thres',
                            'sift_match',
                            'siftnet_nomatch_thres',
                            'siftnet_match',
                            ])
        df.to_csv(os.path.join(self.output_dir, 'results.csv'))


class Output_Pts_Desc(object):
    '''
    output:
    
    '''
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir'])
        self.sift_enable    = True
        self.remove_border_r  = 0

        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_limit_p20s80",
            # "6159_p20s80",
            # "6159_p21s600",
            # "6159_S1_p20s80",
            # "6159_S2_p20s80",
            # "6159_p21s600_data",
            "6159_rot2_p10s300",
        ]
        names = []
        image_paths = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            if select == "6159_p21s600_data":
                path = img_path + select + '/6159_p21s600'
                # print(os.listdir(path)[:10].sort(reverse=False))
                all_files_path = os.listdir(path)
                all_files_path.sort()
                # print(all_files_path[:3480])
                e = [path + '/' + str(p) for p in all_files_path[:3480] if str(p)[-4:] == '.csv']
                n = [(p.split('/')[-1])[:-4].replace('_enhance', '') for p in e]
            elif select == "6159_rot2_p10s300":
                path = img_path + select + '/img_enhance_data'
                # print(os.listdir(path)[:10].sort(reverse=False))
                all_files_path = []
                for n1, _, n3 in os.walk(path):
                    if 'L' in n1 or 'R' in n1:
                        all_files_path.extend([n1 + '/' + fn for fn in n3])
                all_files_path.sort()
                # print(len(all_files_path))
                # print(all_files_path[:1800])
                # print(all_files_path[:3480])
                e = [str(p) for p in all_files_path[:1800] if str(p)[-4:] == '.bmp']
                n = [(p.split('/')[-3] + '_' + p.split('/')[-2] + '_' + (p.split('/')[-1])[:-4].replace('_enhance', '')) for p in e]
            else:
                path = img_path + select + '/images'
                all_files_path = os.listdir(path)
                all_files_path.sort()
                e = [str(p) for p in all_files_path if str(p)[-4:] == '.bmp']
                n = [(p.split('/')[-1])[:-4] for p in e]
            image_paths.extend(e)
            names.extend(n)

        files = {'image_paths': image_paths, 'names': names}
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):
            sample = {'image_path': img, 'name': name}
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    def get_data(self, index):

        img_ori = load_as_float(self.samples[index]['image_path'])
        img_ori = torch.tensor(img_ori, dtype=torch.float32)
        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_data_from_csv(self, index):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(self.samples[index]['image_path'], header=None).to_numpy()
        img_np = img.astype(np.float32) / 255
        img_ori = torch.tensor(img_np[:, :36], dtype=torch.float32)

        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_pts_desc(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) / 1e4
        desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front, desc_back

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) / 1e4
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front

    def get_pts(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        return pts

    def pts_desc_decode(self, path):
        name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, names=name)
        pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
        pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
        pts_ori = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
        pts = copy.deepcopy(pts_ori)
        # if self.isDilation:
        #     pts[:, 0] += 2    # 36->32  cut 2 pixel
        #     mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
        # else:
        #     pts[:, 0] -= 2
        #     mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] <= 33)
        # pts = pts[mask_pts]
        desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
        desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
        desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
        desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        return pts_ori, pts, desc_front, desc_back

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans)
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def quantize_desc(self, desc):
        # hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        # wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans)
        desc_binary = torch.where(desc > 0, torch.ones_like(desc), torch.zeros_like(desc)).int()
        return desc_binary

    def generate_int_desc(self, desc):
        int32_coef = torch.tensor([2 ** n for n in reversed(range(32))]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        # print(norm)
        norm = norm.unsqueeze(-1).expand_as(descs)
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm))
        return descs

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%.10f', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def remove_border(self, pts, bord, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
        toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def test_process(self, FPDT):
        count = 0
        current_fp = ','
        desc_select_double_all = None
        fp_mask = None
        for idx in range(len(self.samples)):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            # sample = self.get_data_from_csv(idx)
            name = sample["name"]
            # print(name)
            # print(name)
            img_path = sample['image_path']
            img = sample["image"].unsqueeze(0).unsqueeze(1)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,40]

            '''SIFT'''
            if self.sift_enable:
                # pts, desc_f, desc_b = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('enhance', '2022_7_11_get_desc').replace('images/', ''))
                
                # sift points 

                # # 600data lib
                # pts, _, desc_f, desc_b = self.pts_desc_decode(img_path.replace('_enhance', '').replace('/6159_p21s600/', '/SIFT_Point_6159_p21s600/'))
                # sift_desc = torch.cat((desc_f, desc_b), dim=-1)

                # rot lib
                pts, desc_f = self.get_pts_desc_rot(img_path.replace('_enhance.', '.')[:-16].replace('/img_enhance_data/', '/pnt_desc_data/' + name + '.csv'))
                sift_desc = copy.deepcopy(desc_f)

                pts = pts.to(self.device)

            # '''NET'''
            # heatmap, desc = FPDT.run_heatmap(img.to(self.device))
            # outputs = heatmap

            # pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)
            # pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)

            # # '''ALikeWithHard'''
            # pts, _ = FPDT.run_pts_desc_dkdwithhard(img.to(self.device), self.conf_thresh, self.nms)
            # pts = torch.tensor(pts.transpose(1, 0)[:, [0, 1]], device=self.device)     # [w, h]
            # pts = (pts + 0.5).int().float()

            '''剔除边界点'''
            if self.isDilation:
                mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
                pts = pts[mask_pts, :]
                sift_desc = sift_desc[mask_pts, :]

            '''还原坐标'''
            if self.isDilation:
                pts[:, 0] -= 2
                # mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] < 36)
                # desc_select = desc_select[mask_pts, :]
            elif not self.sift_enable:
                pts[:, 0] += 2
            else:
                pass

            img_o = sample['image_o'].unsqueeze(0).unsqueeze(1)       # [1, 1, 136, 36]

            if self.remove_border_r > 0:
                pts, border = self.remove_border(pts, self.remove_border_r, img_o.shape[-1], img_o.shape[-2])
            
            '''截断'''
            cut = self.top_k
            pts_mask = torch.ones(cut, 1)
            if pts.shape[0] >= cut:
                pts = pts[:cut, :]
                sift_desc = sift_desc[:cut, :]
            else:
                pts_mask[pts.shape[0]:, :] = 0
                pts = torch.cat((pts, torch.zeros(cut-pts.shape[0], 2).to(pts.device) + W // 2), dim=0)
                sift_desc = torch.cat((sift_desc, torch.zeros(cut-sift_desc.shape[0], sift_desc.shape[1]).to(sift_desc.device)), dim=0)
                
            # desc_select = desc[0, :, pts[:, 1].long(), pts[:, 0].long()].transpose(1, 0)
            # print(pts.shape)
            pts_mask = pts_mask.unsqueeze(0)  
            # sift_desc = sift_desc.unsqueeze(0)     
            # 中心对齐

            # 中心对齐+校准
            desc_select = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(img_o.to(self.device), pts.unsqueeze(0), False, False)
            # 量化
            desc_select_quantize = self.quantize_desc_hadama((desc_select + 1) / 2)
            # 门限化
            sift_desc_thr = self.thresholding_desc(sift_desc)
            sift_desc_quantize = self.quantize_desc_hadama(sift_desc_thr)

            # desc_select_quantize = self.quantize_desc(desc_select)
            # desc_select_double = torch.cat((desc_select[:,:64], desc_select[:,:64], desc_select[:,64:], desc_select[:,64:]), dim=-1)
            desc_select_double = torch.cat((desc_select_quantize, desc_select_quantize), dim=-1).unsqueeze(0)
            # sift_desc_double = copy.deepcopy(sift_desc)
            
            sift_desc_double = torch.cat((sift_desc_quantize, sift_desc_quantize), dim=-1).unsqueeze(0)
            # desc_select_double = (desc_select_double + 1) / 2     # [-1, 1] -> [0, 1]

            if current_fp in sample['name']:
                desc_select_double_all = torch.cat((desc_select_double_all, desc_select_double), dim=0)
                sift_desc_all = torch.cat((sift_desc_all, sift_desc_double), dim=0)
                fp_mask = torch.cat((fp_mask, pts_mask), dim=0)
            else:
                # save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])
                if current_fp != ',':
                    desc_select_double_all = desc_select_double_all.cpu().numpy()
                    sift_desc_all = sift_desc_all.cpu().numpy()
                    np.save(Path(self.output_dir, current_fp + "_desc.npy"), desc_select_double_all)
                    np.save(Path(self.output_dir, current_fp + "_sift_desc.npy"), sift_desc_all)
                    np.save(Path(self.output_dir, current_fp + "_mask.npy"), fp_mask)
                current_fp = name.replace('_enhance', '').split('_')[0] + '_' + name.replace('_enhance', '').split('_')[1]
                desc_select_double_all = copy.deepcopy(desc_select_double)
                sift_desc_all = copy.deepcopy(sift_desc_double)
                fp_mask = copy.deepcopy(pts_mask)


            # # SIFT
            # if self.sift_enable:
            #     desc_select_double = torch.cat((desc_f, desc_f), dim=-1)
                # desc_select_double = torch.cat((desc_f, desc_b), dim=-1)

            # # pts = pts.cpu().numpy()
            # desc_select_double = desc_select_double.cpu().numpy()
            # save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])
            # np.save(Path(save_path, "{}.npy".format()), desc_select_double)
            # self.output_txt(save_path, name, pts[:, :2], desc_select_double)
        
        # Last Finger
        desc_select_double_all = desc_select_double_all.cpu().numpy()
        sift_desc_all = sift_desc_all.cpu().numpy()
        np.save(Path(self.output_dir, current_fp + "_desc.npy"), desc_select_double_all)
        np.save(Path(self.output_dir, current_fp + "_sift_desc.npy"), sift_desc_all)
        np.save(Path(self.output_dir, current_fp + "_mask.npy"), fp_mask)

        pass


class Output_Pts_Desc_Tool(object):

    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir'])
        self.sift_enable    = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w  = 0
        self.remove_border_h  = 0

        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_limit_p20s80",
            # "6159_p20s80",
            # "6159_p21s600",
            # "6159_S1_p20s80",
            # "6159_S2_p20s80",
            # "6159_p21s600_data",
            # "db_811_p10s200",
            "6191_DK7-140_Mul_8_rot_p6",
        ]
        names = []
        image_paths = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            if select == "6159_p21s600_data":
                path = img_path + select + '/6159_p21s600'
                # print(os.listdir(path)[:10].sort(reverse=False))
                all_files_path = os.listdir(path)
                all_files_path.sort()
                # print(all_files_path[:3480])
                e = [path + '/' + str(p) for p in all_files_path[:3480] if str(p)[-4:] == '.csv']
                n = [(p.split('/')[-1])[:-4].replace('_enhance', '') for p in e]
            else:
                logging.info("add image {}.".format(select))
                path = Path(img_path, select, 'images')
                e = [str(p) for p in path.iterdir() if str(p)[-4:] == '.bmp']
                n = [(p.split('/')[-1])[:-4] for p in e]
            image_paths.extend(e)
            names.extend(n)

        files = {'image_paths': image_paths, 'names': names}
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):
            sample = {'image_path': img, 'name': name}
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    def get_data(self, index):

        img_ori = load_as_float(self.samples[index]['image_path'])
        img_ori = torch.tensor(img_ori, dtype=torch.float32)
        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_data_from_csv(self, index):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(self.samples[index]['image_path'], header=None).to_numpy()
        img_np = img.astype(np.float32) / 255
        img_ori = torch.tensor(img_np[:, :36], dtype=torch.float32)

        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_pts_desc(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) / 1e4
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor) / 1e4
        
        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        return pts

    def pts_desc_decode(self, path):
        name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, names=name)
        pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
        pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
        pts_ori = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
        pts = copy.deepcopy(pts_ori)
        # if self.isDilation:
        #     pts[:, 0] += 2    # 36->32  cut 2 pixel
        #     mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
        # else:
        #     pts[:, 0] -= 2
        #     mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] <= 33)
        # pts = pts[mask_pts]
        desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
        desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
        desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
        desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        return pts_ori, pts, desc_front, desc_back

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def quantize_desc(self, desc):
        # hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        # wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans)
        desc_binary = torch.where(desc > 0, torch.ones_like(desc), torch.zeros_like(desc)).int()
        return desc_binary

    def generate_int_desc(self, desc):
        int32_coef = torch.tensor([2 ** n for n in reversed(range(32))]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_revsersed(self, desc):
        int32_coef = torch.tensor([2 ** n for n in range(32)]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_all(self, desc):
        desc_select_thr = self.thresholding_desc(desc)
        # 量化
        desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())
        desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)
        return desc_select_quantize_int

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int()
        return descs

    def test_process(self, FPDT):
        count = 0
        current_fp = ','
        desc_select_double_all = None
        fp_mask = None
        for idx in range(len(self.samples)):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            # sample = self.get_data_from_csv(idx)
            name = sample["name"]
            # print(name)
            img_path = sample['image_path']
            img = sample["image"].unsqueeze(0).unsqueeze(1)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,40]

            '''SIFT'''
            if self.sift_enable:
                # # 原来库
                # pts, desc_f, desc_b = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('enhance', '2022_7_11_get_desc').replace('images/', ''))
                
                # # 自采库
                sift_pts, desc_f, desc_b, ori = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('Test_Tool/v_1_0/enhance', 'data').replace('images/', 'pnt_desc_data_new/'))
                # sift points 
                # pts, _, desc_f, desc_b = self.pts_desc_decode(img_path.replace('_enhance', '').replace('/6159_p21s600/', '/SIFT_Point_6159_p21s600/'))
                sift_desc = torch.cat((desc_f, desc_b), dim=-1)
                sift_pts = sift_pts.to(self.device)

            # '''NET'''
            # heatmap, desc = FPDT.run_heatmap(img.to(self.device))
            # outputs = heatmap

            # pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)
            # pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)

            # # '''ALikeWithHard'''
            # pts, _ = FPDT.run_pts_desc_dkdwithhard(img.to(self.device), self.conf_thresh, self.nms)     # [136, 32]
            # pts = torch.tensor(pts.transpose(1, 0)[:, [0, 1]], device=self.device)     # [w, h]
            # pts = (pts + 0.5).int().float()

            if not self.sift_enable:
                '''剔除边界点'''
                if self.isDilation:
                    mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
                    pts = pts[mask_pts, :]
                    sift_desc = sift_desc[mask_pts, :]

                '''还原坐标'''
                if self.isDilation:
                    pts[:, 0] -= 2
                    # mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] < 36)
                    # desc_select = desc_select[mask_pts, :]
                elif not self.sift_enable:
                    pts[:, 0] += 2
                else:
                    pass
            else:
                pts = copy.deepcopy(sift_pts)

            img_o = sample['image_o'].unsqueeze(0).unsqueeze(1)       # [1, 1, 136, 36]

            if self.remove_border_w > 0 or self.remove_border_h > 0:
                pts, border = self.remove_border(pts, self.remove_border_w, self.remove_border_h, img_o.shape[-1], img_o.shape[-2])
                if self.sift_enable:
                    sift_desc_cut = sift_desc[border, :]
                    ori_cut = ori[border]
            else:
                sift_desc_cut = copy.deepcopy(sift_desc)
                ori_cut = copy.deepcopy(ori)

            '''截断'''
            cut = self.top_k
            pts_mask = torch.ones(cut, 1)
            if pts.shape[0] >= cut:
                pts = pts[:cut, :]
                if self.sift_enable:
                    ori_cut = ori[:cut]
                    sift_desc_cut = sift_desc_cut[:cut, :]
                # sift_desc = sift_desc[:cut, :]
            # else:
            #     pts_mask[pts.shape[0]:, :] = 0
                # pts = torch.cat((pts, torch.zeros(cut-pts.shape[0], 2).to(pts.device) + W // 2), dim=0)
                # sift_desc = torch.cat((sift_desc, torch.zeros(cut-sift_desc.shape[0], sift_desc.shape[1]).to(sift_desc.device)), dim=0)
                
            # desc_select = desc[0, :, pts[:, 1].long(), pts[:, 0].long()].transpose(1, 0)
            # print(pts.shape)
            # pts_mask = pts_mask.unsqueeze(0)  
            # sift_desc = sift_desc.unsqueeze(0)     
            
            # # 中心对齐+校准
            if self.isdense:
                desc_select_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                desc_select_quantize_int_0 = self.generate_int_desc_all(torch.round(desc_select_0 * 10000) + 5000)
                if self.has45: 
                    desc_select_45 = FPDT.detector_net.cut_patch_from_featuremap_pad(img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut, cal_angle=45)
                    
                    desc_select_quantize_int_45 = self.generate_int_desc_all(torch.round(desc_select_45 * 10000) + 5000)
                    desc_select_quantize_int = torch.cat((desc_select_quantize_int_0, desc_select_quantize_int_45), dim=-1)
                else:
                    desc_select_quantize_int = copy.deepcopy(desc_select_quantize_int_0)
                # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
            else:
                desc_select = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                desc_select_quantize_int = self.generate_int_desc_all(torch.round(desc_select * 10000) + 5000)

            # # 门限化
            # # desc_select_thr = self.thresholding_desc((desc_select + 1) / 2)
            # desc_select_thr = self.thresholding_desc(torch.round(desc_select * 10000) + 5000)
            # # 量化
            # desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())

            # # # 重排
            # # 排成sift的形式
            # # rank_indexes = torch.cat((torch.linspace(0,127,128).view(8,-1)[:, :4], torch.linspace(0,127,128).view(8,-1)[:, -4:], torch.linspace(0,127,128).view(8,-1)[:,4:-4]), dim=-1).view(-1).long()
            # # 排成前64同号位的形式
            # # rank_indexes = torch.cat((torch.linspace(0, 127, 128)[(((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()], torch.linspace(0, 127, 128)[(((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool() == False]), dim=-1).long()
            # # desc_select_quantize = desc_select_quantize[:, rank_indexes]
            # # desc_select_quantize_int = self.generate_int_desc_revsersed(desc_select_quantize)
            # desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)

            # # 中值量化
            # # desc_select_quantize = self.quantize_desc(desc_select)

            # desc_select_double = torch.cat((desc_select, desc_select), dim=-1)
            # desc_select_double = (desc_select_double + 1) / 2     # [-1, 1] -> [0, 1]

            # # # SIFT
            # if self.sift_enable:
            #     desc_select_double = copy.deepcopy(sift_desc_cut)
            #     # desc_select_double = torch.cat((desc_f, desc_b), dim=-1)
            
            # 160x36 -> 136x36
            # pts[:, 1] -= self.remove_border_h
            pts = pts.cpu().numpy()
            # 量化
            desc_select_quantize_int = desc_select_quantize_int.cpu().numpy()
            save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            self.output_txt(save_path, name, pts[:, :2], desc_select_quantize_int)

            # # float
            # desc_select_double = desc_select_double.cpu().numpy()
            # save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            # self.output_txt(save_path, name, pts[:, :2], desc_select_double)
            
        
        # # Last Finger
        # desc_select_double_all = desc_select_double_all.cpu().numpy()
        # sift_desc_all = sift_desc_all.cpu().numpy()
        # np.save(Path(self.output_dir, current_fp + "_desc.npy"), desc_select_double_all)
        # np.save(Path(self.output_dir, current_fp + "_sift_desc.npy"), sift_desc_all)
        # np.save(Path(self.output_dir, current_fp + "_mask.npy"), fp_mask)

        pass


class Output_Pts_Desc_Tool_Ext(object):

    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir'])
        self.sift_enable    = True
        self.isdense        = True
        self.has45          = True
        self.remove_border_w  = 0
        self.remove_border_h  = 0

        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_limit_p20s80",
            # "6159_p20s80",
            # "6159_p21s600",
            # "6159_S1_p20s80",
            # "6159_S2_p20s80",
            # "6159_p21s600_data",
            # "db_811_p10s200",
            "6191_DK7-140_Mul_8_rot_p6_7024",
        ]
        names = []
        image_paths = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            if select == "6159_p21s600_data":
                path = img_path + select + '/6159_p21s600'
                # print(os.listdir(path)[:10].sort(reverse=False))
                all_files_path = os.listdir(path)
                all_files_path.sort()
                # print(all_files_path[:3480])
                e = [path + '/' + str(p) for p in all_files_path[:3480] if str(p)[-4:] == '.csv']
                n = [(p.split('/')[-1])[:-4].replace('_enhance', '') for p in e]
            else:
                logging.info("add image {}.".format(select))
                path = Path(img_path, select, 'images')
                e = [str(p) for p in path.iterdir() if str(p)[-4:] == '.bmp']
                n = [(p.split('/')[-1])[:-4] for p in e]
            image_paths.extend(e)
            names.extend(n)

        files = {'image_paths': image_paths, 'names': names}
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):
            sample = {'image_path': img, 'name': name}
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    def get_data(self, index):

        img_ori = load_as_float(self.samples[index]['image_path'])
        img_ori = torch.tensor(img_ori, dtype=torch.float32)
        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            # img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
            img_aug = img_ori[4:-4, 6:-6]   # [1, 136, 40]
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_data_from_csv(self, index):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(self.samples[index]['image_path'], header=None).to_numpy()
        img_np = img.astype(np.float32) / 255
        img_ori = torch.tensor(img_np[:, :36], dtype=torch.float32)

        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_pts_desc(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) / 1e4
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor) / 1e4
        
        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        return pts

    def pts_desc_decode(self, path):
        name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, names=name)
        pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
        pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
        pts_ori = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
        pts = copy.deepcopy(pts_ori)
        # if self.isDilation:
        #     pts[:, 0] += 2    # 36->32  cut 2 pixel
        #     mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
        # else:
        #     pts[:, 0] -= 2
        #     mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] <= 33)
        # pts = pts[mask_pts]
        desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
        desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
        desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
        desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        return pts_ori, pts, desc_front, desc_back

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def quantize_desc(self, desc):
        # hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        # wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans)
        desc_binary = torch.where(desc > 0, torch.ones_like(desc), torch.zeros_like(desc)).int()
        return desc_binary

    def generate_int_desc(self, desc):
        int32_coef = torch.tensor([2 ** n for n in reversed(range(32))]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_revsersed(self, desc):
        int32_coef = torch.tensor([2 ** n for n in range(32)]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_all(self, desc):
        desc_select_thr = self.thresholding_desc(desc)
        # 量化
        desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())
        desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)
        return desc_select_quantize_int

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int()
        return descs

    def test_process(self, FPDT):
        count = 0
        current_fp = ','
        desc_select_double_all = None
        fp_mask = None
        for idx in range(len(self.samples)):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            # sample = self.get_data_from_csv(idx)
            name = sample["name"]
            # print(name)
            img_path = sample['image_path']
            img = sample["image"].unsqueeze(0).unsqueeze(1)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,136,40]

            '''SIFT'''
            if self.sift_enable:
                # # 原来库
                # pts, desc_f, desc_b = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('enhance', '2022_7_11_get_desc').replace('images/', ''))
                
                # # 自采库
                # sift_pts, desc_f, desc_b, ori = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('Test_Tool/v_1_0/enhance', 'data').replace('images/', 'pnt_desc_data/'))
                sift_pts, desc_f, desc_b, ori = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('Test_Tool/v_1_0/enhance', 'data').replace('images/', 'pnt_desc_data_new/').replace('_7024', ''))

                # sift points 
                # pts, _, desc_f, desc_b = self.pts_desc_decode(img_path.replace('_enhance', '').replace('/6159_p21s600/', '/SIFT_Point_6159_p21s600/'))
                sift_desc = torch.cat((desc_f, desc_b), dim=-1)
                sift_pts = sift_pts.to(self.device)

            # '''NET'''
            # heatmap, desc = FPDT.run_heatmap(img.to(self.device))
            # outputs = heatmap

            # pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)
            # pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)

            # # '''ALikeWithHard'''
            # pts, _ = FPDT.run_pts_desc_dkdwithhard(img.to(self.device), self.conf_thresh, self.nms)     # [136, 32]
            # pts = torch.tensor(pts.transpose(1, 0)[:, [0, 1]], device=self.device)     # [w, h]
            # pts = (pts + 0.5).int().float()

            if not self.sift_enable:
                '''剔除边界点'''
                if self.isDilation:
                    mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
                    pts = pts[mask_pts, :]
                    sift_desc = sift_desc[mask_pts, :]

                '''还原坐标'''
                if self.isDilation:
                    pts[:, 0] -= 2
                    # mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] < 36)
                    # desc_select = desc_select[mask_pts, :]
                elif not self.sift_enable:
                    pts[:, 0] += 2
                else:
                    pass
            else:
                pts = copy.deepcopy(sift_pts)

            img_o = sample['image_o'].unsqueeze(0).unsqueeze(1)       # [1, 1, 136, 36]

            if self.remove_border_w > 0 or self.remove_border_h > 0:
                pts, border = self.remove_border(pts, self.remove_border_w, self.remove_border_h, W - 4, H)
                if self.sift_enable:
                    sift_desc_cut = sift_desc[border, :]
                    ori_cut = ori[border]
            else:
                sift_desc_cut = copy.deepcopy(sift_desc)
                ori_cut = copy.deepcopy(ori)

            '''截断'''
            cut = self.top_k
            pts_mask = torch.ones(cut, 1)
            if pts.shape[0] >= cut:
                pts = pts[:cut, :]
                if self.sift_enable:
                    ori_cut = ori[:cut]
                    sift_desc_cut = sift_desc_cut[:cut, :]
                # sift_desc = sift_desc[:cut, :]
            # else:
            #     pts_mask[pts.shape[0]:, :] = 0
                # pts = torch.cat((pts, torch.zeros(cut-pts.shape[0], 2).to(pts.device) + W // 2), dim=0)
                # sift_desc = torch.cat((sift_desc, torch.zeros(cut-sift_desc.shape[0], sift_desc.shape[1]).to(sift_desc.device)), dim=0)
                
            # desc_select = desc[0, :, pts[:, 1].long(), pts[:, 0].long()].transpose(1, 0)
            # print(pts.shape)
            # pts_mask = pts_mask.unsqueeze(0)  
            # sift_desc = sift_desc.unsqueeze(0)     
            
            # # 中心对齐+校准
            if self.isdense:
                # desc_select_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                desc_select_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(img.to(self.device), img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                desc_select_quantize_int_0 = self.generate_int_desc_all(torch.round(desc_select_0 * 10000) + 5000)
                if self.has45: 
                    desc_select_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext(img.to(self.device), img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut, cal_angle=45)
                    
                    desc_select_quantize_int_45 = self.generate_int_desc_all(torch.round(desc_select_45 * 10000) + 5000)
                    desc_select_quantize_int = torch.cat((desc_select_quantize_int_0, desc_select_quantize_int_45), dim=-1)
                else:
                    desc_select_quantize_int = copy.deepcopy(desc_select_quantize_int_0)
                # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
            else:
                desc_select = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                desc_select_quantize_int = self.generate_int_desc_all(torch.round(desc_select * 10000) + 5000)

            # # 门限化
            # # desc_select_thr = self.thresholding_desc((desc_select + 1) / 2)
            # desc_select_thr = self.thresholding_desc(torch.round(desc_select * 10000) + 5000)
            # # 量化
            # desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())

            # # # 重排
            # # 排成sift的形式
            # # rank_indexes = torch.cat((torch.linspace(0,127,128).view(8,-1)[:, :4], torch.linspace(0,127,128).view(8,-1)[:, -4:], torch.linspace(0,127,128).view(8,-1)[:,4:-4]), dim=-1).view(-1).long()
            # # 排成前64同号位的形式
            # # rank_indexes = torch.cat((torch.linspace(0, 127, 128)[(((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()], torch.linspace(0, 127, 128)[(((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool() == False]), dim=-1).long()
            # # desc_select_quantize = desc_select_quantize[:, rank_indexes]
            # # desc_select_quantize_int = self.generate_int_desc_revsersed(desc_select_quantize)
            # desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)

            # # 中值量化
            # # desc_select_quantize = self.quantize_desc(desc_select)

            # desc_select_double = torch.cat((desc_select, desc_select), dim=-1)
            # desc_select_double = (desc_select_double + 1) / 2     # [-1, 1] -> [0, 1]

            # # # SIFT
            # if self.sift_enable:
            #     desc_select_double = copy.deepcopy(sift_desc_cut)
            #     # desc_select_double = torch.cat((desc_f, desc_b), dim=-1)
            
            # 160x36 -> 136x36
            # pts[:, 1] -= self.remove_border_h
            pts = pts.cpu().numpy()
            # 量化
            desc_select_quantize_int = desc_select_quantize_int.cpu().numpy()
            save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            self.output_txt(save_path, name, pts[:, :2], desc_select_quantize_int)

            # # float
            # desc_select_double = desc_select_double.cpu().numpy()
            # save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            # self.output_txt(save_path, name, pts[:, :2], desc_select_double)
            
        
        # # Last Finger
        # desc_select_double_all = desc_select_double_all.cpu().numpy()
        # sift_desc_all = sift_desc_all.cpu().numpy()
        # np.save(Path(self.output_dir, current_fp + "_desc.npy"), desc_select_double_all)
        # np.save(Path(self.output_dir, current_fp + "_sift_desc.npy"), sift_desc_all)
        # np.save(Path(self.output_dir, current_fp + "_mask.npy"), fp_mask)

        pass


class Output_Pts_Desc_Tool_Ext93(object):

    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.top_k          = config['top_k']
        self.conf_thresh    = config['detec_thre']
        self.nms            = config['nms']

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir'])
        self.sift_enable    = True
        self.is_rect128     = False

        self.isdense        = False # False
        self.has45          = False # False

        # 拼接assist模型的128维
        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.remove_border_w  = 0
        self.remove_border_h  = 0

        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_limit_p20s80",
            # "6159_p20s80",
            # "6159_p21s600",
            # "6159_S1_p20s80",
            # "6159_S2_p20s80",
            # "6159_p21s600_data",
            # "db_811_p10s200",
            # "6191_DK7-140_Mul_8_rot_p6_7024",
            # "6193_DK7_merge_test1"
            # "6193_DK7_merge_test1_9604"
            # "6193_DK7_merge_test1_9800"
            # "6193_DK7_merge_test2"
            # "6193_DK7_merge_test2_9604"
            # "6193_DK7_merge_test2_9800"
            # "6193-DK7-140-8-normal_test",
            # "6193-DK7-140-8-normal_test_9800",
            # "6193-DK7-140-8-powder_test",
            # "6193-DK7-140-8-powder_test_9800",
            # "6193-DK7-140-8-rotate_test"
            # "6193-DK7-140-8-rotate_test_9800"
            # "6193-DK7-140-8-suppress_test",
            # "6193-DK7-140-8-suppress_test_9800"
            # "6193-DK7-140-8-wet2_clear_test",
            # "6193-DK7-140-8-wet2_clear_test_9604"
            "6193-DK7-140-8-wet2_clear_test_9800"
            # "6193_DK7-140_Mul_8_rot_p6",
            # '6193_DK7_XA_rot_test',
            # '6193_DK7_rotate_8mul_merge',
            # '6193_DK7_wet_8mul_merge',
            # '6193_DK7_partialPress_8mul_merge',
            # '6193-DK7-140-8-charge_test_9800',
            # '6193-DK7-140-8-charge_test_base_9800'

        ]
        names = []
        image_paths = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            if select == "6159_p21s600_data":
                path = img_path + select + '/6159_p21s600'
                # print(os.listdir(path)[:10].sort(reverse=False))
                all_files_path = os.listdir(path)
                all_files_path.sort()
                # print(all_files_path[:3480])
                e = [path + '/' + str(p) for p in all_files_path[:3480] if str(p)[-4:] == '.csv']
                n = [(p.split('/')[-1])[:-4].replace('_enhance', '') for p in e]
            else:
                logging.info("add image {}.".format(select))
                path = Path(img_path, select, 'images')
                e = [str(p) for p in path.iterdir() if str(p)[-4:] == '.bmp']
                n = [(p.split('/')[-1])[:-4] for p in e]
            image_paths.extend(e)
            names.extend(n)

        files = {'image_paths': image_paths, 'names': names}
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):
            sample = {'image_path': img, 'name': name}
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    def get_data(self, index):

        img_ori = load_as_float(self.samples[index]['image_path'])
        img_ori = torch.tensor(img_ori, dtype=torch.float32)
        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            # img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
            img_aug = img_ori[3:-3, 6:-6]   # [1, 122, 40]
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_data_from_csv(self, index):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(self.samples[index]['image_path'], header=None).to_numpy()
        img_np = img.astype(np.float32) / 255
        img_ori = torch.tensor(img_np[:, :36], dtype=torch.float32)

        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_pts_desc(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) / 1e4
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor) / 1e4
        
        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori
    
    def get_pts_desc_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 259:263].astype(np.int64))
        desc_back   = torch.tensor(mat_info[:, 263:267].astype(np.int64))
        
        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori
    
    def get_pts_desc_rot93_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1])
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2])
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3])
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4])
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5])
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6])
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7])
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8])
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        return pts

    def pts_desc_decode(self, path):
        name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, names=name)
        pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
        pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
        pts_ori = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
        pts = copy.deepcopy(pts_ori)
        # if self.isDilation:
        #     pts[:, 0] += 2    # 36->32  cut 2 pixel
        #     mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
        # else:
        #     pts[:, 0] -= 2
        #     mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] <= 33)
        # pts = pts[mask_pts]
        desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
        desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
        desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
        desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        return pts_ori, pts, desc_front, desc_back

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def quantize_desc(self, desc):
        # hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        # wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans)
        desc_binary = torch.where(desc > 0, torch.ones_like(desc), torch.zeros_like(desc)).int()
        return desc_binary

    def generate_int_desc(self, desc):
        int32_coef = torch.tensor([2 ** n for n in reversed(range(32))]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_revsersed(self, desc):
        int32_coef = torch.tensor([2 ** n for n in range(32)]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_all(self, desc):
        desc_select_thr = self.thresholding_desc(desc)
        # 量化
        desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())
        desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)
        return desc_select_quantize_int

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int()
        return descs

    def test_process(self, FPDT):
        count = 0
        current_fp = ','
        desc_select_double_all = None
        fp_mask = None
        for idx in range(len(self.samples)):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_data(idx)
            # sample = self.get_data_from_csv(idx)
            name = sample["name"]
            # print(name)
            img_path = sample['image_path']
            img = sample["image"].unsqueeze(0).unsqueeze(1)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,122,40]

            # 判断是否被湿手指
            pnt_desc_csv_path = img_path.replace('bmp', 'csv').replace('Test_Tool/v_1_0/enhance', 'data/6193Test').replace('images/', 'pnt_desc_data/') # .replace('_9800', '')
            if os.path.exists(pnt_desc_csv_path) == False:
                continue

            '''SIFT'''
            if self.sift_enable:
                # # 原来库
                # pts, desc_f, desc_b = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('enhance', '2022_7_11_get_desc').replace('images/', ''))
                
                # # 自采库
                # sift_pts, desc_f, desc_b, ori = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('Test_Tool/v_1_0/enhance', 'data').replace('images/', 'pnt_desc_data/'))
                # 6191
                # sift_pts, desc_f, desc_b, ori = self.get_pts_desc(img_path.replace('bmp', 'csv').replace('Test_Tool/v_1_0/enhance', 'data').replace('images/', 'pnt_desc_data_new/').replace('_7024', ''))
                
                # 6193
                sift_pts, desc_f, desc_b, ori = self.get_pts_desc_siftq(pnt_desc_csv_path)

                # sift points 
                # pts, _, desc_f, desc_b = self.pts_desc_decode(img_path.replace('_enhance', '').replace('/6159_p21s600/', '/SIFT_Point_6159_p21s600/'))
                if self.is_rect128:
                    sift_desc = copy.deepcopy(desc_f)
                else:
                    sift_desc = torch.cat((desc_f, desc_b), dim=-1)
                sift_pts = sift_pts.to(self.device)

            # '''NET'''
            # heatmap, desc = FPDT.run_heatmap(img.to(self.device))
            # outputs = heatmap

            # pts = getPtsFromHeatmap(outputs.detach().cpu().squeeze(), self.conf_thresh, self.nms, soft_nms=False)  # (x,y, prob)
            # pts = torch.tensor(pts).type(torch.FloatTensor).transpose(1, 0)

            # # '''ALikeWithHard'''
            # pts, _ = FPDT.run_pts_desc_dkdwithhard(img.to(self.device), self.conf_thresh, self.nms)     # [136, 32]
            # pts = torch.tensor(pts.transpose(1, 0)[:, [0, 1]], device=self.device)     # [w, h]
            # pts = (pts + 0.5).int().float()

            if not self.sift_enable:
                '''剔除边界点'''
                if self.isDilation:
                    mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
                    pts = pts[mask_pts, :]
                    sift_desc = sift_desc[mask_pts, :]

                '''还原坐标'''
                if self.isDilation:
                    pts[:, 0] -= 2
                    # mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] < 36)
                    # desc_select = desc_select[mask_pts, :]
                elif not self.sift_enable:
                    pts[:, 0] += 2
                else:
                    pass
            else:
                pts = copy.deepcopy(sift_pts)

            img_o = sample['image_o'].unsqueeze(0).unsqueeze(1)       # [1, 1, 118, 32]

            if self.remove_border_w > 0 or self.remove_border_h > 0:
                pts, border = self.remove_border(pts, self.remove_border_w, self.remove_border_h, W - 4, H)
                if self.sift_enable:
                    sift_desc_cut = sift_desc[border, :]
                    ori_cut = ori[border]
            else:
                sift_desc_cut = copy.deepcopy(sift_desc)
                ori_cut = copy.deepcopy(ori)

            '''截断'''
            cut = self.top_k
            pts_mask = torch.ones(cut, 1)
            if pts.shape[0] >= cut:
                pts = pts[:cut, :]
                if self.sift_enable:
                    ori_cut = ori[:cut]
                    sift_desc_cut = sift_desc_cut[:cut, :]
                # sift_desc = sift_desc[:cut, :]
            # else:
            #     pts_mask[pts.shape[0]:, :] = 0
                # pts = torch.cat((pts, torch.zeros(cut-pts.shape[0], 2).to(pts.device) + W // 2), dim=0)
                # sift_desc = torch.cat((sift_desc, torch.zeros(cut-sift_desc.shape[0], sift_desc.shape[1]).to(sift_desc.device)), dim=0)
                
            # desc_select = desc[0, :, pts[:, 1].long(), pts[:, 0].long()].transpose(1, 0)
            # print(pts.shape)
            # pts_mask = pts_mask.unsqueeze(0)  
            # sift_desc = sift_desc.unsqueeze(0)     
            
            # # 中心对齐+校准
            if self.isdense:
                # [122, 40] [128, 52]
                # desc_select_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                desc_select_0 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(img.to(self.device), img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                if desc_select_0.shape[-1] == 256:
                    desc_select_quantize_int_0 = self.generate_int_desc_all(torch.round(desc_select_0[:, :128] * 5000) + 5000)
                    desc_select_quantize_int_45 = self.generate_int_desc_all(torch.round(desc_select_0[:, 128:] * 5000) + 5000)
                    desc_select_quantize_int = torch.cat((desc_select_quantize_int_0, desc_select_quantize_int_45), dim=-1)
                else:
                    desc_select_quantize_int_0 = self.generate_int_desc_all(torch.round(desc_select_0 * 5000) + 5000) # 5000
                    if self.has45: 
                        desc_select_45 = FPDT.detector_net.cut_patch_from_featuremap_pad_ext93(img.to(self.device), img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut, cal_angle=45)
                        
                        desc_select_quantize_int_45 = self.generate_int_desc_all(torch.round(desc_select_45 * 5000) + 5000) # 10000 -> 5000
                        desc_select_quantize_int = torch.cat((desc_select_quantize_int_0, desc_select_quantize_int_45), dim=-1)
                    else:
                        desc_select_quantize_int = torch.cat((desc_select_quantize_int_0, desc_select_quantize_int_0), dim=-1)
                    # desc_A_0 = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False) 
            else:
                # desc_select = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                # desc_select_quantize_int = self.generate_int_desc_all(torch.round(desc_select * 5000) + 5000)
                desc_select_0 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93(img.to(self.device), img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)
                if self.is_rect128:
                    desc_select_quantize_int = self.generate_int_desc_all(torch.round(desc_select_0 * 5000) + 5000)
                else:
                    if desc_select_0.shape[-1] == 256:
                        desc_select_quantize_int_0 = self.generate_int_desc_all(torch.round(desc_select_0[:, :128] * 5000) + 5000)
                        desc_select_quantize_int_45 = self.generate_int_desc_all(torch.round(desc_select_0[:, 128:] * 5000) + 5000)
                        # desc_select_quantize_int = torch.cat((desc_select_quantize_int_0, desc_select_quantize_int_45), dim=-1)
                    else:
                        desc_select_quantize_int_0 = self.generate_int_desc_all(torch.round(desc_select_0 * 5000) + 5000)
                        if self.is_rec:
                            if self.cat_sift:
                                desc_select_quantize_int_45 = copy.deepcopy(sift_desc_cut)
                            else:
                                desc_select_45 = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch_ext93_assist(img.to(self.device), img_o.to(self.device), pts.unsqueeze(0), False, False, sift_ori=ori_cut)[:, :128]
                                desc_select_quantize_int_45 = self.generate_int_desc_all(torch.round(desc_select_45 * 5000) + 5000)
                        else:
                            desc_select_quantize_int_45 = self.generate_int_desc_all(torch.round(desc_select_0 * 5000) + 5000)
                    desc_select_quantize_int = torch.cat((desc_select_quantize_int_0, desc_select_quantize_int_45), dim=-1)

            # # 门限化
            # # desc_select_thr = self.thresholding_desc((desc_select + 1) / 2)
            # desc_select_thr = self.thresholding_desc(torch.round(desc_select * 10000) + 5000)
            # # 量化
            # desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())

            # # # 重排
            # # 排成sift的形式
            # # rank_indexes = torch.cat((torch.linspace(0,127,128).view(8,-1)[:, :4], torch.linspace(0,127,128).view(8,-1)[:, -4:], torch.linspace(0,127,128).view(8,-1)[:,4:-4]), dim=-1).view(-1).long()
            # # 排成前64同号位的形式
            # # rank_indexes = torch.cat((torch.linspace(0, 127, 128)[(((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()], torch.linspace(0, 127, 128)[(((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool() == False]), dim=-1).long()
            # # desc_select_quantize = desc_select_quantize[:, rank_indexes]
            # # desc_select_quantize_int = self.generate_int_desc_revsersed(desc_select_quantize)
            # desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)

            # # 中值量化
            # # desc_select_quantize = self.quantize_desc(desc_select)

            # desc_select_double = torch.cat((desc_select, desc_select), dim=-1)
            # desc_select_double = (desc_select_double + 1) / 2     # [-1, 1] -> [0, 1]

            # # # SIFT
            # if self.sift_enable:
            #     desc_select_double = copy.deepcopy(sift_desc_cut)
            #     # desc_select_double = torch.cat((desc_f, desc_b), dim=-1)
            
            # 160x36 -> 136x36
            # pts[:, 1] -= self.remove_border_h
            pts = pts.cpu().numpy()
            # 量化
            desc_select_quantize_int = desc_select_quantize_int.cpu().numpy()
            # desc_select_quantize_int = sift_desc_cut.cpu().numpy()
            save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            self.output_txt(save_path, name, pts[:, :2], desc_select_quantize_int)

            # # float
            # desc_select_double = desc_select_double.cpu().numpy()
            # save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            # self.output_txt(save_path, name, pts[:, :2], desc_select_double)
            
        
        # # Last Finger
        # desc_select_double_all = desc_select_double_all.cpu().numpy()
        # sift_desc_all = sift_desc_all.cpu().numpy()
        # np.save(Path(self.output_dir, current_fp + "_desc.npy"), desc_select_double_all)
        # np.save(Path(self.output_dir, current_fp + "_sift_desc.npy"), sift_desc_all)
        # np.save(Path(self.output_dir, current_fp + "_mask.npy"), fp_mask)

        pass


class Output_Pts_Tool_Ext93(object):

    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.top_k          = 130 # config['top_k']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 2 # config['nms']
        self.repeat_dis     = 1
        self.is_expand      = True
        self.bord           = 0 if not self.is_expand else 2

        self.test_dkd       = False
        self.is9800         = False # False
        self.is91000        = True
        self.pnt_range      = -3500 # -3000 # 10000
        self.partial_filter = True

        self.use_siftkp     = False
        self.use_netori     = False # True

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir'])
        self.sift_enable    = True

        self.is_rect128     = False

        self.isdense        = False # False
        self.has45          = False # False

        # 拼接assist模型的128维
        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.remove_border_w  = 0
        self.remove_border_h  = 0

        print("load dataset_files from: ", img_path)
        gallery = [
            # "6159_cd_p11s400",
            # "6159_limit_p20s80",
            # "6159_p20s80",
            # "6159_p21s600",
            # "6159_S1_p20s80",
            # "6159_S2_p20s80",
            # "6159_p21s600_data",
            # "db_811_p10s200",
            # "6191_DK7-140_Mul_8_rot_p6_7024",
            # "6193_DK7_merge_test1"
            # "6193_DK7_merge_test1_9604"
            # "6193_DK7_merge_test1_9800"
            # "6193_DK7_merge_test2"
            # "6193_DK7_merge_test2_9604"
            # "6193_DK7_merge_test2_9800"
            # "6193-DK7-140-8-normal_test",
            # "6193-DK7-140-8-normal_test_9800",
            # "6193-DK7-140-8-powder_test",
            # "6193-DK7-140-8-powder_test_9800",
            # "6193-DK7-140-8-rotate_test"
            # "6193-DK7-140-8-rotate_test_9800"
            # "6193-DK7-140-8-suppress_test",
            # "6193-DK7-140-8-suppress_test_9800"
            # "6193-DK7-140-8-wet2_clear_test",
            # "6193-DK7-140-8-wet2_clear_test_9604"
            # "6193-DK7-140-8-wet2_clear_test_9800"
            # "6193_DK7-140_Mul_8_rot_p6",
            # '6193_DK7_XA_rot_test',
            # '6193_DK7_XA_rot_test_9800',
            # '6193_DK7_rotate_8mul_merge',
            # '6193_DK7_wet_8mul_merge',
            # '6193_DK7_partialPress_8mul_merge',
            # '6193-DK7-140-8-charge_test_9800',
            # '6193-DK7-140-8-charge_test_base_9800'
            '6193-DK4-130-purple-suppress-SNR28_91100'
            # '6193-DK4-110-tarnish-normal-SNR80_91100'
            # '6193-DK4-130-purple-wet_test_91100'

        ]
        names = []
        image_paths = []
        for select in gallery:
            logging.info("add image {}.".format(select))
            if select == "6159_p21s600_data":
                path = img_path + select + '/6159_p21s600'
                # print(os.listdir(path)[:10].sort(reverse=False))
                all_files_path = os.listdir(path)
                all_files_path.sort()
                # print(all_files_path[:3480])
                e = [path + '/' + str(p) for p in all_files_path[:3480] if str(p)[-4:] == '.csv']
                n = [(p.split('/')[-1])[:-4].replace('_enhance', '') for p in e]
            else:
                logging.info("add image {}.".format(select))
                path = Path(img_path, select, 'images')
                e = [str(p) for p in path.iterdir() if str(p)[-4:] == '.bmp']
                n = [(p.split('/')[-1])[:-4] for p in e]
            image_paths.extend(e)
            names.extend(n)

        files = {'image_paths': image_paths, 'names': names}
        sequence_set = []
        for (img, name) in zip(files['image_paths'], files['names']):
            sample = {'image_path': img, 'name': name}
            sequence_set.append(sample)
        self.samples = sequence_set

        pass

    def get_data(self, index):

        img_ori = load_as_float(self.samples[index]['image_path'])
        img_ori = torch.tensor(img_ori, dtype=torch.float32)
        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            # img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
            img_aug = img_ori[3:-3, 6:-6]   # [1, 122, 40]
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_pnt_data(self, index):

        img_ori = load_as_float(self.samples[index]['image_path'])
        img_ori = torch.tensor(img_ori, dtype=torch.float32)
        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            # img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
            # img_aug = img_ori[3:-3, 6:-6]   # [1, 122, 40]
            if not self.is9800:
                if self.is_expand:
                    pad_size = (2, 2, 3, 3)     
                    img_aug = F.pad(img_ori.unsqueeze(0), pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
                else:
                    pad_size = (2, 2, 0, 0)     
                    img_aug = F.pad(img_ori.unsqueeze(0), pad_size, "constant", 0)    # 122 x 36 -> 122 x 40
            else:
                img_aug = img_ori[:, 6:-6].unsqueeze(0)      # 128 x 52 -> 128 x 40
                # img_o_A = img_o_A[0, 3:-3, 8:-8].unsqueeze(0)   # 128 x 52 -> 122 x 36
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        # load mask
        last_name = str(self.samples[index]['image_path']).split('/')[-1] 
        new_last_name = '/'.join(last_name.split('_')).replace('.bmp', '_mask.bmp')
        img_mask_path = str(self.samples[index]['image_path']).replace('Test_Tool/v_1_0/pnt', 'data/6193Test').replace('images/', 'img_mask_data/').replace(last_name, new_last_name).replace('_91100', '_9800') 
        # print(img_mask_path)
        img_mask = None
        if os.path.exists(img_mask_path):
            img_mask = load_as_float(img_mask_path)
            img_mask = torch.tensor(img_mask, dtype=torch.float32).unsqueeze(0)

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})
        input.update({'image_mask': img_mask})

        return input

    def get_pnt_data_debase(self, index):

        img_ori = load_as_float(self.samples[index]['image_path'])
        img_ori = torch.tensor(img_ori, dtype=torch.float32)
        import torch.nn.functional as F
        img_ori = F.pad(img_ori.unsqueeze(0), (2, 2, 2, 2) , "constant", 0).squeeze(0) # 118 x 32 -> 122 x 36

        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:

            # img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
            # img_aug = img_ori[3:-3, 6:-6]   # [1, 122, 40]
            if not self.is9800:
                if self.is_expand:
                    pad_size = (2, 2, 3, 3)     
                    img_aug = F.pad(img_ori.unsqueeze(0), pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
                else:
                    pad_size = (2, 2, 0, 0)     
                    img_aug = F.pad(img_ori.unsqueeze(0), pad_size, "constant", 0)    # 122 x 36 -> 122 x 40
            else:
                img_aug = img_ori[:, 6:-6].unsqueeze(0)      # 128 x 52 -> 128 x 40
                # img_o_A = img_o_A[0, 3:-3, 8:-8].unsqueeze(0)   # 128 x 52 -> 122 x 36
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        # load mask
        last_name = str(self.samples[index]['image_path']).split('/')[-1] 
        new_last_name = '/'.join(last_name.split('_')).replace('.bmp', '_mask.bmp')
        img_mask_path = str(self.samples[index]['image_path']).replace('Test_Tool/v_1_0/pnt_debase', 'data/6193Test').replace('images/', 'img_mask_data/').replace(last_name, new_last_name).replace('_91100', '_9800') 
        # print(img_mask_path)
        img_mask = None
        if os.path.exists(img_mask_path):
            img_mask = load_as_float(img_mask_path)
            img_mask = torch.tensor(img_mask, dtype=torch.float32).unsqueeze(0)

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})
        input.update({'image_mask': img_mask})

        return input


    def get_data_from_csv(self, index):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        
        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(self.samples[index]['image_path'], header=None).to_numpy()
        img_np = img.astype(np.float32) / 255
        img_ori = torch.tensor(img_np[:, :36], dtype=torch.float32)

        '''传统增强扩边处理-逆：36->32'''  
        if self.isDilation:
            import torch.nn.functional as F
            img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        else:
            img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        input  = {}
        input.update({'image': img_aug})
        input.update({'image_path': self.samples[index]['image_path']})
        input.update({'image_o': img_ori})
        input.update({'name': self.samples[index]['name']})

        return input

    def get_pts_desc(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) / 1e4
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor) / 1e4
        
        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori
    
    def get_pts_desc_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 259:263].astype(np.int64))
        desc_back   = torch.tensor(mat_info[:, 263:267].astype(np.int64))
        
        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori
    
    def get_pts_desc_dbvalue_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 259:263].astype(np.int64))
        desc_back   = torch.tensor(mat_info[:, 263:267].astype(np.int64))
        dbvalue   = torch.tensor(mat_info[:, 3].astype(np.int64))
        
        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori, dbvalue
    

    def get_pts_desc_rot93_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1])
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2])
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3])
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4])
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5])
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6])
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7])
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8])
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        return pts

    def pts_desc_decode(self, path):
        name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, names=name)
        pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
        pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
        pts_ori = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
        pts = copy.deepcopy(pts_ori)
        # if self.isDilation:
        #     pts[:, 0] += 2    # 36->32  cut 2 pixel
        #     mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 37)
        # else:
        #     pts[:, 0] -= 2
        #     mask_pts = (pts[:, 0] >= 0) * (pts[:, 0] <= 33)
        # pts = pts[mask_pts]
        desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
        desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
        desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
        desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
        desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        return pts_ori, pts, desc_front, desc_back

    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def quantize_desc(self, desc):
        # hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        # wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans)
        desc_binary = torch.where(desc > 0, torch.ones_like(desc), torch.zeros_like(desc)).int()
        return desc_binary

    def generate_int_desc(self, desc):
        int32_coef = torch.tensor([2 ** n for n in reversed(range(32))]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_revsersed(self, desc):
        int32_coef = torch.tensor([2 ** n for n in range(32)]).to(torch.int32).to(self.device)
        desc = desc.contiguous()
        desc_seg = desc.view(-1, 32).to(self.device)
        desc_int23_sum = torch.sum(desc_seg * int32_coef, dim=-1).view(-1, 4)
        return desc_int23_sum

    def generate_int_desc_all(self, desc):
        desc_select_thr = self.thresholding_desc(desc)
        # 量化
        desc_select_quantize = self.quantize_desc_hadama(desc_select_thr.float())
        desc_select_quantize_int = self.generate_int_desc(desc_select_quantize)
        return desc_select_quantize_int

    def output_txt(self, root_path, file_name, points: np.array=None, descs: np.array=None):

        if points is not None:
            save_path = Path(root_path, 'point')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), points, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)
        
        if descs is not None:
            save_path = Path(root_path, 'desc')
            os.makedirs(save_path, exist_ok=True)
            np.savetxt(Path(save_path, str(file_name) + '.txt'), descs, fmt='%d', delimiter=',', newline='\n', header='', footer='', encoding=None)

        pass

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int()
        return descs

    def test_process(self, FPDT):
        count = 0
        current_fp = ','
        desc_select_double_all = None
        fp_mask = None
        for idx in range(len(self.samples)):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / len(self.samples))), end="\r")

            sample = self.get_pnt_data_debase(idx)
            # sample = self.get_data_from_csv(idx)
            name = sample["name"]
            # print(name)
            img_path = sample['image_path']
            img = sample["image"].unsqueeze(0)
            img_partialmask = sample['image_mask'].unsqueeze(0)
            img = img.to(self.device)
            H, W = img.shape[2], img.shape[3]   # img:[1,1,122,40]
            # print(H, W)

            # 判断是否被湿手指 pnt_debase
            pnt_desc_csv_path = img_path.replace('bmp', 'csv').replace('Test_Tool/v_1_0/pnt_debase', 'data/6193Test').replace('images/', 'pnt_desc_data/').replace('_91100', '_9800')
            # .replace('_9800', '')
            if os.path.exists(pnt_desc_csv_path) == False:
                continue

            if self.test_dkd:
                pts_ori = FPDT.run_pts_alike(img.to(self.device))   # 120 X 40
            else:
                # pass through network
                if not self.is9800:
                    if self.is_expand:
                        heatmap, _ = FPDT.run_heatmap_alike_9800(img.to(self.device))   # 128 X 40
                        
                        intensity, pnmap = None, None
                        if self.use_netori and heatmap.shape[0] == 3:
                            intensity = heatmap[1, 3:-3, 2:-2]
                            pnmap = heatmap[2, 3:-3, 2:-2]
                        
                        heatmap = heatmap[0, 3:-3, 2:-2]           # 128 x 40 -> 122 x 36
                        if self.partial_filter:
                            heatmap = heatmap * img_partialmask.to(heatmap.device)                   
                    else:
                        heatmap, _ = FPDT.run_heatmap_alike(img.to(self.device))   # 120 X 40
                else:
                    heatmap, _ = FPDT.run_heatmap_alike_9800(img.to(self.device))   # 128 X 40
                
                if self.use_siftkp:
                    sift_pts, _, _, ori, sift_dbvalues = self.get_pts_desc_dbvalue_siftq(pnt_desc_csv_path)
                    pts_ori = torch.cat((sift_pts, sift_dbvalues.unsqueeze(1)), dim=-1).transpose(0, 1)
                    # print(pts_ori.shape)    # 3 x 130 
                    # exit()
                else:
                    pts_ori = getPtsFromHeatmap(heatmap.squeeze().detach().cpu().numpy(), self.conf_thresh, self.nms, soft_nms=False, bord=self.bord)     # 120 x 40
                            
            '''后续计算均还原到原图136*36上'''
            if self.isDilation:
                # img = sample['img_o'].unsqueeze(0) # 122 x 36
                if not self.is9800:
                    if not self.is_expand:
                        mask_pts = (pts_ori[0, :] >= 4) * (pts_ori[0, :] <= 35) * (pts_ori[1, :] >= 1) * (pts_ori[1, :] <= 118)        # 122 x 36 中心 118 x 32
                        pts_ori = pts_ori[:, mask_pts]
                        pts_ori[0, :] -= 2    # 40 -> 36
                        pts_ori[1, :] += 1    # 120 -> 122
                else:
                    mask_pts = (pts_ori[0, :] >= 4) * (pts_ori[0, :] <= 35) * (pts_ori[1, :] >= 5) * (pts_ori[1, :] <= 122)        # 128 x 40 中心 118 x 32
                    pts_ori = pts_ori[:, mask_pts]
                    pts_ori[0, :] -= 2    # 40 -> 36
                    pts_ori[1, :] -= 3    # 128 -> 122

            else:
                # img = sample['img_o'].unsqueeze(0) # 122 x 36
                pts_ori[0, :] += 2

            '''截断'''
            if self.top_k:
                if pts_ori.shape[1] > self.top_k:
                    pts_ori = pts_ori[:, :self.top_k]

            # # A,B点集
            # H, W = img.shape[2], img.shape[3]   # img:[1,1,122,36]
            
            # 网络角度
            net_ori = None
            if self.is_expand:
                H_o, W_o = H - 6, W - 4      # 128x40 -> 122x36 
            points_o = torch.tensor(pts_ori.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            if self.use_netori and intensity is not None and pnmap is not None:
                points_o_normalized = points_o / points_o.new_tensor([W_o - 1, H_o - 1]).to(points_o.device) * 2 - 1  # (w,h) -> (-1~1,-1~1)

                ori_kp_predict_all = F.grid_sample(intensity.view(-1, H_o, W_o).unsqueeze(0),
                                        points_o_normalized.float().view(1, 1, -1, 2),
                                        mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
            
                pn_kp_predict_all = F.grid_sample(pnmap.view(-1, H_o, W_o).unsqueeze(0),
                                        points_o_normalized.float().view(1, 1, -1, 2),
                                        mode='bilinear', align_corners=True).squeeze(2).squeeze(0).transpose(0, 1)          # NTA x 2
                # 正负号mask
                pn_predict = (pn_kp_predict_all > 0.5).float().squeeze()   

                # print(pn_predict.shape, ori_kp_predict_all[:, 0])
                # 强度loss
                pai_coef = 3.1415926
                net_ori = (ori_kp_predict_all[:, 0] * (2 * pn_predict - 1) * 90 + 90) / 180 * pai_coef * 4096  # 转成short类型
                
            # dbvalue 范围[0, self.pnt_range]
            if not self.use_siftkp:
                pts_ori[2,:] = pts_ori[2,:] * self.pnt_range     
            pts = torch.tensor(pts_ori).transpose(1, 0).to(self.device)
            
            # x, y, ori, dbvalue
            if self.use_netori and net_ori is not None:
                # print(net_ori.device, pts.device)
                # exit()
                pts = torch.cat((pts[:, :2], net_ori.unsqueeze(-1), pts[:, -1].unsqueeze(-1)), dim=-1)
            pts = pts.cpu().numpy()

            # 量化
            # desc_select_quantize_int = desc_select_quantize_int.cpu().numpy()
            # desc_select_quantize_int = sift_desc_cut.cpu().numpy()
            save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            self.output_txt(save_path, name, pts, None)

            # # float
            # desc_select_double = desc_select_double.cpu().numpy()
            # save_path = Path(self.output_dir, self.samples[idx]['image_path'].split('/')[-3])

            # self.output_txt(save_path, name, pts[:, :2], desc_select_double)
            
        pass



class Get_Descriptor_Parameter_Smaller(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']

    def test_process(self, FPDT):
        net = FPDT.detector_net.descriptor_net
        net.eval()
        net.to('cpu')
        preconv3x3 = ['conv3x3_pre']

        block1 = ['conv1x1s1_di_1', 'convdw3x3s1_1', 'conv1x1s1_dd_1',
                'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        block2 = ['conv1x1s1_di_2', 'convdw3x3s2_2', 'conv1x1s1_dd_2',
                'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        block3 = ['conv1x1s1_di_3', 'convdw3x3s2_3', 'conv1x1s1_dd_3',
                'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']

        postconv1x1 = ['conv1x1_post']

        block = [preconv3x3, block1, block2, block3, postconv1x1]

        name = []
        for i in block:
            name.extend(i)
        def list_layers(layer):
            layers = []
            if isinstance(layer, Block) or isinstance(layer, nn.Sequential) or isinstance(layer, SeModule):
                for i in layer.children():
                    layers.extend(list_layers(i))
            elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ConvTranspose2d):
                layers.append(layer)
            return layers
        
        def get_parameters_layer(net):
            layers = []
            queue = deque()
            for i in net.children():
                queue.append(i)
            while len(queue):
                root = queue.popleft()
                layers.extend(list_layers(root))
            return layers

        print(net)
        layers = get_parameters_layer(net.module)
        params_num = 0
        count = 0
        res = '#if 1\n'
        for i in range(len(layers)):
            conv = None
            if i + 1 < len(layers) and isinstance(layers[i], nn.Conv2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                conv = nn.utils.fusion.fuse_conv_bn_eval(layers[i], layers[i + 1])
            elif i + 1 < len(layers) and isinstance(layers[i], nn.ConvTranspose2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                # print(layers[i].weight.size())
                # print(layers[i].weight.transpose(0,1)[0,2,:,:])
                fused_deconv = copy.deepcopy(layers[i])
                fused_deconv.weight = torch.nn.Parameter(torch.transpose(layers[i].weight, 0, 1))
                # print(fused_deconv.weight.size())
                # print(fused_deconv.weight[0,2,:,:])
                # exit()
                conv = nn.utils.fusion.fuse_conv_bn_eval(fused_deconv, layers[i + 1])
            elif isinstance(layers[i], nn.Conv2d):
                conv = layers[i]
            if conv is not None:
                convw, convb = conv.weight.detach().numpy(
                ).flatten(), conv.bias.detach().numpy().flatten()
                params_num += convw.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_weight[{0}] = '.format(
                        convw.flatten().shape[0]) + '{ \n'
                if convw.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convw.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convw, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                params_num += convb.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_bias[{0}] = '.format(
                        convb.flatten().shape[0]) + '{ \n'
                if convb.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                elif convb.shape[0] % 4 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 4), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convb, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'
                count += 1
        print(count)
        res += '#endif\n'
        res += '//desc_parameters: {:d}\n'.format(params_num)
        with open(self.output_dir + '0816_smaller_patch_desc.h', 'w') as f:
            f.write(res)
        Path(r'param.txt').unlink()


class Get_Descriptor_Parameter_Half(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']

    def test_process(self, FPDT):
        net = FPDT.detector_net.descriptor_net
        net.eval()
        net.to('cpu')

        block1 = ['conv1x1s1_di_1', 'convdw3x3s2_1', 'conv1x1s1_dd_1',
                'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        block2 = ['conv1x1s1_di_2', 'convdw3x3s2_2', 'conv1x1s1_dd_2',
                'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
                'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3', 'conv1x1_di_shortcut_3']
        block4 = ['conv1x1s1_di_4', 'convdw3x3s1_4', 'conv1x1s1_dd_4',
                'conv1x1s1_dd_se_4', 'conv1x1s1_di_se_4', 'conv1x1_di_shortcut_4']
        block5 = ['conv1x1s1_di_5', 'convdw3x3s1_5', 'conv1x1s1_dd_5',
                'conv1x1s1_dd_se_5', 'conv1x1s1_di_se_5']
        block6 = ['conv1x1s1_di_6', 'convdw3x3s1_6', 'conv1x1s1_dd_6',
                'conv1x1s1_dd_se_6', 'conv1x1s1_di_se_6', 'conv1x1_di_shortcut_6']
                
        postconv1x1 = ['conv1x1_post']

        block = [block1, block2, block3, block4, block5, block6, postconv1x1]

        name = []
        prefix = "para_patch_"
        for i in block:
            name.extend(i)
        for i in range(len(name)):
            name[i] = prefix + name[i]

        def list_layers(layer):
            layers = []
            if isinstance(layer, Block) or isinstance(layer, nn.Sequential) or isinstance(layer, SeModule):
                for i in layer.children():
                    layers.extend(list_layers(i))
            elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ConvTranspose2d):
                layers.append(layer)
            return layers
        
        def get_parameters_layer(net):
            layers = []
            queue = deque()
            for i in net.children():
                queue.append(i)
            while len(queue):
                root = queue.popleft()
                layers.extend(list_layers(root))
            return layers

        # print(net)
        layers = get_parameters_layer(net)
        params_num = 0
        params_short_num = 0
        count = 0
        # conw_short_list = []
        # conb_short_list = []
        maxceo = []
        res = '#if 1\n'
        for i in range(len(layers)):
            conv = None
            if i + 1 < len(layers) and isinstance(layers[i], nn.Conv2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                conv = nn.utils.fusion.fuse_conv_bn_eval(layers[i], layers[i + 1])
            elif i + 1 < len(layers) and isinstance(layers[i], nn.ConvTranspose2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                # print(layers[i].weight.size())
                # print(layers[i].weight.transpose(0,1)[0,2,:,:])
                fused_deconv = copy.deepcopy(layers[i])
                fused_deconv.weight = torch.nn.Parameter(torch.transpose(layers[i].weight, 0, 1))
                # print(fused_deconv.weight.size())
                # print(fused_deconv.weight[0,2,:,:])
                # exit()
                conv = nn.utils.fusion.fuse_conv_bn_eval(fused_deconv, layers[i + 1])
            elif isinstance(layers[i], nn.Conv2d):
                conv = layers[i]
            if conv is not None:
                convw, convb = conv.weight.detach().numpy(
                ).flatten(), conv.bias.detach().numpy().flatten()
                # maxincw = int(32760 / np.maximum(np.max(convw),-np.min(convw)) - 10)
                # convw = (convw*maxincw).astype(np.int)

                # maxceo.append(maxincw)
                # #量化模型准备
                # scale = (1.0 / maxincw)
                # conw_short_list.append((conv.weight.detach()*maxincw).long() *scale)
                # conb_short_list.append(conv.bias.detach())

                # params_short_num += convw.flatten().shape[0]
                print(name[count])
                res += 'static float ' + \
                    name[count] + \
                    '_weight[{0}] = '.format(
                        convw.flatten().shape[0]) + '{ \n'
                if convw.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convw.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convw, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                params_num += convb.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_bias[{0}] = '.format(
                        convb.flatten().shape[0]) + '{ \n'
                if convb.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                elif convb.shape[0] % 4 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 4), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convb, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                count += 1

        # 保存scale
        # params_num += len(maxceo)
        # res += 'static int ' + \
        #         'weight_scale[{0}] = '.format(
        #             len(maxceo)) + '{ \n'
        # maxceo_np = np.array(maxceo)
        # if maxceo_np.shape[0] % 5 == 0:
        #     np.savetxt('param.txt', maxceo_np.reshape(-1, 8), fmt='%d',
        #                 delimiter=', ', newline=',\n')
        # else:
        #     np.savetxt('param.txt', maxceo_np, fmt='%d',
        #                 delimiter=', ', newline=',\n')
        # with open('param.txt', 'r') as f:
        #     res += f.read()
        # res += '};\n\n'

        print(count)
        memory_total = params_num * 4 / 1024
        res += '#endif\n'
        res += '//0422m_155800_desc_parameters: float[{:d}]\n'.format(params_num)
        res += '//0422m_155800_desc_memory: {:.4f} K\n'.format(memory_total)
        with open('0422m_155800_desc_result.h', 'w') as f:
            f.write(res)
        Path(r'param.txt').unlink()


class Get_Descriptor_Parameter_Short(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']

    def test_process(self, FPDT):
        net = FPDT.detector_net.descriptor_net
        net.eval()
        net.to('cpu')

        net_short = HardNet_fast_twice_half3_MO_MOE_short() # HardNet_fast_half_short() # HardNet_fast_twice_big_short() # HardNet_fast_twice_short() HardNet_fast_twice_half3_MO_short()
        net_short.eval()
        net_short.to('cpu')

        # # fast
        # block1 = ['conv1x1s1_di_1', 'convdw3x3s2_1', 'conv1x1s1_dd_1',
        #         'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        # block2 = ['conv1x1s1_di_2', 'convdw3x3s2_2', 'conv1x1s1_dd_2',
        #         'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        # block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
        #         'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']
        # block4 = ['conv1x1s1_di_4', 'convdw3x3s1_4', 'conv1x1s1_dd_4',
        #         'conv1x1s1_dd_se_4', 'conv1x1s1_di_se_4', 'conv1x1_di_shortcut_4']
        # block5 = ['conv1x1s1_di_5', 'convdw3x3s1_5', 'conv1x1s1_dd_5',
        #         'conv1x1s1_dd_se_5', 'conv1x1s1_di_se_5', 'conv1x1_di_shortcut_5']
        # block6 = ['conv1x1s1_di_6', 'convdw3x3s1_6', 'conv1x1s1_dd_6',
        #         'conv1x1s1_dd_se_6', 'conv1x1s1_di_se_6']

        # # fast twice big
        # block1 = ['conv1x1s1_di_1', 'convdw3x3s2_1', 'conv1x1s1_dd_1',
        #         'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        # block2 = ['conv1x1s1_di_2', 'convdw3x3s2_2', 'conv1x1s1_dd_2',
        #         'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        # block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
        #         'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']
        # block4 = ['conv1x1s1_di_4', 'convdw3x3s1_4', 'conv1x1s1_dd_4',
        #         'conv1x1s1_dd_se_4', 'conv1x1s1_di_se_4']
        # block5 = ['conv1x1s1_di_5', 'convdw3x3s1_5', 'conv1x1s1_dd_5',
        #         'conv1x1s1_dd_se_5', 'conv1x1s1_di_se_5', 'conv1x1_di_shortcut_5']
        # block6 = ['conv1x1s1_di_6', 'convdw3x3s1_6', 'conv1x1s1_dd_6',
        #         'conv1x1s1_dd_se_6', 'conv1x1s1_di_se_6']

        # # fast half
        # block1 = ['conv1x1s1_di_1', 'convdw3x3s2_1', 'conv1x1s1_dd_1',
        #         'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        # block2 = ['conv1x1s1_di_2', 'convdw3x3s2_2', 'conv1x1s1_dd_2',
        #         'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        # block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
        #         'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']
        # block4 = ['conv1x1s1_di_4', 'convdw3x3s1_4', 'conv1x1s1_dd_4',
        #         'conv1x1s1_dd_se_4', 'conv1x1s1_di_se_4']
        # block5 = ['conv1x1s1_di_5', 'convdw3x3s1_5', 'conv1x1s1_dd_5',
        #         'conv1x1s1_dd_se_5', 'conv1x1s1_di_se_5']
        # block6 = ['conv1x1s1_di_6', 'convdw3x3s1_6', 'conv1x1s1_dd_6',
        #         'conv1x1s1_dd_se_6', 'conv1x1s1_di_se_6']
                
        # postconv1x1 = ['conv1x1_post']

        # fast half
        wblock1 = ['w_conv1x1s1_di_1', 'w_convdw3x3s2_1', 'w_conv1x1s1_dd_1',
                'w_conv1x1s1_dd_se_1', 'w_conv1x1s1_di_se_1']
        wblock2 = ['w_conv1x1s1_di_2', 'w_convdw3x3s2_2', 'w_conv1x1s1_dd_2',
                'w_conv1x1s1_dd_se_2', 'w_conv1x1s1_di_se_2']
        wblock3 = ['w_conv1x1s1_di_3', 'w_convdw3x3s1_3', 'w_conv1x1s1_dd_3',
                'w_conv1x1s1_dd_se_3', 'w_conv1x1s1_di_se_3']
        wblock4 = ['w_conv1x1s1_di_4', 'w_convdw3x3s1_4', 'w_conv1x1s1_dd_4',
                'w_conv1x1s1_dd_se_4', 'w_conv1x1s1_di_se_4']
        wblock5 = ['w_conv1x1s1_di_5', 'w_convdw3x3s1_5', 'w_conv1x1s1_dd_5',
                'w_conv1x1s1_dd_se_5', 'w_conv1x1s1_di_se_5']
        wblock6 = ['w_conv1x1s1_di_6', 'w_convdw3x3s1_6', 'w_conv1x1s1_dd_6',
                'w_conv1x1s1_dd_se_6', 'w_conv1x1s1_di_se_6']
                
        wpostconv1x1 = ['w_conv1x1_post']

                # fast half
        bblock1 = ['b_conv1x1s1_di_1', 'b_convdw3x3s2_1', 'b_conv1x1s1_dd_1',
                'b_conv1x1s1_dd_se_1', 'b_conv1x1s1_di_se_1']
        bblock2 = ['b_conv1x1s1_di_2', 'b_convdw3x3s2_2', 'b_conv1x1s1_dd_2',
                'b_conv1x1s1_dd_se_2', 'b_conv1x1s1_di_se_2']
        bblock3 = ['b_conv1x1s1_di_3', 'b_convdw3x3s1_3', 'b_conv1x1s1_dd_3',
                'b_conv1x1s1_dd_se_3', 'b_conv1x1s1_di_se_3']
        bblock4 = ['b_conv1x1s1_di_4', 'b_convdw3x3s1_4', 'b_conv1x1s1_dd_4',
                'b_conv1x1s1_dd_se_4', 'b_conv1x1s1_di_se_4']
        bblock5 = ['b_conv1x1s1_di_5', 'b_convdw3x3s1_5', 'b_conv1x1s1_dd_5',
                'b_conv1x1s1_dd_se_5', 'b_conv1x1s1_di_se_5']
        bblock6 = ['b_conv1x1s1_di_6', 'b_convdw3x3s1_6', 'b_conv1x1s1_dd_6',
                'b_conv1x1s1_dd_se_6', 'b_conv1x1s1_di_se_6']
                
        bpostconv1x1 = ['b_conv1x1_post']

        block = [wblock1, wblock2, wblock3, wblock4, wblock5, wblock6, wpostconv1x1, bblock1, bblock2, bblock3, bblock4, bblock5, bblock6, bpostconv1x1]

        name = []
        prefix = "para_patch_"
        for i in block:
            name.extend(i)
        for i in range(len(name)):
            name[i] = prefix + name[i]

        def list_layers(layer):
            layers = []
            if isinstance(layer, Block) or isinstance(layer, Block_noshortcut) or isinstance(layer, MobileOne) or isinstance(layer, MobileOne_new) or isinstance(layer, nn.Sequential) or isinstance(layer, SeModule):
                for i in layer.children():
                    layers.extend(list_layers(i))
            # elif isinstance(layer, MobileOne):
            #     for i in layer.blocks:
            #         layers.extend(list_layers(i))
            elif isinstance(layer, MobileOneBlock) or isinstance(layer, MobileOneBlock_new):
                # 多分支参数融合
                layer.reparameterize()
                for i in layer.children():
                    layers.extend(list_layers(i))
                # layers.append(layer)
            elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ConvTranspose2d):
                layers.append(layer)
            return layers
        
        def get_parameters_layer(net):
            layers = []
            queue = deque()
            for i in net.children():
                queue.append(i)
            while len(queue):
                root = queue.popleft()
                layers.extend(list_layers(root))
            return layers

        # print(net)
        layers = get_parameters_layer(net)
        # print(layers)
        # exit()
        params_num = 0
        params_short_num = 0
        count = 0
        conw_short_list = []
        conb_short_list = []
        maxceo = []
        res = '#if 1\n'
        for i in range(len(layers)):
            conv = None
            if i + 1 < len(layers) and isinstance(layers[i], nn.Conv2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                conv = nn.utils.fusion.fuse_conv_bn_eval(layers[i], layers[i + 1])
            elif i + 1 < len(layers) and isinstance(layers[i], nn.ConvTranspose2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                # print(layers[i].weight.size())
                # print(layers[i].weight.transpose(0,1)[0,2,:,:])
                fused_deconv = copy.deepcopy(layers[i])
                fused_deconv.weight = torch.nn.Parameter(torch.transpose(layers[i].weight, 0, 1))
                # print(fused_deconv.weight.size())
                # print(fused_deconv.weight[0,2,:,:])
                # exit()
                conv = nn.utils.fusion.fuse_conv_bn_eval(fused_deconv, layers[i + 1])
            elif isinstance(layers[i], nn.Conv2d):
                conv = layers[i]
            if conv is not None:
                convw, convb = conv.weight.detach().numpy(
                ).flatten(), conv.bias.detach().numpy().flatten()
                maxincw = int(32760 / np.maximum(np.max(convw),-np.min(convw)) - 10)
                convw = (convw*maxincw).astype(np.int)

                maxceo.append(maxincw)
                #量化模型准备
                scale = (1.0 / maxincw)
                conw_short_list.append((conv.weight.detach()*maxincw).long() *scale)
                conb_short_list.append(conv.bias.detach())

                params_short_num += convw.flatten().shape[0]
                print(name[count])
                res += 'static short ' + \
                    name[count] + \
                    '_weight[{0}] = '.format(
                        convw.flatten().shape[0]) + '{ \n'
                if convw.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convw.reshape(-1, 8), fmt='%d',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convw, fmt='%d',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                params_num += convb.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_bias[{0}] = '.format(
                        convb.flatten().shape[0]) + '{ \n'
                if convb.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                elif convb.shape[0] % 4 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 4), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convb, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                count += 1

        #保存scale
        params_num += len(maxceo)
        res += 'static int ' + \
                'weight_scale[{0}] = '.format(
                    len(maxceo)) + '{ \n'
        maxceo_np = np.array(maxceo)
        if maxceo_np.shape[0] % 5 == 0:
            np.savetxt('param.txt', maxceo_np.reshape(-1, 8), fmt='%d',
                        delimiter=', ', newline=',\n')
        else:
            np.savetxt('param.txt', maxceo_np, fmt='%d',
                        delimiter=', ', newline=',\n')
        with open('param.txt', 'r') as f:
            res += f.read()
        res += '};\n\n'

        print(count)
        memory_total = (params_short_num*2+params_num*4)/1024
        res += '#endif\n'
        res += '//0709n_square_248000_desc_parameters: float[{:d}] short[{:d}]\n'.format(params_num, params_short_num)
        res += '//0709n_square_248000_desc_memory: {:.4f} K\n'.format(memory_total)
        with open('0709n_square_248000_desc_result.h', 'w') as f:
            f.write(res)
        Path(r'param.txt').unlink()

        quant_pth_save_pth = self.output_dir + '0709n_square_248000_short.pth.tar'
        net_short.model_save_quant_param(conw_short_list, conb_short_list, quant_pth_save_pth)


class Get_Descriptor_Parameter_Ecnn(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']


    # def fuse_ecnn_conv_bn_eval(self, conv, bn):
    #     assert(not (conv.training or bn.training)), "Fusion only for eval!"
    #     fused_conv = copy.deepcopy(conv)

    #     fused_conv.weight, fused_conv.bias = \
    #         self.fuse_ecnn_conv_bn_weights(fused_conv.filter, fused_conv.expanded_bias,
    #                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)

    #     return fused_conv

    # def fuse_ecnn_conv_bn_weights(self, conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):
    #     if conv_b is None:
    #         conv_b = torch.zeros_like(bn_rm)
    #     if bn_w is None:
    #         bn_w = torch.ones_like(bn_rm)
    #     if bn_b is None:
    #         bn_b = torch.zeros_like(bn_rm)
    #     bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps)

    #     conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))
    #     conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b

    #     return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b)

    def test_process(self, FPDT):
        net = FPDT.detector_net.descriptor_net
        net.eval()
        net.to('cpu')

        # block1 = ['conv1x1s1_di_1', 'convdw3x3s2_1', 'conv1x1s1_dd_1']
        # block2 = ['conv1x1s1_di_2', 'convdw3x3s2_2', 'conv1x1s1_dd_2']
        # block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3']
        # block4 = ['conv1x1s1_di_4', 'convdw3x3s1_4', 'conv1x1s1_dd_4', 'conv1x1_di_shortcut_4']
        # block5 = ['conv1x1s1_di_5', 'convdw3x3s1_5', 'conv1x1s1_dd_5']
        # # block5 = ['conv1x1s1_di_5', 'convdw3x3s1_5', 'conv1x1s1_dd_5', 'conv1x1_di_shortcut_5']
        # block6 = ['conv1x1s1_di_6', 'convdw3x3s1_6', 'conv1x1s1_dd_6']

        block1 = ['conv1x1s1_di_1', 'convdw3x3s2_1', 'conv1x1s1_dd_1']
        block2 = ['conv1x1s1_di_2', 'convdw3x3s2_2', 'conv1x1s1_dd_2']
        block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3']
        block4 = ['conv1x1s1_di_4', 'convdw3x3s1_4', 'conv1x1s1_dd_4', 'conv1x1_di_shortcut_4']
        block5 = ['conv1x1s1_di_5', 'convdw3x3s1_5', 'conv1x1s1_dd_5', 'conv1x1_di_shortcut_5']
        block6 = ['conv1x1s1_di_6', 'convdw3x3s1_6', 'conv1x1s1_dd_6', 'conv1x1_di_shortcut_6']
        block7 = ['conv1x1s1_di_7', 'convdw3x3s1_7', 'conv1x1s1_dd_7']

        postconv1x1 = ['conv1x1_post']

        block = [block1, block2, block3, block4, block5, block6, block7, postconv1x1]

        name = []
        for i in block:
            name.extend(i)
        def list_layers(layer):
            layers = []
            if isinstance(layer, EBlock) or isinstance(layer, enn.SequentialModule) or isinstance(layer, ESeModule):
                for i in layer.children():
                    layers.extend(list_layers(i))
            elif isinstance(layer, enn.R2Conv) or isinstance(layer, enn.InnerBatchNorm) or isinstance(layer, enn.R2ConvTransposed):
                layers.append(layer.export().eval())
            elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ConvTranspose2d):
                layers.append(layer)
            return layers
        
        def get_parameters_layer(net):
            layers = []
            queue = deque()
            for i in net.children():
                queue.append(i)
            while len(queue):
                root = queue.popleft()
                layers.extend(list_layers(root))
            return layers

        # print(net)
        layers = get_parameters_layer(net)
        params_num = 0
        count = 0
        res = '#if 1\n'
        for i in range(len(layers)):
            conv = None
            if i + 1 < len(layers) and isinstance(layers[i], nn.Conv2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                conv = nn.utils.fusion.fuse_conv_bn_eval(layers[i], layers[i + 1])
                # conv = layers[i]
                # conv.bias = torch.nn.Parameter(torch.zeros(conv.weight.shape[0]))
            elif i + 1 < len(layers) and isinstance(layers[i], nn.ConvTranspose2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                # print(layers[i].weight.size())
                # print(layers[i].weight.transpose(0,1)[0,2,:,:])
                fused_deconv = copy.deepcopy(layers[i])
                fused_deconv.weight = torch.nn.Parameter(torch.transpose(layers[i].weight, 0, 1))
                # print(fused_deconv.weight.size())
                # print(fused_deconv.weight[0,2,:,:])
                # exit()
                conv = nn.utils.fusion.fuse_conv_bn_eval(fused_deconv, layers[i + 1])
            elif isinstance(layers[i], nn.Conv2d):
                conv = layers[i]
            if conv is not None:
                convw, convb = conv.weight.detach().numpy(
                ).flatten(), conv.bias.detach().numpy().flatten()
                params_num += convw.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_weight[{0}] = '.format(
                        convw.flatten().shape[0]) + '{ \n'
                if convw.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convw.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convw, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                params_num += convb.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_bias[{0}] = '.format(
                        convb.flatten().shape[0]) + '{ \n'
                if convb.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                elif convb.shape[0] % 4 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 4), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convb, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'
                count += 1
        print(count)
        res += '#endif\n'
        res += '//desc_parameters: {:d}\n'.format(params_num)
        with open(self.output_dir + '1128n_ecnn_dense_dc_c4_desc_ext_1000_netp_nc_nrt.h', 'w') as f:
            f.write(res)
        Path(r'param.txt').unlink()


class Get_Descriptor_Code(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']

    def test_process(self, FPDT):
        net = FPDT.detector_net.descriptor_net
        net.eval()
        net.to('cpu')

        # 参数
        to_short = False
        input_h, input_w = 16, 16
        input_tensor = torch.zeros(1, 1, input_h, input_w)   
        save_path = self.output_dir + ('short/' if to_short else 'float/')
        os.makedirs(save_path, exist_ok=True)
        framework = 'rect_patch'
        get_main(net, input_tensor, to_short, save_path, framework)


class Get_Keypoint_Parameter_Short(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.memory_optimize = True # True False         # 最后一层省峰值空间
        self.memory_optimize_head_suffix = '_transpose' if self.memory_optimize else ''
        self.dim            = 32 # 32 16 64
        self.out_dim        = 3
        self.model_prefix   = '0121m_217600_angle'


    def test_process(self, FPDT):
        net = FPDT.detector_net
        try:
            net.__delattr__('descriptor_net')
        except:
            print("net descriptor_net attribute is not exists!")
        try:
            net.__delattr__('descriptor_assist_net')
        except:
            print("net descriptor_assist_net attribute is not exists!")
        try:
            net.__delattr__('descriptor_tea_net')
        except:
            print("net descriptor_tea_net attribute is not exists!")
        
        net.eval()
        net.to('cpu')

        net_short = ALNet_Angle_Short() # ALNet_Angle_Deep_Short() ALNet_Angle_Short() HardNet_fast_half_short() # HardNet_fast_twice_big_short() # HardNet_fast_twice_short() HardNet_fast_twice_half3_MO_short()
        net_short.eval()
        net_short.to('cpu')

        # ALNet_Angle
        preconv = ['conv3x3s1_1', 'conv3x3s1_2']
        block1 = ['conv1x1s1_di_1', 'convdw3x3s1_1', 'conv1x1s1_dd_1',
                'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        block2 = ['conv1x1s1_di_2', 'convdw3x3s1_2', 'conv1x1s1_dd_2',
                'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
                'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']
        fs_conv = ['conv1x1_mid_1', 'conv1x1_mid_2', 'conv1x1_mid_3', 'conv1x1_mid_4']
                
        postconv1x1 = ['conv1x1_post']
        block = [preconv, block1, block2, block3, fs_conv, postconv1x1]

        # # ALNet_Angle_Deep
        # preconv = ['conv3x3s1_1', 'conv3x3s1_2']
        # block1 = ['conv1x1s1_di_1', 'convdw3x3s1_1', 'conv1x1s1_dd_1',
        #         'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        # block2 = ['conv1x1s1_di_2', 'convdw3x3s1_2', 'conv1x1s1_dd_2',
        #         'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        # block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
        #         'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']
        # fs_conv = ['conv1x1_mid_1', 'conv1x1_mid_2', 'conv1x1_mid_3', 'conv1x1_mid_4']
        
        # postconv1x1 = ['conv1x1_post']
        # headconv1x1 = ['conv1x1_head']
        # block = [preconv, block1, block2, block3, fs_conv, postconv1x1, headconv1x1]

        name = []
        prefix = "para_point_"
        for i in block:
            name.extend(i)
        for i in range(len(name)):
            name[i] = prefix + name[i]

        def list_layers(layer):
            layers = []
            if isinstance(layer, Block) or isinstance(layer, ConvBlock_New) or isinstance(layer, Block_noshortcut) or isinstance(layer, MobileOne) or isinstance(layer, MobileOne_new) or isinstance(layer, nn.Sequential) or isinstance(layer, SeModule):
                for i in layer.children():
                    layers.extend(list_layers(i))
            # elif isinstance(layer, MobileOne):
            #     for i in layer.blocks:
            #         layers.extend(list_layers(i))
            elif isinstance(layer, MobileOneBlock) or isinstance(layer, MobileOneBlock_new):
                # 多分支参数融合
                layer.reparameterize()
                for i in layer.children():
                    layers.extend(list_layers(i))
                # layers.append(layer)
            elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ConvTranspose2d):
                layers.append(layer)
            return layers
        
        def get_parameters_layer(net):
            layers = []
            queue = deque()
            for i in net.children():
                queue.append(i)
            while len(queue):
                root = queue.popleft()
                layers.extend(list_layers(root))
            return layers

        # print(net)
        # exit()
        layers = get_parameters_layer(net)
        print(layers)
        # exit()
        params_num = 0
        params_short_num = 0
        count = 0
        conw_short_list = []
        conb_short_list = []
        maxceo = []
        res = '#if 1\n'
        for i in range(len(layers)):
            conv = None
            if i + 1 < len(layers) and isinstance(layers[i], nn.Conv2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                conv = nn.utils.fusion.fuse_conv_bn_eval(layers[i], layers[i + 1])
            elif i + 1 < len(layers) and isinstance(layers[i], nn.ConvTranspose2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                # print(layers[i].weight.size())
                # print(layers[i].weight.transpose(0,1)[0,2,:,:])
                fused_deconv = copy.deepcopy(layers[i])
                fused_deconv.weight = torch.nn.Parameter(torch.transpose(layers[i].weight, 0, 1))
                # print(fused_deconv.weight.size())
                # print(fused_deconv.weight[0,2,:,:])
                # exit()
                conv = nn.utils.fusion.fuse_conv_bn_eval(fused_deconv, layers[i + 1])
            elif isinstance(layers[i], nn.Conv2d):
                conv = layers[i]
            if conv is not None:

                # 最后一层
                if self.memory_optimize and conv.weight.shape[0] == self.out_dim and conv.weight.shape[1] == self.dim:
                    branch_num = 4
                    assert conv.weight.shape[1] % branch_num == 0
                    convw, convb = conv.weight.view(conv.weight.shape[0], branch_num, -1).transpose(0, 1).detach().numpy(
                        ).flatten(), conv.bias.detach().numpy().flatten()
                else:
                    convw, convb = conv.weight.detach().numpy(
                        ).flatten(), conv.bias.detach().numpy().flatten()

                # maxincw = int(32760 / np.maximum(np.max(convw),-np.min(convw)) - 10)  # weight 绝对值过大时会出现下溢
                maxincw = int(32760 / np.maximum(np.max(convw),-np.min(convw)))
                convw = (convw*maxincw).astype(np.int)

                maxceo.append(maxincw)
                #量化模型准备
                scale = (1.0 / maxincw)
                conw_short_list.append((conv.weight.detach()*maxincw).long() *scale)
                conb_short_list.append(conv.bias.detach())

                params_short_num += convw.flatten().shape[0]
                print(name[count])
                res += 'static short ' + \
                    name[count] + \
                    '_weight[{0}] = '.format(
                        convw.flatten().shape[0]) + '{ \n'
                if convw.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convw.reshape(-1, 8), fmt='%d',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convw, fmt='%d',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                params_num += convb.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_bias[{0}] = '.format(
                        convb.flatten().shape[0]) + '{ \n'
                if convb.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 8), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                elif convb.shape[0] % 4 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 4), fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convb, fmt='%1.10f',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                count += 1

        #保存scale
        params_num += len(maxceo)
        res += 'static int ' + prefix + \
                'weight_scale[{0}] = '.format(
                    len(maxceo)) + '{ \n'
        maxceo_np = np.array(maxceo)
        if maxceo_np.shape[0] % 8 == 0:
            np.savetxt('param.txt', maxceo_np.reshape(-1, 8), fmt='%d',
                        delimiter=', ', newline=',\n')
        else:
            np.savetxt('param.txt', maxceo_np, fmt='%d',
                        delimiter=', ', newline=',\n')
        with open('param.txt', 'r') as f:
            res += f.read()
        res += '};\n\n'

        print(count)
        memory_total = (params_short_num*2+params_num*4)/1024
        res += '#endif\n'
        res += '//' + self.model_prefix + '_point_parameters: float[{:d}] short[{:d}]\n'.format(params_num, params_short_num)
        res += '//' + self.model_prefix + '_point_memory: {:.4f} K\n'.format(memory_total)
        with open(self.output_dir.replace('Pnt', 'Head') + self.model_prefix + '_point_result' + self.memory_optimize_head_suffix + '.h', 'w') as f:
            f.write(res)
        Path(r'param.txt').unlink()

        quant_pth_save_pth = self.output_dir + self.model_prefix + '_point_short.pth.tar'
        net_short.model_save_quant_param(conw_short_list, conb_short_list, quant_pth_save_pth)


class Get_Keypoint_Parameter_Short_7F(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.memory_optimize = True # True False         # 最后一层省峰值空间
        self.memory_optimize_head_suffix = '_transpose' if self.memory_optimize else ''
        self.dim            = 32 # 32 16 64
        self.out_dim        = 3
        self.model_prefix   = '0303m_124800_angle'


    def test_process(self, FPDT):
        net = FPDT.detector_net
        try:
            net.__delattr__('descriptor_net')
        except:
            print("net descriptor_net attribute is not exists!")
        try:
            net.__delattr__('descriptor_assist_net')
        except:
            print("net descriptor_assist_net attribute is not exists!")
        try:
            net.__delattr__('descriptor_tea_net')
        except:
            print("net descriptor_tea_net attribute is not exists!")
        
        net.eval()
        net.to('cpu')

        net_short = ALNet_Angle_Short() # ALNet_Angle_Deep_Short() ALNet_Angle_Short() HardNet_fast_half_short() # HardNet_fast_twice_big_short() # HardNet_fast_twice_short() HardNet_fast_twice_half3_MO_short()
        net_short.eval()
        net_short.to('cpu')

        # ALNet_Angle
        preconv = ['conv3x3s1_1', 'conv3x3s1_2']
        block1 = ['conv1x1s1_di_1', 'convdw3x3s1_1', 'conv1x1s1_dd_1',
                'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        block2 = ['conv1x1s1_di_2', 'convdw3x3s1_2', 'conv1x1s1_dd_2',
                'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
                'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']
        fs_conv = ['conv1x1_mid_1', 'conv1x1_mid_2', 'conv1x1_mid_3', 'conv1x1_mid_4']
                
        postconv1x1 = ['conv1x1_post']
        block = [preconv, block1, block2, block3, fs_conv, postconv1x1]

        # # ALNet_Angle_Deep
        # preconv = ['conv3x3s1_1', 'conv3x3s1_2']
        # block1 = ['conv1x1s1_di_1', 'convdw3x3s1_1', 'conv1x1s1_dd_1',
        #         'conv1x1s1_dd_se_1', 'conv1x1s1_di_se_1']
        # block2 = ['conv1x1s1_di_2', 'convdw3x3s1_2', 'conv1x1s1_dd_2',
        #         'conv1x1s1_dd_se_2', 'conv1x1s1_di_se_2']
        # block3 = ['conv1x1s1_di_3', 'convdw3x3s1_3', 'conv1x1s1_dd_3',
        #         'conv1x1s1_dd_se_3', 'conv1x1s1_di_se_3']
        # fs_conv = ['conv1x1_mid_1', 'conv1x1_mid_2', 'conv1x1_mid_3', 'conv1x1_mid_4']
        
        # postconv1x1 = ['conv1x1_post']
        # headconv1x1 = ['conv1x1_head']
        # block = [preconv, block1, block2, block3, fs_conv, postconv1x1, headconv1x1]

        name = []
        prefix = "para_point_"
        for i in block:
            name.extend(i)
        for i in range(len(name)):
            name[i] = prefix + name[i]

        def list_layers(layer):
            layers = []
            if isinstance(layer, Block) or isinstance(layer, ConvBlock_New) or isinstance(layer, Block_noshortcut) or isinstance(layer, MobileOne) or isinstance(layer, MobileOne_new) or isinstance(layer, nn.Sequential) or isinstance(layer, SeModule):
                for i in layer.children():
                    layers.extend(list_layers(i))
            # elif isinstance(layer, MobileOne):
            #     for i in layer.blocks:
            #         layers.extend(list_layers(i))
            elif isinstance(layer, MobileOneBlock) or isinstance(layer, MobileOneBlock_new):
                # 多分支参数融合
                layer.reparameterize()
                for i in layer.children():
                    layers.extend(list_layers(i))
                # layers.append(layer)
            elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.ConvTranspose2d):
                layers.append(layer)
            return layers
        
        def get_parameters_layer(net):
            layers = []
            queue = deque()
            for i in net.children():
                queue.append(i)
            while len(queue):
                root = queue.popleft()
                layers.extend(list_layers(root))
            return layers

        # print(net)
        # exit()
        layers = get_parameters_layer(net)
        print(layers)
        # exit()
        params_num = 0
        params_short_num = 0
        count = 0
        conw_short_list = []
        conb_short_list = []
        maxceo = []
        res = '#if 1\n'
        for i in range(len(layers)):
            conv = None
            if i + 1 < len(layers) and isinstance(layers[i], nn.Conv2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                conv = nn.utils.fusion.fuse_conv_bn_eval(layers[i], layers[i + 1])
            elif i + 1 < len(layers) and isinstance(layers[i], nn.ConvTranspose2d) and isinstance(layers[i + 1], nn.BatchNorm2d):
                # print(layers[i].weight.size())
                # print(layers[i].weight.transpose(0,1)[0,2,:,:])
                fused_deconv = copy.deepcopy(layers[i])
                fused_deconv.weight = torch.nn.Parameter(torch.transpose(layers[i].weight, 0, 1))
                # print(fused_deconv.weight.size())
                # print(fused_deconv.weight[0,2,:,:])
                # exit()
                conv = nn.utils.fusion.fuse_conv_bn_eval(fused_deconv, layers[i + 1])
            elif isinstance(layers[i], nn.Conv2d):
                conv = layers[i]
            if conv is not None:

                # 最后一层
                if self.memory_optimize and conv.weight.shape[0] == self.out_dim and conv.weight.shape[1] == self.dim:
                    branch_num = 4
                    assert conv.weight.shape[1] % branch_num == 0
                    convw, convb = conv.weight.view(conv.weight.shape[0], branch_num, -1).transpose(0, 1).detach().numpy(
                        ).flatten(), conv.bias.detach().numpy().flatten()
                else:
                    convw, convb = conv.weight.detach().numpy(
                        ).flatten(), conv.bias.detach().numpy().flatten()

                # maxincw = int(32760 / np.maximum(np.max(convw),-np.min(convw)) - 10)  # weight 绝对值过大时会出现下溢
                maxincw = int(32760 / np.maximum(np.max(convw),-np.min(convw)))
                convw = (convw*maxincw).astype(int)

                maxceo.append(maxincw)
                #量化模型准备
                scale = (1.0 / maxincw)
                conw_short_list.append((conv.weight.detach()*maxincw).long() *scale)
                conb_short_list.append(conv.bias.detach())

                params_short_num += convw.flatten().shape[0]
                print(name[count])
                res += 'static short ' + \
                    name[count] + \
                    '_weight[{0}] = '.format(
                        convw.flatten().shape[0]) + '{ \n'
                if convw.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convw.reshape(-1, 8), fmt='%d',
                            delimiter=', ', newline=',\n')
                else:
                    np.savetxt('param.txt', convw, fmt='%d',
                            delimiter=', ', newline=',\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                params_num += convb.flatten().shape[0]
                res += 'static float ' + \
                    name[count] + \
                    '_bias[{0}] = '.format(
                        convb.flatten().shape[0]) + '{ \n'
                if convb.shape[0] % 8 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 8), fmt='%1.6f',
                            delimiter='f, ', newline='f,\n')
                elif convb.shape[0] % 4 == 0:
                    np.savetxt('param.txt', convb.reshape(-1, 4), fmt='%1.6f',
                            delimiter='f, ', newline='f,\n')
                else:
                    np.savetxt('param.txt', convb, fmt='%1.6f',
                            delimiter='f, ', newline='f,\n')
                with open('param.txt', 'r') as f:
                    res += f.read()
                res += '};\n\n'

                count += 1

        #保存scale
        params_num += len(maxceo)
        res += 'static int ' + prefix + \
                'weight_scale[{0}] = '.format(
                    len(maxceo)) + '{ \n'
        maxceo_np = np.array(maxceo)
        if maxceo_np.shape[0] % 8 == 0:
            np.savetxt('param.txt', maxceo_np.reshape(-1, 8), fmt='%d',
                        delimiter=', ', newline=',\n')
        else:
            np.savetxt('param.txt', maxceo_np, fmt='%d',
                        delimiter=', ', newline=',\n')
        with open('param.txt', 'r') as f:
            res += f.read()
        res += '};\n\n'

        print(count)
        memory_total = (params_short_num*2+params_num*4)/1024
        res += '#endif\n'
        res += '//' + self.model_prefix + '_point_parameters: float[{:d}] short[{:d}]\n'.format(params_num, params_short_num)
        res += '//' + self.model_prefix + '_point_memory: {:.4f} K\n'.format(memory_total)
        with open(self.output_dir.replace('Pnt', 'Head') + self.model_prefix + '_point_result' + self.memory_optimize_head_suffix + '.h', 'w') as f:
            f.write(res)
        Path(r'param.txt').unlink()

        quant_pth_save_pth = self.output_dir + self.model_prefix + '_point_short.pth.tar'
        net_short.model_save_quant_param(conw_short_list, conb_short_list, quant_pth_save_pth)


class Get_Pnt_Code(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']

    def test_process(self, FPDT):
        net = FPDT.detector_net
        net.__delattr__('descriptor_net')
        net.__delattr__('descriptor_assist_net')
        net.__delattr__('descriptor_tea_net')
        net.eval()
        net.to('cpu')

        # 参数
        to_short = False
        input_h, input_w = 128, 40
        input_tensor = torch.zeros(1, 1, input_h, input_w)   
        save_path = self.output_dir + ('short/' if to_short else 'float/')
        os.makedirs(save_path, exist_ok=True)
        framework = 'alike_pnt'
        get_main(net, input_tensor, to_short, save_path, framework)



class Get_Pts_Descriptor_From_Head(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.classify_ori_enhance_flag = False

        self.copy_enhance_flag = True
        self.tool_path      = '/hdd/file-input/linwc/Descriptor/Test_Tool/v_1_0/enhance/6193-DK7-140-8-rotate_test/images/' # '/hdd/file-input/linwc/Descriptor/Test_Tool/v_1_0/enhance/db_811_p10s200/images/'

        self.pts_desc_flag  = False
        self.pts_desc_head  = img_path + '91.desc.h' # img_path + 'desc.add45.h' # img_path + 'des.h'
        self.pts_desc_outdir = img_path + 'pnt_desc_data/'

        self.trans_flag     = False
        self.trans_head     = img_path + 'log.trans.0811.h'
        self.succ_trans_outpath = img_path + 'SIFT_Trans/SIFT_transSucc.csv'  

    def HexStr2int32(self, hexstr):
        # print(hexstr, len(hexstr))
        assert len(hexstr) == 8
        base = 16
        res = 0
        count = 0
        greater10 = {'A': 10, 'B': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15}
        for c in hexstr:
            assert (c >= '0' and c <='9') or (c >= 'A' and c <= 'F')
            if c >= '0' and c <='9':
                midv = int(c)
            else:
                midv = greater10[c]
            if count == 0:
                res = midv
            else:
                res = res * base + midv                
            count += 1
        return res

    def test_process(self, FPDT):
        # 分离原图和增强图
        if self.classify_ori_enhance_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', '6091_DK7-140_Mul_8_rot_p6_a')
            for n1, _, n3 in os.walk(str(imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace('6091_DK7-140_Mul_8_rot_p6_a', 'img_ori_data'), exist_ok=True)
                    os.makedirs(n1.replace('6091_DK7-140_Mul_8_rot_p6_a', 'img_enhance_data'), exist_ok=True)
                    for name in n3:
                        if '_' not in name:
                            shutil.move(n1 + '/' + name, n1.replace('6091_DK7-140_Mul_8_rot_p6_a', 'img_ori_data') + '/')  
                        elif 'mskEnhance' in name:
                            shutil.copyfile(n1 + '/' + name, n1.replace('6091_DK7-140_Mul_8_rot_p6_a', 'img_enhance_data') + '/' + name.replace('mskEnhance', 'enhance')) 
                            # shutil.move(n1 + '/' + name, n1.replace('6091_DK7-140_Mul_8_rot_p6_a', 'img_enhance_data') + '/') 
                        else:
                            continue
            pass

        # 拷贝enhance到工具文件下
        if self.copy_enhance_flag:
            for n1, _, n3 in os.walk(str(self.pts_desc_outdir.replace('pnt_desc_data', 'img_extend_data'))):
                if 'L' in n1 or 'R' in n1:
                    for name in n3:
                        if 'extend' in name:
                            source_path = n1 + '/' + name
                            shutil.copyfile(n1 + '/' + name, self.tool_path + '_'.join(source_path.split('/')[-3:]).replace('_extend', '')) 
            pass

        # 解析点和描述子
        if self.pts_desc_flag:
            with open(self.pts_desc_head, mode='r') as pts_desc_f:
                img_name_already = ['0000_L1_0000']
                message_all = None
                message = None
                flag = False
                for line in pts_desc_f:
                    if 'PRINT_KEYPOINT,X,Y,ORI' in line:
                        line.replace('\n', '')
                        pt = torch.tensor([int(p) for p in line.split(',')[4:6]]).to(self.device)
                        desc0_ori = torch.tensor([int(d) for d in line.split(',')[11:139]]).to(self.device)
                        desc0_quant_list = [self.HexStr2int32(hexstr) for hexstr in line.split(',')[7:11]]
                        orient = int(line.split(',')[6])
                        flag = True
                        continue
                    
                    if flag:
                        desc45_ori = torch.tensor([int(d) for d in line.split(',')[4:132]]).to(self.device)
                        desc45_quant_list = [self.HexStr2int32(hexstr) for hexstr in line.split(',')[0:4]]
                        desc0_quant_list.extend(desc45_quant_list)
                        desc0_quant_list.append(orient)
                        desc_quant = torch.tensor(desc0_quant_list).to(self.device)
                        message = torch.cat((pt, desc0_ori, desc45_ori, desc_quant), dim=-1).unsqueeze(0)
                        if message_all is None:
                            message_all = copy.deepcopy(message)
                        else:
                            message_all = torch.cat((message_all, message), dim=0)
                        flag = False


                    # 图像名
                    # if 'E:'in line and 'PRINT_KEYPOINT' in line:
                    if ('E:' in line and '=' not in line) or ('image =E:' in line):
                        img_name = '_'.join(line.split('/')[-3:]).replace('.bmp', '')[:-1]   # 0000_L1_0000
                        # print(img_name, img_name in img_name_already)
                        if not img_name in img_name_already:
                            desc_save_path = self.pts_desc_outdir + img_name_already[-1] + '.csv'
                            df = pd.DataFrame(message_all.detach().cpu().numpy())
                            df.to_csv(desc_save_path, header=None)
                            message_all = None
                            img_name_already.append(img_name)

                
                # 最后一张图
                desc_save_path = self.pts_desc_outdir + img_name_already[-1] + '.csv'
                df = pd.DataFrame(message_all.detach().cpu().numpy())
                df.to_csv(desc_save_path, header=None)        
                pts_desc_f.close()

        # 解析succ对的trans
        if self.trans_flag:
            trans_message_all = []
            with open(self.trans_head, mode='r') as trans_f:
                for line in trans_f:
                    if 'Trans:' in line:
                        trans_message = [int(t) for t in line.split(':')[1].split(',')[:-1]]
                        verify_path = line.split(':')[-1].split(',')[0]
                        enrollverify = 'ENROLL: ' + str(int(line.split('.bmp')[0][-4:])) + ' verify: ' + str(int(line.split('.bmp')[1][-4:]))
                        trans_message.extend([0, 0, 0, enrollverify, verify_path, 0, 0])
                        trans_message_all.append(trans_message) 
   
                df_trans = pd.DataFrame(np.array(trans_message_all))
                df_trans.to_csv(self.succ_trans_outpath, header=None)        
                trans_f.close()
                
        pass   


class Generate_Enhance_Img(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.fusion_ori_ratio   = 0.35
        gallary = [
            # '6191_DK4_halfpress_p10s150',
            # '6191_DK4_powder_p10s150',
            # '6191_DK4_rot90_p10s150',
            # '6191_DK4_SH_p10s150',
            # '6191_DK4_SH_rot_p9s150',
            # '6191_DK4_wet_p10s150',
            # '6191_DK7-140-normal-X888-SH',
            # '6191_DK7_halfpress_p10s150',
            # '6191_DK7_p10s200',
            # '6191_DK7_powder_p9s150',
            # '6191_DK7_SH_rot_p9s150',
            # '6191_DK7_wet_p10s150'
            # '6191_DK4_SH_SZ_p20s150',
            # '6091_DK7-140_Mul_8_rot_p6'    
            # '6193_DK7_merge_test1'
            # '6193_DK7_merge_test2'  
            '6193_DK7_rotate_8mul_merge'
        ]
        self.desired_libray = [img_path + g + '/' for g in gallary] 
        self.enhance        = False
        self.expand         = True


    def test_process(self, FPDT):
        # 分离原图和增强图
        if self.enhance:
            for imgall_dir in self.desired_libray:
                for n1, _, n3 in os.walk(str(imgall_dir)):
                    # 混手指
                    # if n1.count('X') > 1:
                    #     continue
                    if 'L' in n1 or 'R' in n1:
                        ori_path = n1.replace('liugq', 'linwc/Descriptor/data').replace('DB_X888rest', 'DB_X888rest/img_ori_data')
                        os.makedirs(ori_path, exist_ok=True)
                        if n1.count('X') > 1 and n1.count('X888') != 2:
                            for name in n3:
                                if '_' not in name:
                                    shutil.copyfile(n1 + '/' + name, ori_path + '/' + name)    
                        else:
                            enhance_path = n1.replace('liugq', 'linwc/Descriptor/data').replace('DB_X888rest', 'DB_X888rest/img_enhance_data')
                            mask_path  = n1.replace('liugq', 'linwc/Descriptor/data').replace('DB_X888rest', 'DB_X888rest/img_mask_data')
                            os.makedirs(enhance_path, exist_ok=True)
                            os.makedirs(mask_path, exist_ok=True)
                            # print(n3)
                            for name in n3:                            
                                if '_' not in name:
                                    shutil.copyfile(n1 + '/' + name, ori_path + '/' + name) 
                                    # continue 
                                elif 'expend1' in name:
                                    # prep1raw_name = name.replace('expend1', 'prep1raw')
                                    # if os.path.exists(prep1raw_name):
                                    #     continue
                                    # img_expand = load_as_float(n1 + '/' + name)
                                    # img_prep = load_as_float(n1 + '/' + prep1raw_name)
                                    # img_fusion = img_expand * self.fusion_ori_ratio + img_prep * (1 - self.fusion_ori_ratio)
                                    # cv2.imwrite(enhance_path + '/' + name.replace('expend1', 'enhance'), 255*img_fusion)
                                    continue
                                elif 'msk1' in name:
                                    # shutil.copyfile(n1 + '/' + name, mask_path + '/' + name.replace('msk1', 'mask'))
                                    continue
                                else:  
                                    continue

        # 分离原图和扩边图
        if self.expand:
            for imgall_dir in self.desired_libray:
                for n1, _, n3 in os.walk(str(imgall_dir)):
                    # 混手指
                    # if n1.count('X') > 1:
                    #     continue
                    if 'L' in n1 or 'R' in n1:
                        if n1.count('X') > 1 and n1.count('X888') != 2:
                            continue     
                        else:
                            expand_path = n1.replace('tmp_new', 'img_extend_data_7204')
                            os.makedirs(expand_path, exist_ok=True)
                            # print(n3)
                            for name in n3:                            
                                if '_desc' in name:
                                    shutil.copyfile(n1 + '/' + name, expand_path + '/' + name.replace('desc', 'extend'))    


class Head2Log(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.trans_pts_flag = False
        self.trans_trans_flag = True 
        self.temp_num       = 20   
        gallary = [
            # '6191_DK4_halfpress_p10s150',
            # '6191_DK4_powder_p10s150',
            # '6191_DK4_rot90_p10s150',
            # '6191_DK4_SH_p10s150',
            # '6191_DK4_SH_rot_p9s150',
            # '6191_DK4_wet_p10s150',
            # '6191_DK7-140-normal-X888-SH',
            # '6191_DK7_halfpress_p10s150',
            # '6191_DK7_p10s200',
            # '6191_DK7_powder_p9s150',
            # '6191_DK7_SH_rot_p9s150',
            # '6191_DK7_wet_p10s150',
            # '6191_DK4_SH_SZ_p20s150'  
            # '6191_DK7-140_Mul_8_rot_p6',
            # '6191_DK4_cd_p8s120',
            # '6191test_20p1_X8',
            # '6191test_19p2_X8',
            # '6191test_19p1_X8',
            # '6191_DK7_CD_p11s200',
            # '6193_DK7_merge_test1',
            # '6193_DK7_merge_test2'
            '6193_DK7_XA_rot_test'

        ]
        self.desired_libray = [img_path + g + '/' for g in gallary] 

    def get_pts_str(self, ori_str):
        name = ['x:', ',y:', ',ori:']
        out_str = ''
        for c, n in zip(ori_str.split(','), name):
            out_str += n + c
        return out_str

    def get_trans_str(self, ori_message):
        name = ['ENROLL: ', 'verify: ', 'path: ', 'Trans:', 'score=', 'up=']
        out_str = ''
        for n in name:
            
            if n in ['ENROLL: ', 'verify: ']:
                out_str += n + str(ori_message[n]) + ', '
            else:
                out_str += n + ori_message[n] + ','
        return out_str[:-1]


    def test_process(self, FPDT):
        for imgall_dir in self.desired_libray:

            if self.trans_pts_flag:
                # 转换点
                pts_head = imgall_dir + 'kpts.h'
                out_pts_txt = imgall_dir + 'point.txt'
                pts_flag = False
                pts_count = 0
                with open(pts_head, mode='r') as pts_f:
                    with open(out_pts_txt, mode="w", encoding='utf-8') as out_pts_f:
                        for line in pts_f:
                            # qint
                            if 'F:' in line and '>' not in line:
                                if line[0] == 'F':
                            # # lwc
                            # if 'E:' in line and '>' not in line:
                            #     if line[0] == 'E':
                                    line_new = line
                                elif 'image =' in line:
                                    line_new = line.split('=')[-1]
                                else:
                                    continue
                                img_path = line_new.replace(line_new.split('/')[0], imgall_dir[:-1]).replace('.bmp', '_enhance.bmp').replace('\n', '')
                                out_pts_f.write(img_path + '\n')
                                out_pts_f.write('n=' + str(150) + '\n')

                                pts_count = 0
                                pts_flag = True


                            if pts_flag and 'PRINT_KEYPOINT,X,Y,ORI,' in line:
                                line.replace('\n', '')
                                pt_str = line.split('PRINT_KEYPOINT,X,Y,ORI,')[-1]
                                out_pts_f.write('id:' + str(pts_count) + '\n')

                                pts_count += 1
                                out_pts_f.write(self.get_pts_str(pt_str) + '\n')
                        out_pts_f.write('/\n')
                        out_pts_f.close()
                    pts_f.close()

            if self.trans_trans_flag:
                # 转换trans
                trans_head = imgall_dir + 'trans.h'
                out_trans_txt = imgall_dir + 'trans.txt'
            
                with open(trans_head, mode='r') as trans_f:
                    with open(out_trans_txt, mode="w", encoding='utf-8') as out_trans_f:
                        for line in trans_f:
                            # if 'Trans:' in line and line[0] == 'T':
                            #     line.replace('\n', '')
                            #     # print(line)
                            #     ori_verify_path = line.split('verify: ')[1].split(',')[0]
                            #     message ={
                            #         'Trans:': line.split('Trans:')[-1].split('ENROLL:')[0][:-1],
                            #         'ENROLL: ': int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', '')),
                            #         'verify: ': int(ori_verify_path.split('/')[-1].replace('.bmp', '')),
                            #         'path: ': ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1]).replace('.bmp', '_enhance.bmp').replace('\n', ''),
                            #         'score=': '0',
                            #         'up=': '1',
                            #     }

                            #     # 筛选trans
                            #     if message['ENROLL: '] >= self.temp_num or message['verify: '] < self.temp_num:
                            #     # if message['ENROLL: '] >= self.temp_num:
                            #         continue
                            #     out_trans_f.write(self.get_trans_str(message) + '\n')        
                            
                            # new concept
                            if 'Trans:' in line and line[0] == 'T':
                                line = line.replace('\n', '')
                                # print(line)
                                message ={
                                    'Trans:': line.split('Trans:')[-1],
                                    # 'ENROLL: ': int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', '')),
                                    # 'verify: ': int(ori_verify_path.split('/')[-1].replace('.bmp', '')),
                                    # 'path: ': ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1]).replace('.bmp', '_enhance.bmp').replace('\n', ''),
                                }

                            if 'ENROLL: ' in line and line[0] == ',':
                                line = line.replace('\n', '')
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]
                                message['ENROLL: '] = int(line.split(',ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                                message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                                message['path: '] = ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['score='] = '0'
                                message['up='] = '1'
                                # 筛选trans
                                if message['ENROLL: '] >= self.temp_num or message['verify: '] < self.temp_num:
                                # if message['ENROLL: '] >= self.temp_num:
                                    continue
                                out_trans_f.write(self.get_trans_str(message) + '\n')            
                        out_trans_f.close()
                    trans_f.close()

                                              

        pass      


class Head2Log93(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.trans_pts_flag = False
        self.trans_trans_flag = False 
        self.trans_trans_program_flag = False
        self.trans_trans_compare_flag = True
        self.trans_trans_info0_flag = False
        self.trans_trans_info1_flag = False
        self.trans_trans_info012_flag = False
        self.pick_info_common_trans_flag = False
        self.temp_num       = 20 # 20
        self.true_finger_num = 50 # 150   
        self.p_thr          = 200 # 30 200 16 18
        self.FAtrans_trans_flag = False
        self.FRtrans_trans_flag = False
        self.FRtrans_trans_net_flag = False
        self.FAtrans_trans_net_flag = False
        self.NetName        = 'Net61000/'      # ''为sift 'Net13600/'
        self.cal_common_flag        = False # True
        self.plot_img       = False
        self.enroll_full_flag = False
        self.plot_ROC       = False

        self.mode           = 'FR'   # FA  # FR
        self.model          = 'SIFT'  # 'SIFT' # Net
        gallary = [
            # '6191_DK4_halfpress_p10s150',
            # '6191_DK4_powder_p10s150',
            # '6191_DK4_rot90_p10s150',
            # '6191_DK4_SH_p10s150',
            # '6191_DK4_SH_rot_p9s150',
            # '6191_DK4_wet_p10s150',
            # '6191_DK7-140-normal-X888-SH',
            # '6191_DK7_halfpress_p10s150',
            # '6191_DK7_p10s200',
            # '6191_DK7_powder_p9s150',
            # '6191_DK7_SH_rot_p9s150',
            # '6191_DK7_wet_p10s150',
            # '6191_DK4_SH_SZ_p20s150'  
            # '6191_DK7-140_Mul_8_rot_p6',
            # '6191_DK4_cd_p8s120',
            # '6191test_20p1_X8',
            # '6191test_19p2_X8',
            # '6191test_19p1_X8',
            # '6191_DK7_CD_p11s200',
            # '6193_DK7_merge_test1',
            # '6193_DK7_merge_test1_9800'
            # '6193_DK7_merge_test2',
            # '6193_DK7_wet_8mul_merge',
            # '6193_DK7_wet_8mul_merge_9800',
            # '6193_DK7_suppress',
            # '6193_DK7_suppress_9800',
            # '6193_DK7_normal_8mul_merge',
            # '6193_DK7_normal_8mul_merge_9800'
            # '6193_DK7_partialPress_8mul_merge',
            # '6193_DK7_partialPress_8mul_merge_9800'
            # '6193_DK7_powder_8mul',
            # '6193_DK7_powder_8mul_9800',
            # '6193_DK7_random_merge_total',
            # '6193_DK7_random_merge_total_9800',
            # '6193_DK7_rotate_8mul_merge'
            '6193_DK7_rotate_8mul_merge_9800',
            # '6193-DK7-140-8-rotate_test'
            # '6193_DK7_XA_rot_test'
            # '6193_DK7_XA_rot_test_9800'
            # '6193_DK7_partialpress_test'
            # '6193-DK7-140-8-powder_test'
            # '6193-DK7-140-8-suppress_test'
            # '6193-DK7-140-8-wet2_clear_test'
            # '6193-DK7-140-8-lhz_supress_test'
            # '6193-DK7-160-8-normal-nocontrol_test_9800'
            # '6193-DK7-160-8_test_9800'
            # '6193-DK4-130-purple-suppress-SNR28_9800'
            # '6193-DK4-110-normal-purple-SNR40_9800'
            # '6193-DK4-130-normal-purple-SNR28_9800'
            # '6193-DK4-110-tarnish-normal-SNR80_9800'
            # '6193-DK4-110-tarnish-normal-fix-SNR80_9800'
            # '6193-DK4-130-purple-tickle-fix_9800'
            # '6193-DK4-110-tarnish-normal-fix-standard-SNR80_9800'
            # '6193-DK4-110-tarnish-normal-fix-three-SNR80_9800'
            # '6193-DK4-110-tarnish-normal-fix-mix-SNR80_9800'
            # '6193-DK4-130-purple-wet_test_9800'
            # '6193-DK4-130-purple-rot_test_9800'
            # '6193-DK4-130-purple-powder_test_9800'
            # '6193-DK4-130-purple-partialPress_test_9800'
            # '6193-DK4-130-purple-charge_test_9800'
            # '6193-DK4-130-purple-normal_test_9800'
            # '6193-DK4-130-purer-supPress_merge_9800'
            # '6193-DK4-150-mul8-base_9800'
            # '6195-Dk7-110-normal-auto_FingerImage-1#_auto-ori'
            # '6195-Dk7-110-normal-auto_FingerImage-2#_auto-ori'
            # '6195-DK7-110-partialpress_FingerImage1#_auto-ori'
            # '6195-DK7-110-partialpress_FingerImage2#_auto-ori'
            # '6195-DK7-110-rotate-_FingerImage1#_auto-ori'
            # '6195-DK7-110-rotate-_FingerImage2#_auto-ori'
            # '6195-DK7-110-suppress_Ori_1#_auto'
            # '6195-DK7-110-suppress_Ori_2#_auto' 
        ]
        self.desired_libray = [img_path + g + '/' for g in gallary] 

    def get_pts_str(self, ori_str):
        name = ['x:', ',y:', ',ori:']
        out_str = ''
        for c, n in zip(ori_str.split(','), name):
            out_str += n + c
        return out_str

    def get_trans_str(self, ori_message):
        name = ['ENROLL: ', 'verify: ', 'path: ', 'Trans:', 'score=', 'up=']
        out_str = ''
        for n in name:
            
            if n in ['ENROLL: ', 'verify: ']:
                out_str += n + str(ori_message[n]) + ', '
            else:
                out_str += n + ori_message[n] + ','
        return out_str[:-1]

    def get_trans_str_more(self, ori_message):
        name = ['ENROLL: ', 'verify: ', 'path: ', 'Trans:', 'score=', 'up=', 'rotation=', 'shear=', 'scale=', 'translation=', 'Rarea=', 'Bsim=']
        out_str = ''
        for n in name:        
            if n in ['ENROLL: ', 'verify: ']:
                out_str += n + str(ori_message[n]) + ', '
            else:
                out_str += n + ori_message[n] + ','
        return out_str[:-1]

    def get_trans_str_more_enroll(self, ori_message, enroll_num):
        name = ['ENROLL: ', 'verify: ', 'path: ', 'Trans:', 'score=', 'up=', 'rotation=', 'shear=', 'scale=', 'translation=', 'Rarea=', 'Bsim=']
        out_str = ''
        for n in name:        
            if n in ['verify: ']:
                out_str += n + str(ori_message[n]) + ', '
            elif n == 'ENROLL: ':
                out_str += n + str(enroll_num) + ', '
            else:
                out_str += n + ori_message[n] + ','
        out_str = out_str.replace('img_extend_data/', '')
        return out_str[:-1]


    def isnumber(self, ori_str):
        try:
            int(ori_str)
            return True
        except:
            return False

    def inv_warp_image_batch_cv2(self, img, mat_homo_inv, device='cpu', mode='bilinear'):
        '''
        Inverse warp images in batch

        :param img:
            batch of images
            tensor [batch_size, 1, H, W]
        :param mat_homo_inv:
            batch of homography matrices
            tensor [batch_size, 3, 3]
        :param device:
            GPU device or CPU
        :return:
            batch of warped images
            tensor [batch_size, 1, H, W]
        '''
        # compute inverse warped points
        if len(img.shape) == 2:
            img = img.view(1, 1, img.shape[0], img.shape[1])
        if len(img.shape) == 3:
            img = img.view(1, img.shape[0], img.shape[1], img.shape[2])
        if len(mat_homo_inv.shape) == 2:
            mat_homo_inv = mat_homo_inv.view(1,3,3)

        _, _, H, W = img.shape

        warped_img = cv2.warpPerspective(img.squeeze().numpy(), mat_homo_inv.squeeze().numpy(), (W, H))
        warped_img = torch.tensor(warped_img, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(1)

        # warped_img = cv2.warpAffine(img.squeeze().numpy(), mat_homo_inv[0, :2, :].squeeze().numpy(), (W, H))
        # warped_img = torch.tensor(warped_img, dtype=torch.float32).unsqueeze(0).unsqueeze(1)
        return warped_img

    def inv_warp_image(self, img, mat_homo_inv, device='cpu', mode='bilinear'):
        '''
        Inverse warp images in batch

        :param img:
            batch of images
            tensor [H, W]
        :param mat_homo_inv:
            batch of homography matrices
            tensor [3, 3]
        :param device:
            GPU device or CPU
        :return:
            batch of warped images
            tensor [H, W]
        '''
        warped_img = self.inv_warp_image_batch_cv2(img, mat_homo_inv, device, mode)
        return warped_img.squeeze()

    def draw_keypoints_pair_train(self, input_img, input_pts, color=(0, 255, 0), radius=3, s=3, H=None):
        '''
        :param img:
            image:
            numpy [H, W]
        :param corners:
            Points
            numpy [N, 2]
        :param color:
        :param radius:
        :param s:
        :return:
            overlaying image
            numpy [H, W]
        '''
        anchor = int(input_img['img'].shape[1])
        img = np.hstack((input_img['img'], input_img['img_H'])) * 255
        img = np.repeat(cv2.resize(img, None, fx=s, fy=s)[..., np.newaxis], 3, -1)
        for c in np.stack(input_pts['pts']):
            if c.size == 1:
                break
            cv2.circle(img, tuple((s * c[:2]).astype(int)), radius, (0, 0, 0), thickness=-1)    # 图像坐标系，先列后行
        
        for c in np.stack(input_pts['pts_nncandA']):
            if c.size == 1:
                break
            cv2.circle(img, tuple((s * c[:2]).astype(int)), radius, (0, 255, 0), thickness=-1)    # 图像坐标系，先列后行
            
        for c in np.stack(input_pts['pts_H']):
            if c.size == 1:
                break
            c[0] += anchor
            cv2.circle(img, tuple((s * c[:2]).astype(int)), radius, (0, 0, 0), thickness=-1)
        
        for c in np.stack(input_pts['pts_nncandB']):
            if c.size == 1:
                break
            c[0] += anchor
            cv2.circle(img, tuple((s * c[:2]).astype(int)), radius, (255, 0, 0), thickness=-1)

        try:
            for c in np.stack(input_pts['pts_TH']):
                if c.size == 1:
                    break
                c[0] += anchor
                cv2.circle(img, tuple((s * c[:2]).astype(int)), radius, (0, 0, 255), thickness=-1)
        except:
            print("Coarse Match Point is out of range")    

        image_warp = self.inv_warp_image(torch.from_numpy(input_img['img']), H)
        image_warp = np.array(image_warp * 255).astype(np.uint8)
        imageB = (input_img['img_H'] * 255).astype(np.uint8)
        b = np.zeros_like(imageB)
        g = image_warp
        r = imageB
        img_match = cv2.merge([b, g, r])
        img = np.hstack((img, img_match))

        return img

    def draw_keypoints_pair_match(self, input_img, H):
        '''
        :param img:
            image:
            numpy [H, W]
        :param corners:
            Points
            numpy [N, 2]
        :param color:
        :param radius:
        :param s:
        :return:
            overlaying image
            numpy [H, W]
        '''
        image_warp = self.inv_warp_image(torch.from_numpy(input_img['img']), H)
        image_warp = np.array(image_warp * 255).astype(np.uint8)
        imageB = (input_img['img_H'] * 255).astype(np.uint8)
        b = np.zeros_like(imageB)
        g = image_warp
        r = imageB
        img_match = cv2.merge([b, g, r])
        img = np.array(img_match)

        return img


    def get_point_from_csv(self, point_csv):
        df = pd.read_csv(point_csv, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1).to(self.device)

        return pts

    def get_trans_from_list(self, trans_list):
        homography = np.array(trans_list)
        homography = torch.tensor(homography.reshape(2, 3), dtype=torch.float32)
        homography = 2 * homography / 512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homography = torch.cat((homography, vec_one), 0).inverse()

        '''Trans angle'''
        trans_obj = AffineTransform(matrix=homography.cpu().numpy())         # 122 x 36
        trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

        return homography, trans_angle

    def get_trans_from_list_message(self, trans_list):
        homography = np.array(trans_list)
        homography = torch.tensor(homography.reshape(2, 3), dtype=torch.float32)
        homography = 2 * homography / 512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homography = torch.cat((homography, vec_one), 0).inverse()

        '''Trans angle'''
        trans_obj = AffineTransform(matrix=homography.cpu().numpy())         # 122 x 36
        trans_angle = trans_obj.rotation * 180 / math.pi  # [-180, 180]
        trans_shear = trans_obj.shear * 180 / math.pi
        trans_scale = '_'.join(['%.4f' % s for s in trans_obj.scale])
        trans_translation = '_'.join(['%.4f' % t for t in trans_obj.translation])
        return homography, trans_angle, trans_shear, trans_scale, trans_translation    

    def get_trans(self, df, index):
        H0 = df['h0'].to_list()
        H0 = [(int)(h[6:]) if h[0] == 'T' else h for h in H0]
        H1 = df['h1'].to_list()
        H2 = df['h2'].to_list()
        H3 = df['h3'].to_list()
        H4 = df['h4'].to_list()
        H5 = df['h5'].to_list()
        homography = np.array([H0[index], H1[index], H2[index], H3[index], H4[index], H5[index]])
        # print(homography)
        homography = torch.tensor(homography.reshape(2, 3), dtype=torch.float32)
        homography = 2 * homography / 512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homography = torch.cat((homography, vec_one), 0).inverse()

        '''Trans angle'''
        trans_obj = AffineTransform(matrix=homography.cpu().numpy())         # 122 x 36
        trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

        return homography, trans_angle

    def get_point_index(self, point_csv, points, h=122):
        # H x W: 122 x 36
        df = pd.read_csv(point_csv, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pnts_index = (pnts_x * h + pnts_y).to(self.device)
        points_index = points[:, 0] * h + points[:, 1]
        # pts = torch.stack((pnts_x, pnts_y), dim=1).to(self.device)   # (x, y)

        inlier_index = torch.zeros(points.shape[0], device=self.device)
        # print(point_csv)
        for in_p_i in range(points.shape[0]):
            # print(torch.where(pnts_index == points_index[in_p_i])[0])
            # 重复点取index小的
            inlier_index[in_p_i] = torch.where(pnts_index == points_index[in_p_i])[0][0]

        return inlier_index.long(), pnts_index.shape[0]

    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_img_point_nncand_warp_info(self, pts_A, pts_B, imgall_dir, inliers_message, trans, W, H):
        pts_nncandA = torch.tensor(inliers_message, device=self.device)[:, :2]
        pts_nncandB = torch.tensor(inliers_message, device=self.device)[:, 4:6]

        warped_pts_A = warp_points(pts_nncandA, trans.squeeze().to(self.device), device=self.device)  # 利用变换矩阵变换坐标点
        inliers_pdis = torch.diag(self.get_dis(warped_pts_A[:, :2], pts_nncandB[:, :2]))
        warped_pts_A, _ = filter_points(warped_pts_A, torch.tensor([W, H], device=self.device), return_mask=True)

        pred = {}
        pred.update({
            "pts": pts_A.detach().cpu().numpy(), 
            "pts_H": pts_B.detach().cpu().numpy(),
            "pts_TH": warped_pts_A.detach().cpu().numpy(),
            "pts_nncandA": pts_nncandA.detach().cpu().numpy(),
            "pts_nncandB": pts_nncandB.detach().cpu().numpy(),
            })

        return pred, inliers_pdis

    def get_img_point_nncand_warp(self, pts_A, pts_B, imgall_dir, inliers_npy, trans, index, W, H):
        pts_nncandA = torch.tensor(inliers_npy['inliers_index'][index], device=self.device)[:, :2]
        pts_nncandB = torch.tensor(inliers_npy['inliers_index'][index], device=self.device)[:, 4:6]

        warped_pts_A = warp_points(pts_nncandA, trans.squeeze().to(self.device), device=self.device)  # 利用变换矩阵变换坐标点
        inliers_pdis = torch.diag(self.get_dis(warped_pts_A[:, :2], pts_nncandB[:, :2]))
        warped_pts_A, _ = filter_points(warped_pts_A, torch.tensor([W, H], device=self.device), return_mask=True)

        pred = {}
        pred.update({
            "pts": pts_A.detach().cpu().numpy(), 
            "pts_H": pts_B.detach().cpu().numpy(),
            "pts_TH": warped_pts_A.detach().cpu().numpy(),
            "pts_nncandA": pts_nncandA.detach().cpu().numpy(),
            "pts_nncandB": pts_nncandB.detach().cpu().numpy(),
            })

        return pred, inliers_pdis


    def get_common_match_index(self, sift_npy, net_npy, imgall_dir):
        sift_inliers_ms = np.load(sift_npy, allow_pickle=True).item()
        sift_match_num = len(sift_inliers_ms['match_name'])
        net_inliers_ms = np.load(net_npy, allow_pickle=True).item()
        net_match_num = len(net_inliers_ms['match_name'])

        common_dict = {}
        for i in range(sift_match_num + net_match_num):
            if i < sift_match_num:
                if sift_inliers_ms['match_name'][i] not in common_dict.keys():
                    common_dict[sift_inliers_ms['match_name'][i]] = [(i, sift_inliers_ms['inliers_index'][i].shape[0], 0)]
                else:
                    common_dict[sift_inliers_ms['match_name'][i]].append((i, sift_inliers_ms['inliers_index'][i].shape[0], 0))
            else:
                if net_inliers_ms['match_name'][i-sift_match_num] not in common_dict.keys():
                    common_dict[net_inliers_ms['match_name'][i-sift_match_num]] = [(i-sift_match_num, net_inliers_ms['inliers_index'][i-sift_match_num].shape[0], 1)]
                else:
                    common_dict[net_inliers_ms['match_name'][i-sift_match_num]].append((i-sift_match_num, net_inliers_ms['inliers_index'][i-sift_match_num].shape[0], 1))

        sift_inliers_common = []
        net_inliers_common = []
        content_inliers_common = []
        
        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df_sift = pd.read_csv(sift_npy.replace('fea', 'Trans').replace(sift_npy.split('/')[-1], 'SIFT_tranSucc' + self.mode + '.csv'), names=namelist)

        df_net = pd.read_csv(net_npy.replace('fea', 'Trans').replace(net_npy.split('/')[-1], 'Net_tranSucc' + self.mode + '.csv'), names=namelist)
        
        common_inliers_pdis = None
        common_inliers_pdis_net = None
        for match_name, match_list in common_dict.items():
            temp_name = '_'.join(match_name.split('_')[:3])
            samp_name = '_'.join(match_name.split('_')[3:])

            if len(match_list) == 2:
                sift_inliers_common.append(match_list[0][1])
                net_inliers_common.append(match_list[1][1])
                sift_index = match_list[0][0]
                net_index = match_list[1][0]
                sift_onescore = df_sift['score_flag'].tolist()[sift_index]
                net_onescore = df_net['score_flag'].tolist()[net_index]
                sift_trans, sift_trans_obj = self.get_trans(df_sift, sift_index)
                net_trans, net_trans_obj = self.get_trans(df_net, net_index)
                content_inliers_common.append([temp_name, samp_name, sift_index, net_index, match_list[0][1], match_list[1][1], sift_onescore, net_onescore, sift_trans_obj, net_trans_obj])          
                
                # points
                pts_A = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + temp_name + '.csv')
                pts_B = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + samp_name + '.csv')
                
                
                if abs(match_list[0][1] - match_list[1][1]) >= 0:
                    temp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(temp_name.split('_')) + '_extend.bmp'
                    samp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(samp_name.split('_')) + '_extend.bmp'
                    
                    # image
                    imgA = load_as_float(temp_img_name)         # 128 x 52
                    imgB = load_as_float(samp_img_name)
                    imgA = torch.tensor(imgA, dtype=torch.float32)
                    imgB = torch.tensor(imgB, dtype=torch.float32)

                    imgA = imgA[3:-3, 8:-8].unsqueeze(0)     # 122 x 36
                    imgB = imgB[3:-3, 8:-8].unsqueeze(0)

                    H, W = imgA.shape[1], imgA.shape[2]

                    pred, inliers_pdis = self.get_img_point_nncand_warp(pts_A, pts_B, imgall_dir, sift_inliers_ms, sift_trans, sift_index, W, H)
                    pred_net, inliers_pdis_net = self.get_img_point_nncand_warp(pts_A, pts_B, imgall_dir, net_inliers_ms, net_trans, net_index, W, H)

                    if common_inliers_pdis is None:
                        common_inliers_pdis = inliers_pdis
                    else:
                        common_inliers_pdis = torch.cat((common_inliers_pdis, inliers_pdis), dim=-1)

                    if common_inliers_pdis_net is None:
                        common_inliers_pdis_net = inliers_pdis_net
                    else:
                        common_inliers_pdis_net = torch.cat((common_inliers_pdis_net, inliers_pdis_net), dim=-1)

                    # print(imgA[0].shape)
                    img_pair = {}
                    img_pair.update({
                        "img": imgA[0].cpu().numpy().squeeze(),
                        "img_H": imgB[0].cpu().numpy().squeeze()
                        })

                    if self.plot_img:
                        img_pts = self.draw_keypoints_pair_train(img_pair, pred, radius=1, s=1, H=sift_trans.detach().cpu().squeeze())
                        img_pts_net = self.draw_keypoints_pair_train(img_pair, pred_net, radius=1, s=1, H=net_trans.detach().cpu().squeeze())
                        if match_list[0][1] - match_list[1][1] > 0:
                            os.makedirs(imgall_dir + 'common/' + self.mode + '/' + self.NetName + 'sift/', exist_ok=True)
                            f_all = imgall_dir + 'common/' + self.mode + '/' + self.NetName + 'sift/' + match_name + '_siftnet_' + str(match_list[0][1]) + '_' + str(match_list[1][1]) + ".bmp"
                        else:
                            os.makedirs(imgall_dir + 'common/' + self.mode + '/' + self.NetName + 'net/', exist_ok=True)
                            f_all = imgall_dir + 'common/'+ self.mode + '/' + self.NetName + 'net/' + match_name + '_siftnet_' + str(match_list[0][1]) + '_' + str(match_list[1][1]) + ".bmp"
                        # os.mkdirs()
                        img_pts_all = np.hstack((img_pts, img_pts_net))
                        # cv2.imwrite(f, img_pts)
                        # cv2.imwrite(f_net, img_pts_net)
                        cv2.imwrite(f_all, img_pts_all)
            elif len(match_list) == 1:
                temp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(temp_name.split('_')) + '_extend.bmp'
                samp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(samp_name.split('_')) + '_extend.bmp'
                
                # image
                imgA = load_as_float(temp_img_name)         # 128 x 52
                imgB = load_as_float(samp_img_name)
                imgA = torch.tensor(imgA, dtype=torch.float32)
                imgB = torch.tensor(imgB, dtype=torch.float32)

                imgA = imgA[3:-3, 8:-8].unsqueeze(0)     # 122 x 36
                imgB = imgB[3:-3, 8:-8].unsqueeze(0)

                H, W = imgA.shape[1], imgA.shape[2]
                # points
                pts_A = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + temp_name + '.csv')
                pts_B = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + samp_name + '.csv')
                
                img_pair = {}
                img_pair.update({
                    "img": imgA[0].cpu().numpy().squeeze(),
                    "img_H": imgB[0].cpu().numpy().squeeze()
                    })
                if self.plot_img:
                    if match_list[0][2] == 0:
                        # if match_list[0][1] <= 0:
                        #     continue
                        # sift
                        trans, _ = self.get_trans(df_sift, match_list[0][0])
                        pred, _ = self.get_img_point_nncand_warp(pts_A, pts_B, imgall_dir, sift_inliers_ms, trans, match_list[0][0], W, H)
                        os.makedirs(imgall_dir + 'unique/'+ self.mode + '/' + self.NetName + 'sift/', exist_ok=True)
                        f = imgall_dir + 'unique/'+ self.mode + '/' + self.NetName + 'sift/' + match_name + '_sift_' + str(match_list[0][1]) + ".bmp"
                    else:
                        # if match_list[0][1] > 60:
                        #     continue
                        trans, _ = self.get_trans(df_net, match_list[0][0])
                        pred, _ = self.get_img_point_nncand_warp(pts_A, pts_B, imgall_dir, net_inliers_ms, trans, match_list[0][0], W, H)
                        os.makedirs(imgall_dir + 'unique/'+ self.mode + '/' + self.NetName + 'net/', exist_ok=True)
                        f = imgall_dir + 'unique/'+ self.mode + '/' + self.NetName + 'net/' + match_name + '_net_' + str(match_list[0][1]) + ".bmp"

                    img_pts = self.draw_keypoints_pair_train(img_pair, pred, radius=1, s=1, H=trans.detach().cpu().squeeze())
                    cv2.imwrite(f, img_pts)
            else:
                continue

        common_inliers_pdis_hist = torch.histc(common_inliers_pdis, bins=100, min=0, max=10)
        common_inliers_pdis_hist_cs = torch.cumsum(common_inliers_pdis_hist / torch.sum(common_inliers_pdis_hist), dim=-1)
        common_inliers_pdis_net_hist = torch.histc(common_inliers_pdis_net, bins=100, min=0, max=10)   
        common_inliers_pdis_net_hist_cs = torch.cumsum(common_inliers_pdis_net_hist / torch.sum(common_inliers_pdis_net_hist), dim=-1)         

        return sift_inliers_common, net_inliers_common, content_inliers_common, common_inliers_pdis_hist, common_inliers_pdis_net_hist, common_inliers_pdis_hist_cs, common_inliers_pdis_net_hist_cs 


    def plot_fafr(self, fr_log, fa_log):
        fa100_frnum = []
        content_roc = []
        for score in range(10000, 310, -1):
            FR_num = torch.sum(fr_log >= score).item()
            FA_num = torch.sum(fa_log >= score).item()

            if FA_num <= 200:
                content_roc.append([FA_num, FR_num])
            if FA_num in [99, 100, 101]:
                fa100_frnum.append(FR_num)
        fa100_frnum_mean = torch.mean(torch.tensor(fa100_frnum).float()).item()
        df_roc = pd.DataFrame(content_roc, 
                            columns=[
                                'FA_num',
                                'FR_num',
                                ])
        return fa100_frnum_mean, df_roc


    def cal_info_trans(self, imgall_dir, info_head, out_info_txt):
        inlier_num_all = []
        inlier_num = 0
        inlier_flag = False
        inlier_message = []
        inlier_message_all = []
        inlier_mask_all = []
        match_name_all = []
        g_log_cnt_all = 0
        with open(info_head, mode='r') as trans_info_f:
            with open(out_info_txt, mode="w", encoding='utf-8') as out_trans_info_f:
                for line in trans_info_f:
                    # new concept
                    if 'g_log_cnt=' in line and 'g_classifier_th=' in line:
                        line = line.replace('\n', '')
                        g_log_cnt_all += int(line.split('g_log_cnt=')[-1])

                    # if 'ENROLL:' not in line and len(line.split(',')) == 2 and self.isnumber(line.split(',')[0]):
                    #     inlier_num = int(line.split(',')[0])
                    #     inlier_flag = True
                    #     inlier_message = []

                    if 'nNoInNumO=' in line and 'nNoInNumR=' in line:
                        line = line.replace('\n', '')
                        inlier_numO = int(line.split('nNoInNumO=')[-1].split(',')[0])
                        inlier_numR = int(line.split('nNoInNumR=')[-1])
                        OorR = 'NO REMATCH' in line # True: O False: R
                        inlier_flag = True
                        inliner_message = []
                    
                    if line[0] == ' ' and inlier_flag and len(line.split(',')) == 9:
                        line = line.replace('\n', '')
                        inlier_message.append([int(number) for number in line.split(',')[:-1]])

                    if 'Trans:' in line and line[0] == 'T':
                        line = line.replace('\n', '')
                        message = {
                            'Trans:': ','.join(line.split('Trans:')[-1].split(',')[:6])
                        }
                        # fea33 = line.split('Trans:')[-1].split(',')[6:-1]
                    
                    if 'ENROLL: ' in line and 'verify: ' in line and 'bmp,' in line and line[0] != 'E':
                        line = line.replace('\n', '')
                        ori_verify_path = line.split('verify: ')[1].split(',')[0]
                        ori_enroll_path = line.split('ENROLL: ')[1].split(' ')[0]

                        enroll_name = '_'.join(line.split('ENROLL: ')[1].split(' verify:')[0].replace('.bmp', '').split('/')[-3:])
                        verify_name = '_'.join(ori_verify_path.replace('.bmp', '').split('/')[-3:])
                        lib_name = imgall_dir[:-1].split('/')[-1]

                        message['ENROLL: '] = ori_enroll_path.replace(ori_enroll_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                        message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                        
                        person = int(ori_verify_path.split('/')[-3])
                        enroll_num = int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                        inlier_flag = False

                        # # 'FA'
                        # if enroll_num >= self.temp_num or person >= self.p_thr:
                        #    continue

                        # # 'FR'
                        # if enroll_num >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                        #     continue

                        message['path: '] = ori_verify_path.replace(ori_verify_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                        message['score='] = line.split(',')[0]
                        message['up='] = str(inlier_numR)
                        homography, trans_angle, trans_shear, trans_scale, trans_translation  = self.get_trans_from_list_message([int(h) for h in message['Trans:'].split(',')])
                        message['rotation='] = '%.4f' % trans_angle # str(trans_angle)
                        message['shear='] = '%.4f' % trans_shear # str(trans_shear)
                        message['scale='] = trans_scale # str(trans_scale)
                        message['translation='] = trans_translation # str(trans_translation)

                        temp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(enroll_name.split('_')) + '_extend.bmp'
                        samp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(verify_name.split('_')) + '_extend.bmp'
                        
                        # image
                        imgA = load_as_float(temp_img_name)         # 128 x 52
                        imgB = load_as_float(samp_img_name)
                        imgA = torch.tensor(imgA, dtype=torch.float32)
                        imgB = torch.tensor(imgB, dtype=torch.float32)

                        imgA = imgA[3:-3, 8:-8].unsqueeze(0)     # 122 x 36
                        imgB = imgB[3:-3, 8:-8].unsqueeze(0)

                        h, w = imgA.shape[1], imgA.shape[2]

                        # 部分按压mask
                        imgA_mask = load_as_float(temp_img_name.replace('_extend', '_mask'))         # 128 x 52
                        imgB_mask = load_as_float(samp_img_name.replace('_extend', '_mask'))
                        imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32)
                        imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32)

                        # 计算重叠面积
                        x = torch.linspace(0, w-1, w)     # ex: [-2, -1, 0, 1, 2]
                        y = torch.linspace(0, h-1, h) 
                        mesh_points = torch.stack(torch.meshgrid([y, x])).view(2, -1).t()[:, [1, 0]].to(homography.device)
                        mesh_points_norm = mesh_points / mesh_points.new_tensor([w - 1, h - 1]).to(mesh_points.device) * 2 - 1
                        pmask_kpA = F.grid_sample(imgA_mask.unsqueeze(0).unsqueeze(0),
                                        mesh_points_norm.view(1, 1, -1, 2),
                                        mode='bilinear', align_corners=True).squeeze() > (230 / 255)        # 1 X 1 X 1 X (n)

                        mesh_points_H = warp_points(mesh_points[:, :2], homography, device=homography.device)  # 利用变换矩阵变换坐标点
                        mesh_points_H, mask_point = filter_points(mesh_points_H, torch.tensor([w, h], device=homography.device), return_mask=True)
                        mesh_points_H_norm = mesh_points_H / mesh_points_H.new_tensor([w - 1, h - 1]).to(mesh_points_H.device) * 2 - 1
                        pmask_kpB = F.grid_sample(imgB_mask.unsqueeze(0).unsqueeze(0),
                                        mesh_points_H_norm.view(1, 1, -1, 2),
                                        mode='bilinear', align_corners=True).squeeze() > (230 / 255)        # 1 X 1 X 1 X (n)
                        valid_kpB_indexes = torch.tensor(range(h*w)).to(mesh_points_H.device)[mask_point][pmask_kpB]
                        valid_kpB_mask = torch.zeros_like(pmask_kpA).to(mesh_points_H.device)
                        valid_kpB_mask[valid_kpB_indexes] = 1

                        Repeat_area = mesh_points[(pmask_kpA * (valid_kpB_mask == 1)) == 1].shape[0]
                        message['Rarea='] = '%.4f' % (float(Repeat_area) / (h * w)) # str(Repeat_area)

                        # 计算黑白相似度              
                        image_warp = self.inv_warp_image(torch.from_numpy((imgA[0] * imgA_mask[0]).cpu().numpy().squeeze()), homography)
                        image_warp = np.array(image_warp * 255).astype(np.uint8)
                        imageB = ((imgB[0] * imgB_mask[0]).cpu().numpy().squeeze() * 255).astype(np.uint8)
                        bin_img_warp = torch.tensor(cv2.adaptiveThreshold(image_warp, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=7, C=0)).float() / 255
                        bin_imgB = torch.tensor(cv2.adaptiveThreshold(imageB, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=7, C=0)).float() / 255                       
                        bin_sim = torch.sum(bin_img_warp * bin_imgB) / Repeat_area
                        message['Bsim='] = '%.4f' % bin_sim.item() # str(bin_sim.item())
        
                        inlier_num_all.append(inlier_numR)
                        # inlier_message_all.append(np.array(inlier_message)) 
                        # inlier_mask_all.append(inlier_mask.cpu().numpy())
                        # fea33_all.append([enroll_name] + [verify_name] + fea33)
                        # match_name_all.append(enroll_name + '_' + verify_name)
                        out_trans_info_f.write(self.get_trans_str_more(message) + '\n')  
                out_trans_info_f.close()
            trans_info_f.close()
        print('Log_all_num:', g_log_cnt_all)
        print('Trans_num:', len(inlier_num_all))    
        print('Average_inlier_num:', torch.mean(torch.tensor(inlier_num_all, device=self.device).float()))   
        print('Std_inlier_num:', torch.std(torch.tensor(inlier_num_all, device=self.device).float()))   

    def get_info_messge_all(self, out_trans_info_txt):
        df_info_txt = pd.read_csv(
                out_trans_info_txt,
                header=None,
                encoding = "gb2312",
                names=['enroll', 'verify', 'path', 
                    'h0', 'h1', 'h2', 'h3', 'h4', 'h5',
                    'score', 'up', 'rotation', 'shear', 'scale', 'translation', 'Rarea', 'Bsim']           
        )
        im2_list = [i[7:] for i in df_info_txt['path'].to_list()]
        if self.mode == 'FR':
            suffix = '.bmp'
            im1_list = [('/'.join(i2.split('/')[:-1]) + '/' + i1[8:].rjust(4, '0') + suffix) for i1, i2 in zip(df_info_txt['enroll'].to_list(), im2_list)]  
        else:
            im1_list = [i[8:] for i in df_info_txt['enroll'].to_list()]
        image1_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im1_list]
        image2_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im2_list]
        image12_name = [(i1 + '#' + i2) for i1, i2 in zip(image1_name, image2_name)]
        return image12_name, df_info_txt

    def get_df_rowK(self, df_info, k):
        out_message = {}
        out_message['ENROLL: '] = df_info['enroll'].to_list()[k].split('ENROLL: ')[-1]
        out_message['verify: '] = df_info['verify'].to_list()[k].split('verify: ')[-1]
        out_message['path: '] =  df_info['path'].to_list()[k].split('path: ')[-1]
        out_message['Trans:'] = ','.join([str(h) for h in [df_info['h0'].to_list()[k][6:], df_info['h1'].to_list()[k], df_info['h2'].to_list()[k], df_info['h3'].to_list()[k], df_info['h4'].to_list()[k], df_info['h5'].to_list()[k]]])
        out_message['score='] = df_info['score'].to_list()[k].split('score=')[-1]
        out_message['up='] = df_info['up'].to_list()[k].split('up=')[-1]
        out_message['rotation='] = df_info['rotation'].to_list()[k].split('rotation=')[-1]
        out_message['shear='] = df_info['shear'].to_list()[k].split('shear=')[-1]
        out_message['scale='] = df_info['scale'].to_list()[k].split('scale=')[-1]
        out_message['translation='] = df_info['translation'].to_list()[k].split('translation=')[-1]
        out_message['Rarea='] = df_info['Rarea'].to_list()[k].split('Rarea=')[-1]
        out_message['Bsim='] = df_info['Bsim'].to_list()[k].split('Bsim=')[-1]
        return out_message

    def test_process(self, FPDT):
        for imgall_dir in self.desired_libray:

            if self.trans_pts_flag:
                # 转换点
                pts_head = imgall_dir + 'kpts.h'
                out_pts_txt = imgall_dir + 'point.txt'
                pts_flag = False
                pts_count = 0
                with open(pts_head, mode='r') as pts_f:
                    with open(out_pts_txt, mode="w", encoding='utf-8') as out_pts_f:
                        for line in pts_f:
                            # qint
                            if 'F:' in line and '>' not in line:
                                if line[0] == 'F':
                            # # lwc
                            # if 'E:' in line and '>' not in line:
                            #     if line[0] == 'E':
                                    line_new = line
                                elif 'image =' in line:
                                    line_new = line.split('=')[-1]
                                else:
                                    continue
                                img_path = line_new.replace(line_new.split('/')[0], imgall_dir[:-1]).replace('.bmp', '_enhance.bmp').replace('\n', '')
                                out_pts_f.write(img_path + '\n')
                                out_pts_f.write('n=' + str(150) + '\n')

                                pts_count = 0
                                pts_flag = True


                            if pts_flag and 'PRINT_KEYPOINT,X,Y,ORI,' in line:
                                line.replace('\n', '')
                                pt_str = line.split('PRINT_KEYPOINT,X,Y,ORI,')[-1]
                                out_pts_f.write('id:' + str(pts_count) + '\n')

                                pts_count += 1
                                out_pts_f.write(self.get_pts_str(pt_str) + '\n')
                        out_pts_f.write('/\n')
                        out_pts_f.close()
                    pts_f.close()

            if self.trans_trans_flag:
                # 转换trans
                trans_head = imgall_dir + 'trans.h'
                out_trans_txt = imgall_dir + 'trans.txt'  # 'trans.txt'
                print(imgall_dir)
                with open(trans_head, mode='r') as trans_f:
                    with open(out_trans_txt, mode="w", encoding='utf-8') as out_trans_f:
                        for line in trans_f:
                            # new concept
                            if 'Trans:' in line and 'ENROLL' in line and 'verify' in line and line[0] == 'T' and 'bmp,' in line and 'func' not in line:
                                line = line.replace('\n', '')
                                # print(line)
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]

                                message ={
                                    'Trans:': line.split('Trans:')[-1].split('ENROLL:')[0][:-1],
                                    'ENROLL: ': int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', '')),
                                    'verify: ': int(ori_verify_path.split('/')[-1].replace('.bmp', '')),
                                    'path: ': ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', ''),
                                    'score=': '0',
                                    'up=': '1'
                                }

                                person = int(ori_verify_path.split('/')[1])

                                # 判断图像质量和湿手指mask（是否能产生点和描述）
                                pnt_desc_path = imgall_dir[:-1] + '/pnt_desc_data/' + '_'.join(ori_verify_path.split('/')[1:]).replace('.bmp', '.csv')
                                print(pnt_desc_path)
                                # if not os.path.exists(pnt_desc_path):
                                #     continue

                                # 模板数和人数卡控
                                if message['ENROLL: '] >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr: # or message['verify: '] >= 120:
                                
                                # # if message['ENROLL: '] >= self.temp_num:
                                
                                # # 保留前20张的两两配对关系
                                # if message['ENROLL: '] >= self.temp_num or message['verify: '] >= self.temp_num or person >= self.p_thr:
                                    continue

                                out_trans_f.write(self.get_trans_str(message) + '\n')  
                                # 筛选trans
                        out_trans_f.close()
                    trans_f.close()

            if self.trans_trans_program_flag:
                # 转换trans
                trans_head = imgall_dir + 'trans_program.h'
                out_trans_txt = imgall_dir + 'trans_program.txt'  # 'trans.txt'
                print(imgall_dir)
                with open(trans_head, mode='r') as trans_f:
                    with open(out_trans_txt, mode="w", encoding='utf-8') as out_trans_f:
                        for line in trans_f:
                            # new concept
                            if ',ENROLL' in line and 'verify' in line and line[0] == ',' and 'bmp,' in line and 'func' not in line:
                                line = line.replace('\n', '')
                                # print(line)
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]

                                message ={
                                    'Trans:': ','.join(line.split('.bmp,')[-1].split(',')[:-1]),
                                    'ENROLL: ': int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', '')),
                                    'verify: ': int(ori_verify_path.split('/')[-1].replace('.bmp', '')),
                                    'path: ': ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', ''),
                                    'score=': '0',
                                    'up=': '1'
                                }

                                person = int(ori_verify_path.split('/')[1])

                                # 判断图像质量和湿手指mask（是否能产生点和描述）
                                pnt_desc_path = imgall_dir[:-1] + '/pnt_desc_data/' + '_'.join(ori_verify_path.split('/')[1:]).replace('.bmp', '.csv')
                                print(pnt_desc_path)
                                # if not os.path.exists(pnt_desc_path):
                                #     continue

                                # # 模板数和人数卡控
                                # if message['ENROLL: '] >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr: # or message['verify: '] >= 120:
                                
                                # # # if message['ENROLL: '] >= self.temp_num:
                                
                                # # # 保留前20张的两两配对关系
                                # # if message['ENROLL: '] >= self.temp_num or message['verify: '] >= self.temp_num or person >= self.p_thr:
                                #     continue

                                out_trans_f.write(self.get_trans_str(message) + '\n')  
                                # 筛选trans
                        out_trans_f.close()
                    trans_f.close()

            if self.trans_trans_compare_flag:
                # 转换trans
                frr_name = 'far' # 'frr' 'far'
                trans_head = imgall_dir + 'trans_compare_' + frr_name + '.h'
                out_trans_txt = imgall_dir + 'trans_compare_' + frr_name + '.txt'  # 'trans.txt'
                frr_flag = 'frr' in trans_head
                print(imgall_dir)
                with open(trans_head, mode='r') as trans_f:
                    with open(out_trans_txt, mode="w", encoding='utf-8') as out_trans_f:
                        for line in trans_f:
                            # new concept
                            if 'ENROLL:' in line and 'verify:' in line and line[0] == 'E' and 'trans,' in line and 'feats,' in line:
                                line = line.replace('\n', '')
                                # print(line)
                                ori_enroll_path = line.split('ENROLL: ')[1].split(' verify:')[0]
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]   # '../6193_DK7_random_merge_total/0000/L1/0004.bmp'

                                message ={
                                    'Trans:': line.split('trans,')[-1].split(',feats')[0],
                                    'ENROLL: ': int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', '')) if frr_flag else ori_enroll_path.replace(ori_enroll_path.split('/0')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', ''),
                                    'verify: ': int(ori_verify_path.split('/')[-1].replace('.bmp', '')),
                                    'path: ': ori_verify_path.replace(ori_verify_path.split('/0')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', ''),
                                    'score=': '0',
                                    'up=': '1'
                                }

                                person = int(ori_verify_path.split('/')[-3])

                                # 判断图像质量和湿手指mask（是否能产生点和描述）
                                pnt_desc_path = imgall_dir[:-1] + '/pnt_desc_data/' + '_'.join(ori_verify_path.split('/')[1:]).replace('.bmp', '.csv')
                                print(pnt_desc_path)
                                # if not os.path.exists(pnt_desc_path):
                                #     continue

                                # # 模板数和人数卡控
                                # if message['ENROLL: '] >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr: # or message['verify: '] >= 120:
                                
                                # # # if message['ENROLL: '] >= self.temp_num:
                                
                                # # # 保留前20张的两两配对关系
                                # # if message['ENROLL: '] >= self.temp_num or message['verify: '] >= self.temp_num or person >= self.p_thr:
                                #     continue

                                # trans 参数卡控

                                out_trans_f.write(self.get_trans_str(message) + '\n')  
                                # 筛选trans
                        out_trans_f.close()
                    trans_f.close()


            if self.trans_trans_info0_flag:
                # 转换info1 trans
                trans_info1_head = imgall_dir + 'trans_info0.h'
                out_trans_info1_txt = imgall_dir + 'trans_info0_FN.txt'

                # # 用于判断是否同时配上的info0和info1
                # trans_info0_txt = out_trans_info1_txt.replace('_info0', '') 
                # if self.mode == 'FA':
                #     trans_info0_txt = trans_info0_txt.replace('trans', 'FA_trans')

                # df_trans = pd.read_csv(
                #         trans_info0_txt,
                #         header=None,
                #         encoding = "gb2312",
                #         names=['enroll', 'verify', 'path', 
                #             'h0', 'h1', 'h2', 'h3', 'h4', 'h5',
                #             'score', 'up']           
                #         )
                # im2_list = [i[7:] for i in df_trans['path'].to_list()]
                # if self.mode == 'FR':
                #     suffix = '.bmp'
                #     im1_list = [('/'.join(i2.split('/')[:-1]) + '/' + i1[8:].rjust(4, '0') + suffix) for i1, i2 in zip(df_trans['enroll'].to_list(), im2_list)]  
                # else:
                #     im1_list = [i[8:] for i in df_trans['enroll'].to_list()]
                # image1_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im1_list]
                # image2_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im2_list]
                # image12_name = [(i1 + '#' + i2) for i1, i2 in zip(image1_name, image2_name)]
                # # print(image12_name)

                fea33_all = []  
                fea33_name = ['Enroll_name', 'Verify_name', 'nInDenseOrg', 'nInDenseRem', 'nInDenseQyh', 'nInliorig', 'nInliRem', 'nInliQyh', 'pkstInfo->nNoInNumO', 'pkstInfo->nNoInNumR', 'pkstInfo->nRecurNum', 'pkstInfo->nvalidNum', 
                              'nScoreL', 'nSimScore', 'nScoreEn', 'nOverLapTmp', 'pkstInfo->stSLSim.nOverlapAbs', 'nvliLhsScore', 'nScoreH', 'nPhi', 'nRot', 'nScoreSums', 'nImgQualitySum', 'nSnrSum', 'nInlinerSum', 
                              'pkstInfo->nInliRatio', 'pkstInfo->nInliRatioRem', 'pkstInfo->nInliRatioRecur', 'pkstInfo->nRecuRatio', 'nImgQualityDif', 'nMagDet', 'nSnrDif', 'nSignDif', 'nRecuImgRatio', 'pkstInfo->nGridsimi']  
                fea33_all.append(fea33_name)
                inlier_num_all = []
                inlier_num = 0
                inlier_flag = False
                inlier_message = []
                inlier_message_all = []
                inlier_mask_all = []
                match_name_all = []
                g_log_cnt_all = 0
                
                with open(trans_info1_head, mode='r') as trans_info1_f:
                    with open(out_trans_info1_txt, mode="w", encoding='utf-8') as out_trans_info1_f:
                        for line in trans_info1_f:
                            # new concept
                            if 'g_log_cnt=' in line and 'g_classifier_th=' in line:
                                line = line.replace('\n', '')
                                g_log_cnt_all += int(line.split('g_log_cnt=')[-1])

                            if 'ENROLL:' not in line and len(line.split(',')) == 2 and self.isnumber(line.split(',')[0]):
                                inlier_num = int(line.split(',')[0])
                                inlier_flag = True
                                inlier_message = []
                            
                            if line[0] == ' ' and inlier_flag and len(line.split(',')) == 9:
                                line = line.replace('\n', '')
                                inlier_message.append([int(number) for number in line.split(',')[:-1]])

                            if 'Trans:' in line and line[0] == 'T':
                                line = line.replace('\n', '')
                                message = {
                                    'Trans:': ','.join(line.split('Trans:')[-1].split(',')[:6])
                                }
                                # fea33 = line.split('Trans:')[-1].split(',')[6:-1]
                            
                            if 'ENROLL: ' in line and 'verify: ' in line and 'bmp,' in line and line[0] != 'E':
                                line = line.replace('\n', '')
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]
                                ori_enroll_path = line.split('ENROLL: ')[1].split(' ')[0]

                                enroll_name = '_'.join(line.split('ENROLL: ')[1].split(' verify:')[0].replace('.bmp', '').split('/')[-3:])
                                verify_name = '_'.join(ori_verify_path.replace('.bmp', '').split('/')[-3:])
                                lib_name = imgall_dir[:-1].split('/')[-1]

                                # message['ENROLL: '] = ori_enroll_path.replace(ori_enroll_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                                message['ENROLL: '] = int(ori_enroll_path.split('/')[-1].replace('.bmp', ''))
                                # ori_enroll_path.replace(ori_enroll_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')

                                person = int(ori_verify_path.split('/')[-3])
                                enroll_num = int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                                inlier_flag = False

                                # # 'FA'
                                # if enroll_num >= self.temp_num or person >= self.p_thr:
                                #    continue

                                # # 'FR'
                                # if enroll_num >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                                #     continue

                                # 真手指trans
                                # if enroll_num >= self.true_finger_num or message['verify: '] >= self.true_finger_num:
                                # if enroll_num >= self.true_finger_num or message['verify: '] < self.true_finger_num:
                                if enroll_num < self.true_finger_num or message['verify: '] < self.true_finger_num:
                                    continue

                                message['path: '] = ori_verify_path.replace(ori_verify_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['score='] = line.split(',')[0]
                                message['up='] = str(inlier_num)
                                
                                # temp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(enroll_name.split('_')) + '_extend.bmp'
                                # samp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(verify_name.split('_')) + '_extend.bmp'
                                
                                # # image
                                # imgA = load_as_float(temp_img_name)         # 128 x 52
                                # imgB = load_as_float(samp_img_name)
                                # imgA = torch.tensor(imgA, dtype=torch.float32)
                                # imgB = torch.tensor(imgB, dtype=torch.float32)

                                # imgA = imgA[3:-3, 8:-8].unsqueeze(0)     # 122 x 36
                                # imgB = imgB[3:-3, 8:-8].unsqueeze(0)

                                # H, W = imgA.shape[1], imgA.shape[2]
                                                                       
                                # img_pair = {}
                                # img_pair.update({
                                #     "img": imgA[0].cpu().numpy().squeeze(),
                                #     "img_H": imgB[0].cpu().numpy().squeeze()
                                #     })    

                                # # points
                                # pts_A = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + enroll_name + '.csv')
                                # pts_B = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + verify_name + '.csv')

                                # ev_name = enroll_name + '#' + verify_name
                                # print(ev_name)
                                # info1_trans, info1_trans_angle = self.get_trans_from_list([int(h) for h in message['Trans:'].split(',')])
                                # pred_info1, inliers_pdis_info1 = self.get_img_point_nncand_warp_info(pts_A, pts_B, imgall_dir, inlier_message, info1_trans, W, H)

                                # if self.plot_img:
                                #     img_pts_info1 = self.draw_keypoints_pair_train(img_pair, pred_info1, radius=1, s=1, H=info1_trans.detach().cpu().squeeze())

                                # if ev_name in image12_name:
                                #     # info0, info1同时有
                                #     ev_commen_index = image12_name.index(ev_name)
                                #     info0_trans, info0_trans_angle  = self.get_trans(df_trans, ev_commen_index)

                                #     img_pts_info0 = self.draw_keypoints_pair_match(img_pair, H=info0_trans.detach().cpu().squeeze())
                                #     os.makedirs(imgall_dir + 'common/info01/' + self.mode + '/', exist_ok=True)
                                #     f_all = imgall_dir + 'common/info01/' + self.mode + '/' + ev_name + ".bmp"
                                #     img_pts_all = np.hstack((img_pts_info1, img_pts_info0))
                                #     cv2.imwrite(f_all, img_pts_all)
                                # else:
                                #     os.makedirs(imgall_dir + 'unique/info1/' + self.mode + '/', exist_ok=True)
                                #     f_info1 = imgall_dir + 'unique/info1/' + self.mode + '/' + ev_name + ".bmp"
                                #     cv2.imwrite(f_info1, img_pts_info1)

                                # # inlier_mask
                                # inlier_message_torch = torch.tensor(inlier_message, device=self.device)
                                # enroll_inlier_index, enroll_pnum = self.get_point_index(imgall_dir + 'pnt_desc_data/' + enroll_name + '.csv', inlier_message_torch[:, :2])
                                # verify_inlier_index, verify_pnum = self.get_point_index(imgall_dir + 'pnt_desc_data/' + verify_name + '.csv', inlier_message_torch[:, 4:6])
                                # inlier_mask = torch.zeros((enroll_pnum, verify_pnum), device=self.device)

                                # inlier_mask[enroll_inlier_index, verify_inlier_index] = 1

                                inlier_num_all.append(inlier_num)
                                # inlier_message_all.append(np.array(inlier_message)) 
                                # inlier_mask_all.append(inlier_mask.cpu().numpy())
                                # fea33_all.append([enroll_name] + [verify_name] + fea33)
                                # match_name_all.append(enroll_name + '_' + verify_name)
                                out_trans_info1_f.write(self.get_trans_str(message) + '\n')  
                        out_trans_info1_f.close()
                    trans_info1_f.close()
                print('Log_all_num:', g_log_cnt_all)
                print('Trans_num:', len(inlier_num_all))    
                print('Average_inlier_num:', torch.mean(torch.tensor(inlier_num_all, device=self.device).float()))   
                print('Std_inlier_num:', torch.std(torch.tensor(inlier_num_all, device=self.device).float()))   


            if self.trans_trans_info1_flag:
                # 转换info1 trans
                trans_info1_head = imgall_dir + 'trans_info1.h'
                out_trans_info1_txt = imgall_dir + 'trans_info1.txt'

                # 用于判断是否同时配上的info0和info1
                trans_info0_txt = out_trans_info1_txt.replace('_info1', '') 
                if self.mode == 'FA':
                    trans_info0_txt = trans_info0_txt.replace('trans', 'FA_trans')

                df_trans = pd.read_csv(
                        trans_info0_txt,
                        header=None,
                        encoding = "gb2312",
                        names=['enroll', 'verify', 'path', 
                            'h0', 'h1', 'h2', 'h3', 'h4', 'h5',
                            'score', 'up']           
                        )
                im2_list = [i[7:] for i in df_trans['path'].to_list()]
                if self.mode == 'FR':
                    suffix = '.bmp'
                    im1_list = [('/'.join(i2.split('/')[:-1]) + '/' + i1[8:].rjust(4, '0') + suffix) for i1, i2 in zip(df_trans['enroll'].to_list(), im2_list)]  
                else:
                    im1_list = [i[8:] for i in df_trans['enroll'].to_list()]
                image1_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im1_list]
                image2_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im2_list]
                image12_name = [(i1 + '#' + i2) for i1, i2 in zip(image1_name, image2_name)]
                # print(image12_name)

                fea33_all = []  
                fea33_name = ['Enroll_name', 'Verify_name', 'nInDenseOrg', 'nInDenseRem', 'nInDenseQyh', 'nInliorig', 'nInliRem', 'nInliQyh', 'pkstInfo->nNoInNumO', 'pkstInfo->nNoInNumR', 'pkstInfo->nRecurNum', 'pkstInfo->nvalidNum', 
                              'nScoreL', 'nSimScore', 'nScoreEn', 'nOverLapTmp', 'pkstInfo->stSLSim.nOverlapAbs', 'nvliLhsScore', 'nScoreH', 'nPhi', 'nRot', 'nScoreSums', 'nImgQualitySum', 'nSnrSum', 'nInlinerSum', 
                              'pkstInfo->nInliRatio', 'pkstInfo->nInliRatioRem', 'pkstInfo->nInliRatioRecur', 'pkstInfo->nRecuRatio', 'nImgQualityDif', 'nMagDet', 'nSnrDif', 'nSignDif', 'nRecuImgRatio', 'pkstInfo->nGridsimi']  
                fea33_all.append(fea33_name)
                inlier_num_all = []
                inlier_num = 0
                inlier_flag = False
                inlier_message = []
                inlier_message_all = []
                inlier_mask_all = []
                match_name_all = []
                g_log_cnt_all = 0
                
                with open(trans_info1_head, mode='r') as trans_info1_f:
                    with open(out_trans_info1_txt, mode="w", encoding='utf-8') as out_trans_info1_f:
                        for line in trans_info1_f:
                            # new concept
                            if 'g_log_cnt=' in line and 'g_classifier_th=' in line:
                                line = line.replace('\n', '')
                                g_log_cnt_all += int(line.split('g_log_cnt=')[-1])

                            if 'ENROLL:' not in line and len(line.split(',')) == 2 and self.isnumber(line.split(',')[0]):
                                inlier_num = int(line.split(',')[0])
                                inlier_flag = True
                                inlier_message = []
                            
                            if line[0] == ' ' and inlier_flag and len(line.split(',')) == 9:
                                line = line.replace('\n', '')
                                inlier_message.append([int(number) for number in line.split(',')[:-1]])

                            if 'Trans:' in line and line[0] == 'T':
                                line = line.replace('\n', '')
                                message = {
                                    'Trans:': ','.join(line.split('Trans:')[-1].split(',')[:6])
                                }
                                # fea33 = line.split('Trans:')[-1].split(',')[6:-1]
                            
                            if 'ENROLL: ' in line and 'verify: ' in line and 'bmp,' in line and line[0] != 'E':
                                line = line.replace('\n', '')
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]
                                ori_enroll_path = line.split('ENROLL: ')[1].split(' ')[0]

                                enroll_name = '_'.join(line.split('ENROLL: ')[1].split(' verify:')[0].replace('.bmp', '').split('/')[-3:])
                                verify_name = '_'.join(ori_verify_path.replace('.bmp', '').split('/')[-3:])
                                lib_name = imgall_dir[:-1].split('/')[-1]

                                message['ENROLL: '] = ori_enroll_path.replace(ori_enroll_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                                
                                person = int(ori_verify_path.split('/')[-3])
                                enroll_num = int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                                inlier_flag = False

                                # # 'FA'
                                # if enroll_num >= self.temp_num or person >= self.p_thr:
                                #    continue

                                # # 'FR'
                                # if enroll_num >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                                #     continue

                                message['path: '] = ori_verify_path.replace(ori_verify_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['score='] = line.split(',')[0]
                                message['up='] = str(inlier_num)
                                
                                temp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(enroll_name.split('_')) + '_extend.bmp'
                                samp_img_name = imgall_dir + 'img_extend_data/' + '/'.join(verify_name.split('_')) + '_extend.bmp'
                                
                                # image
                                imgA = load_as_float(temp_img_name)         # 128 x 52
                                imgB = load_as_float(samp_img_name)
                                imgA = torch.tensor(imgA, dtype=torch.float32)
                                imgB = torch.tensor(imgB, dtype=torch.float32)

                                imgA = imgA[3:-3, 8:-8].unsqueeze(0)     # 122 x 36
                                imgB = imgB[3:-3, 8:-8].unsqueeze(0)

                                H, W = imgA.shape[1], imgA.shape[2]
                                                                       
                                img_pair = {}
                                img_pair.update({
                                    "img": imgA[0].cpu().numpy().squeeze(),
                                    "img_H": imgB[0].cpu().numpy().squeeze()
                                    })    

                                # points
                                pts_A = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + enroll_name + '.csv')
                                pts_B = self.get_point_from_csv(imgall_dir + 'pnt_desc_data/' + verify_name + '.csv')

                                ev_name = enroll_name + '#' + verify_name
                                print(ev_name)
                                info1_trans, info1_trans_angle = self.get_trans_from_list([int(h) for h in message['Trans:'].split(',')])
                                pred_info1, inliers_pdis_info1 = self.get_img_point_nncand_warp_info(pts_A, pts_B, imgall_dir, inlier_message, info1_trans, W, H)

                                if self.plot_img:
                                    img_pts_info1 = self.draw_keypoints_pair_train(img_pair, pred_info1, radius=1, s=1, H=info1_trans.detach().cpu().squeeze())

                                if ev_name in image12_name:
                                    # info0, info1同时有
                                    ev_commen_index = image12_name.index(ev_name)
                                    info0_trans, info0_trans_angle  = self.get_trans(df_trans, ev_commen_index)

                                    img_pts_info0 = self.draw_keypoints_pair_match(img_pair, H=info0_trans.detach().cpu().squeeze())
                                    os.makedirs(imgall_dir + 'common/info01/' + self.mode + '/', exist_ok=True)
                                    f_all = imgall_dir + 'common/info01/' + self.mode + '/' + ev_name + ".bmp"
                                    img_pts_all = np.hstack((img_pts_info1, img_pts_info0))
                                    cv2.imwrite(f_all, img_pts_all)
                                else:
                                    os.makedirs(imgall_dir + 'unique/info1/' + self.mode + '/', exist_ok=True)
                                    f_info1 = imgall_dir + 'unique/info1/' + self.mode + '/' + ev_name + ".bmp"
                                    cv2.imwrite(f_info1, img_pts_info1)

                                # # inlier_mask
                                # inlier_message_torch = torch.tensor(inlier_message, device=self.device)
                                # enroll_inlier_index, enroll_pnum = self.get_point_index(imgall_dir + 'pnt_desc_data/' + enroll_name + '.csv', inlier_message_torch[:, :2])
                                # verify_inlier_index, verify_pnum = self.get_point_index(imgall_dir + 'pnt_desc_data/' + verify_name + '.csv', inlier_message_torch[:, 4:6])
                                # inlier_mask = torch.zeros((enroll_pnum, verify_pnum), device=self.device)

                                # inlier_mask[enroll_inlier_index, verify_inlier_index] = 1

                                inlier_num_all.append(inlier_num)
                                # inlier_message_all.append(np.array(inlier_message)) 
                                # inlier_mask_all.append(inlier_mask.cpu().numpy())
                                # fea33_all.append([enroll_name] + [verify_name] + fea33)
                                # match_name_all.append(enroll_name + '_' + verify_name)
                                out_trans_info1_f.write(self.get_trans_str(message) + '\n')  
                        out_trans_info1_f.close()
                    trans_info1_f.close()
                print('Log_all_num:', g_log_cnt_all)
                print('Trans_num:', len(inlier_num_all))    
                print('Average_inlier_num:', torch.mean(torch.tensor(inlier_num_all, device=self.device).float()))   
                print('Std_inlier_num:', torch.std(torch.tensor(inlier_num_all, device=self.device).float()))   

            if self.trans_trans_info012_flag:
                
                trans_info0_head = imgall_dir + 'SIFT_Trans/trans_info012/trans_info0.h'
                trans_info1_head = imgall_dir + 'SIFT_Trans/trans_info012/trans_info1.h'
                trans_info2_head = imgall_dir + 'SIFT_Trans/trans_info012/trans_info2.h'
                out_trans_info0_txt = imgall_dir + 'SIFT_Trans/trans_info012/trans_info0.txt'
                out_trans_info1_txt = imgall_dir + 'SIFT_Trans/trans_info012/trans_info1.txt'
                out_trans_info2_txt = imgall_dir + 'SIFT_Trans/trans_info012/trans_info2.txt'
                # self.cal_info_trans(imgall_dir, trans_info0_head, out_trans_info0_txt)
                # self.cal_info_trans(imgall_dir, trans_info1_head, out_trans_info1_txt)
                # self.cal_info_trans(imgall_dir, trans_info2_head, out_trans_info2_txt)
                print('Cal common_txt:')
                out_trans_info_txt = imgall_dir + 'SIFT_Trans/trans_info012/trans_info_common.txt'

                image12_name_info0, df_info_txt0 = self.get_info_messge_all(out_trans_info0_txt)
                image12_name_info1, df_info_txt1 = self.get_info_messge_all(out_trans_info1_txt)
                image12_name_info2, df_info_txt2 = self.get_info_messge_all(out_trans_info2_txt)
                with open(out_trans_info_txt, mode="w", encoding='utf-8') as out_trans_info_f:
                    for i2 in image12_name_info2:
                        if i2 in image12_name_info0 and i2 in image12_name_info1:
                            common_index0 = image12_name_info0.index(i2)
                            common_index1 = image12_name_info1.index(i2)
                            common_index2 = image12_name_info2.index(i2)
                            # print(common_index0, common_index1, common_index2)
                            # info0 info1 info2 common
                            common_mes0 = self.get_df_rowK(df_info_txt0, common_index0)
                            common_mes1 = self.get_df_rowK(df_info_txt1, common_index1)
                            common_mes2 = self.get_df_rowK(df_info_txt2, common_index2)
                            out_trans_info_f.write(self.get_trans_str_more(common_mes0) + '\n')
                            out_trans_info_f.write(self.get_trans_str_more(common_mes1) + '\n')
                            out_trans_info_f.write(self.get_trans_str_more(common_mes2) + '\n')
                out_trans_info_f.close() 

            if self.pick_info_common_trans_flag:
                trans_info_common_txt = imgall_dir + 'SIFT_Trans/trans_info012/trans_info_common.txt'
                df_trans_common = pd.read_csv(
                        trans_info_common_txt,
                        header=None,
                        encoding = "gb2312",
                        names=['enroll', 'verify', 'path', 
                                'h0', 'h1', 'h2', 'h3', 'h4', 'h5',
                                'score', 'up', 'rotation', 'shear', 'scale', 'translation', 'Rarea', 'Bsim']          
                        )  
                im2_list = [i[7:] for i in df_trans_common['path'].to_list()]
                if self.mode == 'FR':
                    suffix = '.bmp'
                    im1_list = [('/'.join(i2.split('/')[:-1]) + '/' + i1[8:].rjust(4, '0')) for i1, i2 in zip(df_trans_common['enroll'].to_list(), im2_list)]  
                else:
                    im1_list = [i[8:] for i in df_trans_common['enroll'].to_list()]
                image1_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im1_list]
                image2_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in im2_list]
                image12_name = [(i1 + '#' + i2) for i1, i2 in zip(image1_name, image2_name)]

                up_all = [int(u[3:]) for u in df_trans_common['up'].to_list()]
                rotation_all = [float(r[9:]) for r in df_trans_common['rotation'].to_list()]
                shear_all = [float(s[6:]) for s in df_trans_common['shear'].to_list()]
                scale_all = [(float(sc[6:].split('_')[0]), float(sc[6:].split('_')[1])) for sc in df_trans_common['scale'].to_list()]
                translation_all = [(float(tr[12:].split('_')[0]), float(tr[12:].split('_')[1])) for tr in df_trans_common['translation'].to_list()]
                Rarea_all = [float(Ra[6:]) for Ra in df_trans_common['Rarea'].to_list()]
                Bsim_all = [float(Bs[5:]) for Bs in df_trans_common['Bsim'].to_list()]
                H0 = [int(h0[6:]) for h0 in df_trans_common['h0']]
                H1 = [int(h1) for h1 in df_trans_common['h1']]
                H2 = [int(h2) for h2 in df_trans_common['h2']]
                H3 = [int(h3) for h3 in df_trans_common['h3']]
                H4 = [int(h4) for h4 in df_trans_common['h4']]
                H5 = [int(h5) for h5 in df_trans_common['h5']]

                # 原训练集的trans
                trans_origin_txt = imgall_dir + 'trans_all.txt'
                df_trans_origin = pd.read_csv(
                        trans_origin_txt,
                        header=None,
                        encoding = "gb2312",
                        names=['enroll', 'verify', 'path', 
                            'h0', 'h1', 'h2', 'h3', 'h4', 'h5',
                            'score', 'up']           
                        )
                ori_im2_list = [i[7:] for i in df_trans_origin['path'].to_list()]
                if self.mode == 'FR':
                    suffix = '.bmp'
                    ori_im1_list = [('/'.join(i2.split('/')[:-1]) + '/' + i1[8:].rjust(4, '0') + suffix) for i1, i2 in zip(df_trans_origin['enroll'].to_list(), ori_im2_list)]  
                else:
                    ori_im1_list = [i[8:] for i in df_trans_origin['enroll'].to_list()]
                ori_image1_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in ori_im1_list]
                ori_image2_name = ['_'.join(k.replace('_extend', '').split('/')[-3:])[:-4] for k in ori_im2_list]
                ori_image12_name = [(i1 + '#' + i2) for i1, i2 in zip(ori_image1_name, ori_image2_name)]

                out_trans_info_choose_txt = imgall_dir + 'SIFT_Trans/trans_info012/trans_info_choose.txt'
                with open(out_trans_info_choose_txt, mode="w", encoding='utf-8') as out_trans_info_choose_f:
                    count = 0
                    for i in range(0, len(image12_name), 3):
                        samp_img_name = df_trans_common['path'].to_list()[i][7:].replace(imgall_dir.split('/')[-2], imgall_dir.split('/')[-2] + '/img_extend_data')
                        if self.enroll_full_flag:
                            temp_img_name = df_trans_common['enroll'].to_list()[i][8:]
                        else:
                            temp_img_name_suffix = df_trans_common['enroll'].to_list()[i][8:]
                            temp_img_name = samp_img_name.replace(samp_img_name[-23:], temp_img_name_suffix)

                        samp_num = int(samp_img_name.split('/')[-1][:-11])
                        samp_person = int(samp_img_name.split('/')[-3])
                        temp_num = int(temp_img_name.split('/')[-1][:-11])
                        temp_person = int(temp_img_name.split('/')[-3])

                        # 模板数和人数卡控
                        if temp_num >= self.temp_num or temp_person >= self.p_thr:
                        # if temp_num >= self.temp_num or samp_num < self.temp_num or temp_person >= self.p_thr:
                            continue

                        # 筛选指标： Rarea x Bsim 最大且 Bsim > 0.41
                        Bsim_thr = 0.41
                        shear_thr = 6
                        info0_metric = Rarea_all[i] * Bsim_all[i] * (Bsim_all[i] > Bsim_thr)
                        info1_metric = Rarea_all[i + 1] * Bsim_all[i + 1] * (Bsim_all[i + 1] > Bsim_thr)
                        info2_metric = Rarea_all[i + 2] * Bsim_all[i + 2] * (Bsim_all[i + 2] > Bsim_thr)
                        info_metric = [info0_metric, info1_metric, info2_metric]
                        if any(info_metric):
                            choose_info_num = info_metric.index(max(info_metric))
                            choose_mes = self.get_df_rowK(df_trans_common, i + choose_info_num)
                            out_trans_info_choose_f.write(self.get_trans_str_more_enroll(choose_mes, temp_num) + '\n')
                        else:
                            choose_info_num = -1
                            
                        info0_trans, _ = self.get_trans_from_list([H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]])
                        info1_trans, _ = self.get_trans_from_list([H0[i+1], H1[i+1], H2[i+1], H3[i+1], H4[i+1], H5[i+1]])
                        info2_trans, _ = self.get_trans_from_list([H0[i+2], H1[i+2], H2[i+2], H3[i+2], H4[i+2], H5[i+2]])

                        if self.plot_img:
                            # image
                            imgA = load_as_float(temp_img_name)         # 128 x 52
                            imgB = load_as_float(samp_img_name)
                            imgA = torch.tensor(imgA, dtype=torch.float32)
                            imgB = torch.tensor(imgB, dtype=torch.float32)

                            imgA = imgA[3:-3, 8:-8].unsqueeze(0)     # 122 x 36
                            imgB = imgB[3:-3, 8:-8].unsqueeze(0)

                            H, W = imgA.shape[1], imgA.shape[2]
                                                                    
                            img_pair = {}
                            img_pair.update({
                                "img": imgA[0].cpu().numpy().squeeze(),
                                "img_H": imgB[0].cpu().numpy().squeeze()
                                })  
                            img_pts_info0 = self.draw_keypoints_pair_match(img_pair, H=info0_trans.detach().cpu().squeeze())
                            img_pts_info1 = self.draw_keypoints_pair_match(img_pair, H=info1_trans.detach().cpu().squeeze())
                            img_pts_info2 = self.draw_keypoints_pair_match(img_pair, H=info2_trans.detach().cpu().squeeze())
                            os.makedirs(imgall_dir + 'common/info012/' + self.mode + '/', exist_ok=True)
                            f_all = imgall_dir + 'common/info012/' + self.mode + '/' + image12_name[i] + '_' + '_'.join([str(u) for u in Bsim_all[i:i+3]]) + '_info' + str(choose_info_num) + ".bmp"
                            if image12_name[i] in ori_image12_name:
                                ori_index = ori_image12_name.index(image12_name[i])
                                ori_trans, _ = self.get_trans(df_trans_origin, ori_index)
                                img_pts_ori = self.draw_keypoints_pair_match(img_pair, H=ori_trans.detach().cpu().squeeze())
                                img_pts_all = np.hstack((img_pts_info0, img_pts_info1, img_pts_info2, img_pts_ori))
                            else:
                                img_pts_all = np.hstack((img_pts_info0, img_pts_info1, img_pts_info2))
                            cv2.imwrite(f_all, img_pts_all)    
                out_trans_info_choose_f.close()    

            if self.FAtrans_trans_flag:
                # 转换trans
                FAtrans_head = imgall_dir + 'SIFT_Trans/FA/FA_trans.h'
                FAout_trans_txt = imgall_dir + 'SIFT_Trans/FA/FA_trans.txt'
                FAout_fea33_csv = imgall_dir + 'SIFT_fea/FA/FA_fea33.csv'
    
                fea33_all = []  
                fea33_name = ['Enroll_name', 'Verify_name', 'nInDenseOrg', 'nInDenseRem', 'nInDenseQyh', 'nInliorig', 'nInliRem', 'nInliQyh', 'pkstInfo->nNoInNumO', 'pkstInfo->nNoInNumR', 'pkstInfo->nRecurNum', 'pkstInfo->nvalidNum', 
                              'nScoreL', 'nSimScore', 'nScoreEn', 'nOverLapTmp', 'pkstInfo->stSLSim.nOverlapAbs', 'nvliLhsScore', 'nScoreH', 'nPhi', 'nRot', 'nScoreSums', 'nImgQualitySum', 'nSnrSum', 'nInlinerSum', 
                              'pkstInfo->nInliRatio', 'pkstInfo->nInliRatioRem', 'pkstInfo->nInliRatioRecur', 'pkstInfo->nRecuRatio', 'nImgQualityDif', 'nMagDet', 'nSnrDif', 'nSignDif', 'nRecuImgRatio', 'pkstInfo->nGridsimi']  
                fea33_all.append(fea33_name)
                inlier_num_all = []
                inlier_num = 0

                with open(FAtrans_head, mode='r') as FAtrans_f:
                    with open(FAout_trans_txt, mode="w", encoding='utf-8') as FAout_trans_f:
                        for line in FAtrans_f:      
                            # new concept
                            if 'ENROLL:' not in line and len(line.split(',')) == 2 and self.isnumber(line.split(',')[0]):
                                inlier_num = int(line.split(',')[0])

                            if 'Trans:' in line and line[0] == 'T':
                                line = line.replace('\n', '')
                                message = {
                                    'Trans:': ','.join(line.split('Trans:')[-1].split(',')[:6])
                                }
                                fea33 = line.split('Trans:')[-1].split(',')[6:-1]
                            
                            if 'ENROLL: ' in line and 'verify: ' in line and 'bmp,' in line :
                                line = line.replace('\n', '')
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]
                                ori_enroll_path = line.split('ENROLL: ')[1].split(',')[0]

                                enroll_name = '_'.join(line.split('ENROLL: ')[1].split(' verify:')[0].replace('.bmp', '').split('/')[1:])
                                verify_name = '_'.join(ori_verify_path.replace('.bmp', '').split('/')[1:])

                                message['ENROLL: '] = ori_enroll_path.replace(ori_enroll_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                                message['path: '] = ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['score='] = line.split(',')[0]
                                message['up='] = str(inlier_num)

                                person = int(ori_verify_path.split('/')[1])
                                enroll_num = int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                                if enroll_num >= self.temp_num or person >= self.p_thr:
                                # if message['ENROLL: '] >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                                # if message['ENROLL: '] >= self.temp_num:
                                    continue
                                
                                inlier_num_all.append(inlier_num) 
                                fea33_all.append([enroll_name] + [verify_name] + fea33)
                                FAout_trans_f.write(self.get_trans_str(message) + '\n')  
          
                        FAout_trans_f.close()
                    FAtrans_f.close()

                df_fea33 = pd.DataFrame(np.array(fea33_all))
                df_fea33.to_csv(FAout_fea33_csv, header=None)  
                print('Trans_num:', len(inlier_num_all))    
                print('Average_inlier_num:', torch.mean(torch.tensor(inlier_num_all, device=self.device).float()))                                    

            if self.FRtrans_trans_flag:
                # 转换trans
                FRtrans_head = imgall_dir + 'SIFT_Trans/FR/FR_trans.h'
                FRout_trans_txt = imgall_dir + 'SIFT_Trans/FR/FR_trans.txt'
                FRout_fea33_csv = imgall_dir + 'SIFT_fea/FR/FR_fea33.csv'
    
                fea33_all = []  
                fea33_name = ['Enroll_name', 'Verify_name', 'nInDenseOrg', 'nInDenseRem', 'nInDenseQyh', 'nInliorig', 'nInliRem', 'nInliQyh', 'pkstInfo->nNoInNumO', 'pkstInfo->nNoInNumR', 'pkstInfo->nRecurNum', 'pkstInfo->nvalidNum', 
                              'nScoreL', 'nSimScore', 'nScoreEn', 'nOverLapTmp', 'pkstInfo->stSLSim.nOverlapAbs', 'nvliLhsScore', 'nScoreH', 'nPhi', 'nRot', 'nScoreSums', 'nImgQualitySum', 'nSnrSum', 'nInlinerSum', 
                              'pkstInfo->nInliRatio', 'pkstInfo->nInliRatioRem', 'pkstInfo->nInliRatioRecur', 'pkstInfo->nRecuRatio', 'nImgQualityDif', 'nMagDet', 'nSnrDif', 'nSignDif', 'nRecuImgRatio', 'pkstInfo->nGridsimi']  
                fea33_all.append(fea33_name)
                inlier_num_all = []
                inlier_num = 0
                with open(FRtrans_head, mode='r') as FRtrans_f:
                    with open(FRout_trans_txt, mode="w", encoding='utf-8') as FRout_trans_f:
                        for line in FRtrans_f:      
                            # new concept
                            if 'ENROLL:' not in line and len(line.split(',')) == 2 and self.isnumber(line.split(',')[0]):
                                inlier_num = int(line.split(',')[0])

                            if 'Trans:' in line and line[0] == 'T':
                                line = line.replace('\n', '')
                                message = {
                                    'Trans:': ','.join(line.split('Trans:')[-1].split(',')[:6])
                                }
                                fea33 = line.split('Trans:')[-1].split(',')[6:-1]
                            
                            if 'ENROLL: ' in line and 'verify: ' in line and 'bmp,' in line :
                                line = line.replace('\n', '')
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]
                                ori_enroll_path = line.split('ENROLL: ')[1].split(',')[0]

                                enroll_name = '_'.join(line.split('ENROLL: ')[1].split(' verify:')[0].replace('.bmp', '').split('/')[1:])
                                verify_name = '_'.join(ori_verify_path.replace('.bmp', '').split('/')[1:])

                                message['ENROLL: '] = ori_enroll_path.replace(ori_enroll_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                                message['path: '] = ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['score='] = line.split(',')[0]
                                message['up='] = str(inlier_num)

                                person = int(ori_verify_path.split('/')[1])
                                enroll_num = int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                                # if enroll_num >= self.temp_num or person >= self.p_thr:
                                if enroll_num >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                                # if message['ENROLL: '] >= self.temp_num:
                                    continue
                                
                                inlier_num_all.append(inlier_num) 
                                fea33_all.append([enroll_name] + [verify_name] + fea33)
                                FRout_trans_f.write(self.get_trans_str(message) + '\n')  
          
                        FRout_trans_f.close()
                    FRtrans_f.close()

                df_fea33 = pd.DataFrame(np.array(fea33_all))
                df_fea33.to_csv(FRout_fea33_csv, header=None)     
                print('Trans_num:', len(inlier_num_all))    
                print('Average_inlier_num:', torch.mean(torch.tensor(inlier_num_all, device=self.device).float()))                                 

            if self.FRtrans_trans_net_flag:
                # 转换trans
                FRtrans_net_head = imgall_dir + 'Net_Trans/FR/FR_trans_net.h'
                FRout_trans_net_txt = imgall_dir + 'Net_Trans/FR/FR_trans_net.txt'
                FRout_fea33_net_csv = imgall_dir + 'Net_fea/FR/FR_net_fea33.csv'
    
                fea33_all = []  
                fea33_name = ['Enroll_name', 'Verify_name', 'nInDenseOrg', 'nInDenseRem', 'nInDenseQyh', 'nInliorig', 'nInliRem', 'nInliQyh', 'pkstInfo->nNoInNumO', 'pkstInfo->nNoInNumR', 'pkstInfo->nRecurNum', 'pkstInfo->nvalidNum', 
                              'nScoreL', 'nSimScore', 'nScoreEn', 'nOverLapTmp', 'pkstInfo->stSLSim.nOverlapAbs', 'nvliLhsScore', 'nScoreH', 'nPhi', 'nRot', 'nScoreSums', 'nImgQualitySum', 'nSnrSum', 'nInlinerSum', 
                              'pkstInfo->nInliRatio', 'pkstInfo->nInliRatioRem', 'pkstInfo->nInliRatioRecur', 'pkstInfo->nRecuRatio', 'nImgQualityDif', 'nMagDet', 'nSnrDif', 'nSignDif', 'nRecuImgRatio', 'pkstInfo->nGridsimi']  
                fea33_all.append(fea33_name)
                inlier_num_all = []
                inlier_num = 0
                with open(FRtrans_net_head, mode='r') as FRtrans_net_f:
                    with open(FRout_trans_net_txt, mode="w", encoding='utf-8') as FRout_trans_net_f:
                        for line in FRtrans_net_f:      
                            # new concept
                            if 'ENROLL:' not in line and len(line.split(',')) == 2 and self.isnumber(line.split(',')[0]):
                                inlier_num = int(line.split(',')[0])

                            if 'Trans:' in line and line[0] == 'T':
                                line = line.replace('\n', '')
                                message = {
                                    'Trans:': ','.join(line.split('Trans:')[-1].split(',')[:6])
                                }
                                fea33 = line.split('Trans:')[-1].split(',')[6:-1]
                            
                            if 'ENROLL: ' in line and 'verify: ' in line and 'bmp,' in line :
                                line = line.replace('\n', '')
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]
                                ori_enroll_path = line.split('ENROLL: ')[1].split(',')[0]

                                enroll_name = '_'.join(line.split('ENROLL: ')[1].split(' verify:')[0].replace('.bmp', '').split('/')[1:])
                                verify_name = '_'.join(ori_verify_path.replace('.bmp', '').split('/')[1:])

                                message['ENROLL: '] = ori_enroll_path.replace(ori_enroll_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                                message['path: '] = ori_verify_path.replace(ori_verify_path.split('/')[0], imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['score='] = line.split(',')[0]
                                message['up='] = str(inlier_num)

                                person = int(ori_verify_path.split('/')[1])
                                enroll_num = int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                                # if enroll_num >= self.temp_num or person >= self.p_thr:
                                if enroll_num >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                                # if message['ENROLL: '] >= self.temp_num:
                                    continue

                                inlier_num_all.append(inlier_num) 
                                fea33_all.append([enroll_name] + [verify_name] + fea33)
                                FRout_trans_net_f.write(self.get_trans_str(message) + '\n') 
          
                        FRout_trans_net_f.close()
                    FRtrans_net_f.close()

                df_fea33 = pd.DataFrame(np.array(fea33_all))
                df_fea33.to_csv(FRout_fea33_net_csv, header=None)    

                print('Trans_num:', len(inlier_num_all))    
                print('Average_inlier_num:', torch.mean(torch.tensor(inlier_num_all, device=self.device).float()))                         

            if self.FAtrans_trans_net_flag:
                # 转换trans
                FAtrans_net_head = imgall_dir + self.model + '_Trans/' + self.mode + '/' + self.NetName +  self.mode + '_trans_' + self.model + '.h'
                FAout_trans_net_txt = FAtrans_net_head.replace('.h', '.txt')
                FAout_fea33_net_csv = FAtrans_net_head.replace('Trans', 'fea').replace('trans_' + self.model + '.h', self.model + '_fea33.csv')
                FAout_inliers_net_npy = FAout_fea33_net_csv.replace('fea33.csv', 'inliers.npy')
    
                fea33_all = []  
                fea33_name = ['Enroll_name', 'Verify_name', 'nInDenseOrg', 'nInDenseRem', 'nInDenseQyh', 'nInliorig', 'nInliRem', 'nInliQyh', 'pkstInfo->nNoInNumO', 'pkstInfo->nNoInNumR', 'pkstInfo->nRecurNum', 'pkstInfo->nvalidNum', 
                              'nScoreL', 'nSimScore', 'nScoreEn', 'nOverLapTmp', 'pkstInfo->stSLSim.nOverlapAbs', 'nvliLhsScore', 'nScoreH', 'nPhi', 'nRot', 'nScoreSums', 'nImgQualitySum', 'nSnrSum', 'nInlinerSum', 
                              'pkstInfo->nInliRatio', 'pkstInfo->nInliRatioRem', 'pkstInfo->nInliRatioRecur', 'pkstInfo->nRecuRatio', 'nImgQualityDif', 'nMagDet', 'nSnrDif', 'nSignDif', 'nRecuImgRatio', 'pkstInfo->nGridsimi']  
                fea33_all.append(fea33_name)
                inlier_num_all = []
                inlier_num = 0
                inlier_flag = False
                inlier_message = []
                inlier_message_all = []
                inlier_mask_all = []
                match_name_all = []
                g_log_cnt_all = 0
                with open(FAtrans_net_head, mode='r') as FAtrans_net_f:
                    with open(FAout_trans_net_txt, mode="w", encoding='utf-8') as FAout_trans_net_f:
                        for line in FAtrans_net_f:      
                            # new concept
                            if 'g_log_cnt=' in line and 'g_classifier_th=' in line:
                                line = line.replace('\n', '')
                                g_log_cnt_all += int(line.split('g_log_cnt=')[-1])

                            if 'ENROLL:' not in line and len(line.split(',')) == 2 and self.isnumber(line.split(',')[0]):
                                inlier_num = int(line.split(',')[0])
                                inlier_flag = True
                                inlier_message = []
                            
                            if line[0] == ' ' and inlier_flag and len(line.split(',')) == 9:
                                line = line.replace('\n', '')
                                inlier_message.append([int(number) for number in line.split(',')[:-1]])

                            if 'Trans:' in line and line[0] == 'T':
                                line = line.replace('\n', '')
                                message = {
                                    'Trans:': ','.join(line.split('Trans:')[-1].split(',')[:6])
                                }
                                fea33 = line.split('Trans:')[-1].split(',')[6:-1]
                            
                            if 'ENROLL: ' in line and 'verify: ' in line and 'bmp,' in line and line[0] != 'E':
                                line = line.replace('\n', '')
                                ori_verify_path = line.split('verify: ')[1].split(',')[0]
                                ori_enroll_path = line.split('ENROLL: ')[1].split(' ')[0]

                                enroll_name = '_'.join(line.split('ENROLL: ')[1].split(' verify:')[0].replace('.bmp', '').split('/')[-3:])
                                verify_name = '_'.join(ori_verify_path.replace('.bmp', '').split('/')[-3:])
                                lib_name = imgall_dir[:-1].split('/')[-1]

                                message['ENROLL: '] = ori_enroll_path.replace(ori_enroll_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['verify: '] = int(ori_verify_path.split('/')[-1].replace('.bmp', ''))
                                
                                person = int(ori_verify_path.split('/')[-3])
                                enroll_num = int(line.split('ENROLL: ')[1].split(' verify:')[0].split('/')[-1].replace('.bmp', ''))
                                inlier_flag = False

                                # # 'FA'
                                # if enroll_num >= self.temp_num or person >= self.p_thr:
                                #    continue

                                # 'FR'
                                if enroll_num >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                                    continue

                                message['path: '] = ori_verify_path.replace(ori_verify_path.split(lib_name)[0] + lib_name, imgall_dir[:-1] + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '')
                                message['score='] = line.split(',')[0]
                                message['up='] = str(inlier_num)
                                
                                inlier_message_torch = torch.tensor(inlier_message, device=self.device)
                                enroll_inlier_index, enroll_pnum = self.get_point_index(imgall_dir + 'pnt_desc_data/' + enroll_name + '.csv', inlier_message_torch[:, :2])
                                verify_inlier_index, verify_pnum = self.get_point_index(imgall_dir + 'pnt_desc_data/' + verify_name + '.csv', inlier_message_torch[:, 4:6])
                                inlier_mask = torch.zeros((enroll_pnum, verify_pnum), device=self.device)
                                # print(enroll_inlier_index, verify_inlier_index)
                                inlier_mask[enroll_inlier_index, verify_inlier_index] = 1


                                # if enroll_num >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr:
                                # if message['ENROLL: '] >= self.temp_num:
                                #     continue

                                inlier_num_all.append(inlier_num)
                                inlier_message_all.append(np.array(inlier_message)) 
                                inlier_mask_all.append(inlier_mask.cpu().numpy())
                                fea33_all.append([enroll_name] + [verify_name] + fea33)
                                match_name_all.append(enroll_name + '_' + verify_name)
                                FAout_trans_net_f.write(self.get_trans_str(message) + '\n') 
          
                        FAout_trans_net_f.close()
                    FAtrans_net_f.close()

                df_fea33 = pd.DataFrame(np.array(fea33_all))
                df_fea33.to_csv(FAout_fea33_net_csv, header=None)    
                
                inliers_dict = {'match_name': match_name_all, 'inliers_index': inlier_message_all, 'inliers_mask': inlier_mask_all}
                np.save(FAout_inliers_net_npy, inliers_dict)

                print('Log_all_num:', g_log_cnt_all)
                print('Trans_num:', len(inlier_num_all))    
                print('Average_inlier_num:', torch.mean(torch.tensor(inlier_num_all, device=self.device).float()))   
                print('Std_inlier_num:', torch.std(torch.tensor(inlier_num_all, device=self.device).float()))                       

            if self.cal_common_flag:
                # sift
                sift_inlier_npy = imgall_dir + 'SIFT_fea/' + self.mode + '/' + self.mode + '_SIFT_inliers.npy'
                # Net
                net_inlier_npy = imgall_dir + 'Net_fea/' + self.mode + '/' + self.NetName + self.mode + '_Net_inliers.npy'

                sift_inliers_common, net_inliers_common, content_inliers_common, common_inliers_pdis_hist, common_inliers_pdis_net_hist, common_inliers_pdis_hist_cs, common_inliers_pdis_net_hist_cs = self.get_common_match_index(sift_inlier_npy, net_inlier_npy, imgall_dir)
                common_inliers_pdis_hist_all = torch.cat((common_inliers_pdis_hist.unsqueeze(0), common_inliers_pdis_net_hist.unsqueeze(0), common_inliers_pdis_hist_cs.unsqueeze(0), common_inliers_pdis_net_hist_cs.unsqueeze(0)), dim=0)
                
                print('Trans_num_common:', len(sift_inliers_common))   
                print('SIFT:') 
                print('Average_inlier_num_common:', torch.mean(torch.tensor(sift_inliers_common, device=self.device).float()))  
                print('Std_inlier_num_common:', torch.std(torch.tensor(sift_inliers_common, device=self.device).float()))  
                print('Net:') 
                print('Average_inlier_num_common:', torch.mean(torch.tensor(net_inliers_common, device=self.device).float()))  
                print('Std_inlier_num_common:', torch.std(torch.tensor(net_inliers_common, device=self.device).float()))  

                df = pd.DataFrame(content_inliers_common, 
                        columns=[
                            'temp_name',
                            'samp_name',
                            'sift_index',
                            'net_index',
                            'sift_inliers',
                            'net_inliers',
                            'sift_onescore',
                            'net_onescore',
                            'sift_trans_obj',
                            'net_trans_obj'
                            ])
                df.to_csv(os.path.join(imgall_dir + 'common/' + self.mode + '/' + self.NetName, self.mode + '_inliers_common.csv'))

                df_inliers_pdis_hist = pd.DataFrame(common_inliers_pdis_hist_all.tolist(), columns=[str(i*0.1) for i in range(100)])
                df_inliers_pdis_hist.to_csv(os.path.join(imgall_dir + 'common/' + self.mode + '/' + self.NetName, self.mode + '_inliers_pdis_common.csv'))

            if self.plot_ROC:
                namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
                # sift
                sift_fr_npy = imgall_dir + 'SIFT_fea/FR/FR_SIFT_inliers.npy'
                sift_fa_npy = imgall_dir + 'SIFT_fea/FA/FA_SIFT_inliers.npy'
                # Net
                net_fr_npy = imgall_dir + 'Net_fea/FR/' + self.NetName + 'FR_Net_inliers.npy'
                net_fa_npy = imgall_dir + 'Net_fea/FA/' + self.NetName + 'FA_Net_inliers.npy'

                df_sift_fr = pd.read_csv(sift_fr_npy.replace('fea', 'Trans').replace(sift_fr_npy.split('/')[-1], 'SIFT_tranSuccFR.csv'), names=namelist)
                df_sift_fa = pd.read_csv(sift_fa_npy.replace('fea', 'Trans').replace(sift_fa_npy.split('/')[-1], 'SIFT_tranSuccFA.csv'), names=namelist)
                df_net_fr = pd.read_csv(net_fr_npy.replace('fea', 'Trans').replace(net_fr_npy.split('/')[-1], 'Net_tranSuccFR.csv'), names=namelist)
                df_net_fa = pd.read_csv(net_fa_npy.replace('fea', 'Trans').replace(net_fa_npy.split('/')[-1], 'Net_tranSuccFA.csv'), names=namelist)

                sift_fr_log = torch.tensor(df_sift_fr['score_flag'].tolist()).to(self.device)
                sift_fa_log = torch.tensor(df_sift_fa['score_flag'].tolist()).to(self.device)
                net_fr_log = torch.tensor(df_net_fr['score_flag'].tolist()).to(self.device)
                net_fa_log = torch.tensor(df_net_fa['score_flag'].tolist()).to(self.device)

                sift_fa100, df_roc_sift = self.plot_fafr(sift_fr_log, sift_fa_log)
                net_fa100, df_roc_net = self.plot_fafr(net_fr_log, net_fa_log)
                print('SIFT FA100:', sift_fa100)
                print('Net FA100:', net_fa100) 
                df_roc_sift.to_csv(os.path.join(imgall_dir + 'roc/', 'sift_roc.csv'))
                df_roc_net.to_csv(os.path.join(imgall_dir + 'roc/' + self.NetName, 'net_roc.csv')) 

        pass      


class Get_Pts_Descriptor_From_Head2(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.classify_ori_enhance_flag = False  #  True
        self.classify_ori_enhance_net_flag = False

        self.generate_small_templet_flag = True

        self.get_basecase_match_pairByzsn = False
        self.get_merge_match_pair = False

        self.category       = '6193_DK7_stitch' + '_a'
        self.siftp_path     = img_path + 'point.txt'

        self.copy_partial_mask_flag = False

        self.classify_ori_partial_mask_flag = False
        self.classify_ori_pnt_img_flag = False

        self.copy_enhance_flag = False
        self.tool_path      = '/hdd/file-input/linwc/Descriptor/Test_Tool/v_1_0/enhance/6193-DK7-140-8-charge_test_base_9800/images/'

        self.copy_pnt_flag = False
        self.tool_pnt_path  = '/hdd/file-input/linwc/Descriptor/Test_Tool/v_1_0/pnt_debase/6193-DK4-130-purple-suppress-SNR28_9800/images/'

        self.pts_desc_flag  = False
        self.pts_desc_head  = img_path + '91.desc.h' # img_path + 'desc.add45.h' # img_path + 'des.h'
        self.pts_desc_outdir = img_path + 'pnt_desc_data/'
        self.pts_desc_rect_outdir = img_path + 'pnt_desc_rect_data/'

        self.trans_flag     = False # False
        self.trans_morefinger_flag = False
        self.trans_head     = img_path  + self.category.replace('_a', '') + '/' + 'trans_program.txt' # 'log.trans.0811.h' trans.txt trans_info0_TN.txt
        self.trans_head_badcase = img_path + 'Trans_Frr_' + self.category.replace('_a', '') + '.txt'
        self.trans_head_merge1_txt = img_path + 'Trans_Frr_' + self.category.replace('_a', '') + '.txt'
        self.trans_head_merge2_txt = img_path + 'Trans_Frr_' + self.category.replace('_a', '') + '_new.txt'
        self.succ_trans_outpath = img_path + self.category.replace('_a', '') + '/SIFT_Trans/SIFT_transSucc_program.csv'  
        self.orientation_poor_path = img_path + self.category.replace('_a', '') + '/' + 'orientation_poor.npy' 
        self.p_thr          = 200         # 20
        self.verify_num     = 2000       # 500

        self.pnt_img_flag   = False

    def HexStr2int32(self, hexstr):
        # print(hexstr, len(hexstr))
        assert len(hexstr) == 8
        base = 16
        res = 0
        count = 0
        greater10 = {'A': 10, 'B': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15}
        for c in hexstr:
            assert (c >= '0' and c <='9') or (c >= 'A' and c <= 'F')
            if c >= '0' and c <='9':
                midv = int(c)
            else:
                midv = greater10[c]
            if count == 0:
                res = midv
            else:
                res = res * base + midv                
            count += 1
        return res

    def get_trans_str(self, ori_message):
        name = ['ENROLL: ', 'verify: ', 'path: ', 'Trans:', 'score=', 'up=']
        out_str = ''
        for n in name:
            
            if n in ['ENROLL: ', 'verify: ']:
                out_str += n + str(ori_message[n]) + ', '
            else:
                out_str += n + ori_message[n] + ','
        return out_str[:-1]

    def get_pts_str(self, ori_str):
        name = ['x:', ',y:', ',ori:']
        out_str = ''
        for c, n in zip(ori_str.split(','), name):
            out_str += n + c
        return out_str

    def get_pts(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor)    # [-90, 90]
        pts_ori = torch.stack((pnts_x, pnts_y, ori), dim=1)   # (x, y)

        return pts_ori

    def test_process(self, FPDT):
        # 分离原图和增强图
        if self.classify_ori_enhance_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            for n1, _, n3 in os.walk(str(imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace(self.category, 'img_ori_data'), exist_ok=True)
                    os.makedirs(n1.replace(self.category, 'img_extend_data'), exist_ok=True)
                    # os.makedirs(n1.replace(self.category, 'pnt_img_data'), exist_ok=True)
                    for name in n3:
                        if '_' not in name:
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_ori_data') + '/')  
                            continue
                        elif '_desc' in name:
                            shutil.copyfile(n1 + '/' + name, n1.replace(self.category, 'img_extend_data') + '/' + name.replace('_desc', '_extend')) 
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_extend_data') + '/') 
                            # continue
                        elif '_kpt' in name:
                            # shutil.copyfile(n1 + '/' + name, n1.replace('6191_DK7-140_Mul_8_rot_p6_a', 'pnt_img_data') + '/' + name) 
                            continue
                        elif 'des.rect.txt' in name:
                            message_all = None
                            orient_all = []
                            print(n1 + '/' + name)
                            with open(n1 + '/' + name, mode='r') as pts_desc_f:
                                with open(n1 + '/' + name.replace('des','kpt'), mode='r') as pts_f: 
                                    # img_path = (n1 + '/' + name).replace('mskEnhance.bmp.des.txt', 'extend.bmp').replace('6193Test/', '6193/process/').replace(self.category, '')
                                    # pts_out_f.write(img_path + '\n')
                                    # pts_out_f.write('n=' + str(130) + '\n') 
                                    # pts_count = 0
                                    for linep, lined0 in zip(pts_f, pts_desc_f):
                                        # if 'PRINT_KEYPOINT,X,Y,ORI' in linep:
                                        pt = torch.tensor([int(p) for p in linep.split(',')[4:6]]).to(self.device)
                                        # orient = int(int(linep.split(',')[6]) / (4096 * 1.5707963273) * 90)     
                                        orient = int(linep.split(',')[6]) / (4096 * 1.570796327) * 90                                   
                                        desc0_f = torch.tensor([int(d0f) for d0f in range(128)]).to(self.device)
                                        desc0_q = torch.tensor([int(d0q) for d0q in lined0.split(',')[:4]]).to(self.device)
                                        desc45_f = torch.tensor([int(d45f) for d45f in range(128, 256, 1)]).to(self.device)
                                        desc45_q = copy.deepcopy(desc0_q)

                                        message = torch.cat((pt, desc0_f, desc45_f, desc0_q, desc45_q), dim=-1).unsqueeze(0)
                                        if message_all is None:
                                            message_all = copy.deepcopy(message)
                                        else:
                                            message_all = torch.cat((message_all, message), dim=0)
                                        orient_all.append(orient)

                                    desc_save_path = self.pts_desc_rect_outdir + '_'.join(n1.split('/')[-2:] + [name.split('_')[0]]) + '.csv'
                                    df = pd.DataFrame(message_all.detach().cpu().numpy().astype(np.int64))
                                    df.insert(loc=message_all.shape[1], column='orient', value=orient_all)
                                    df.to_csv(desc_save_path, header=None)     

                                    pts_f.close()
                                pts_desc_f.close()
                            # continue
                        elif 'des.txt' in name:
                            message_all = None
                            orient_all = []
                            print(n1 + '/' + name)
                            with open(n1 + '/' + name, mode='r') as pts_desc_f:
                                with open(n1 + '/' + name.replace('des', 'des45'), mode='r') as pts_desc_f45:
                                    with open(n1 + '/' + name.replace('des','kpt'), mode='r') as pts_f: 
                                        # img_path = (n1 + '/' + name).replace('mskEnhance.bmp.des.txt', 'extend.bmp').replace('6193Test/', '6193/process/').replace(self.category, '')
                                        # pts_out_f.write(img_path + '\n')
                                        # pts_out_f.write('n=' + str(130) + '\n') 
                                        # pts_count = 0
                                        for linep, lined0, lined45 in zip(pts_f, pts_desc_f, pts_desc_f45):
                                            # if 'PRINT_KEYPOINT,X,Y,ORI' in linep:
                                            pt = torch.tensor([int(p) for p in linep.split(',')[4:6]]).to(self.device)
                                            # orient = int(int(linep.split(',')[6]) / (4096 * 1.5707963273) * 90)     
                                            orient = int(linep.split(',')[6]) / (4096 * 1.570796327) * 90    
                                            dbvalue = int(linep.split(',')[7])                               
                                            desc0_f = torch.tensor([dbvalue] + [int(d0f + 1) for d0f in range(127)]).to(self.device)
                                            desc0_q = torch.tensor([int(d0q) for d0q in lined0.split(',')[:4]]).to(self.device)
                                            desc45_f = torch.tensor([int(d45f) for d45f in range(128, 256, 1)]).to(self.device)
                                            desc45_q = torch.tensor([int(d45q) for d45q in lined45.split(',')[:4]]).to(self.device)

                                            # print([int(d45q) for d45q in lined45.split(',')[:4]] + [orient])
                                            # exit()
                                            message = torch.cat((pt, desc0_f, desc45_f, desc0_q, desc45_q), dim=-1).unsqueeze(0)
                                            if message_all is None:
                                                message_all = copy.deepcopy(message)
                                            else:
                                                message_all = torch.cat((message_all, message), dim=0)
                                            orient_all.append(orient)
                                            # pt_str = linep.replace('\n', '').split('PRINT_KEYPOINT,X,Y,ORI,')[-1]
                                            # pts_out_f.write('id:' + str(pts_count) + '\n')
                                            # pts_out_f.write(self.get_pts_str(pt_str) + '\n')  
                                            # pts_count += 1

                                        desc_save_path = self.pts_desc_outdir + '_'.join(n1.split('/')[-2:] + [name.split('_')[0]]) + '.csv'
                                        df = pd.DataFrame(message_all.detach().cpu().numpy().astype(np.int64))
                                        df.insert(loc=message_all.shape[1], column='orient', value=orient_all)
                                        df.to_csv(desc_save_path, header=None)     

                                        pts_f.close()
                                    pts_desc_f45.close()
                                pts_desc_f.close()
                            # continue
                        else:
                            continue
            
            # 输出sift point.txt
            with open(self.siftp_path, mode='w') as pts_out_f:
                # print(self.pts_desc_outdir)
                pts_all = os.listdir(self.pts_desc_outdir)  
                pts_all.sort()  
                for pc in pts_all:
                    pc_path = self.pts_desc_outdir + pc
                    pts_ori = self.get_pts(pc_path)     # Nx3 x y ori
                    img_path = self.pts_desc_outdir.replace('pnt_desc_data', '').replace('6193Test', '6193/process9800').replace('_9800', '') + '/'.join(pc.replace('.csv', '').split('_')) + '_extend.bmp'
                    img_path = img_path.replace('//', '/')
                    pts_out_f.write(img_path + '\n')
                    pts_out_f.write('n=' + str(pts_ori.shape[0]) + '\n') 
                    for i in range(pts_ori.shape[0]):
                        pts_str = ','.join([str(int(c.item())) for c in pts_ori[i, :]])
                        pts_out_f.write('id:' + str(i) + '\n')
                        pts_out_f.write(self.get_pts_str(pts_str) + '\n')  

                pts_out_f.write('/\n')
                pts_out_f.close()  
     
            # pass

        # 分离原图和增强图 net描述子
        if self.classify_ori_enhance_net_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            for n1, _, n3 in os.walk(str(imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace(self.category, 'img_ori_data'), exist_ok=True)
                    os.makedirs(n1.replace(self.category, 'img_extend_data'), exist_ok=True)
                    # os.makedirs(n1.replace(self.category, 'pnt_img_data'), exist_ok=True)
                    for name in n3:
                        if '_' not in name:
                            shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_ori_data') + '/')  
                            # continue
                        elif '_desc' in name:
                            shutil.copyfile(n1 + '/' + name, n1.replace(self.category, 'img_extend_data') + '/' + name.replace('_desc', '_extend')) 
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_extend_data') + '/') 
                            # continue
                        elif '_kpt' in name:
                            # shutil.copyfile(n1 + '/' + name, n1.replace('6191_DK7-140_Mul_8_rot_p6_a', 'pnt_img_data') + '/' + name) 
                            continue
                        elif 'des.rect.txt' in name:
                            message_all = None
                            orient_all = []
                            print(n1 + '/' + name)
                            with open(n1 + '/' + name, mode='r') as pts_desc_f:
                                with open(n1 + '/' + name.replace('des','kpt'), mode='r') as pts_f: 
                                    # img_path = (n1 + '/' + name).replace('mskEnhance.bmp.des.txt', 'extend.bmp').replace('6193Test/', '6193/process/').replace(self.category, '')
                                    # pts_out_f.write(img_path + '\n')
                                    # pts_out_f.write('n=' + str(130) + '\n') 
                                    # pts_count = 0
                                    for linep, lined0 in zip(pts_f, pts_desc_f):
                                        # if 'PRINT_KEYPOINT,X,Y,ORI' in linep:
                                        pt = torch.tensor([int(p) for p in linep.split(',')[4:6]]).to(self.device)
                                        # orient = int(int(linep.split(',')[6]) / (4096 * 1.5707963273) * 90)     
                                        orient = int(linep.split(',')[6]) / (4096 * 1.570796327) * 90                                   
                                        desc0_f = torch.tensor([int(d0f) for d0f in range(128)]).to(self.device)
                                        desc0_q = torch.tensor([int(d0q) for d0q in lined0.split(',')[:4]]).to(self.device)
                                        desc45_f = torch.tensor([int(d45f) for d45f in range(128, 256, 1)]).to(self.device)
                                        desc45_q = copy.deepcopy(desc0_q)

                                        message = torch.cat((pt, desc0_f, desc45_f, desc0_q, desc45_q), dim=-1).unsqueeze(0)
                                        if message_all is None:
                                            message_all = copy.deepcopy(message)
                                        else:
                                            message_all = torch.cat((message_all, message), dim=0)
                                        orient_all.append(orient)

                                    desc_save_path = self.pts_desc_rect_outdir + '_'.join(n1.split('/')[-2:] + [name.split('_')[0]]) + '.csv'
                                    df = pd.DataFrame(message_all.detach().cpu().numpy().astype(np.int64))
                                    df.insert(loc=message_all.shape[1], column='orient', value=orient_all)
                                    df.to_csv(desc_save_path, header=None)     

                                    pts_f.close()
                                pts_desc_f.close()
                            # continue
                        elif 'des.txt' in name:
                            message_all = None
                            orient_all = []
                            print(n1 + '/' + name)
                            with open(n1 + '/' + name, mode='r') as pts_desc_f:
                                with open(n1 + '/' + name.replace('des','kpt'), mode='r') as pts_f: 
                                    for linep, lined0 in zip(pts_f, pts_desc_f):
                                        # if 'PRINT_KEYPOINT,X,Y,ORI' in linep:
                                        pt = torch.tensor([int(p) for p in linep.split(',')[4:6]]).to(self.device)
                                        # orient = int(int(linep.split(',')[6]) / (4096 * 1.5707963273) * 90)     
                                        orient = int(linep.split(',')[6]) / (4096 * 1.570796327) * 90    
                                        dbvalue = int(linep.split(',')[7])                               
                                        desc0_f = torch.tensor([dbvalue] + [int(d0f + 1) for d0f in range(127)]).to(self.device)
                                        desc0_q = torch.tensor([int(d0q) for d0q in lined0.split(',')[:4]]).to(self.device)
                                        desc45_f = torch.tensor([int(d45f) for d45f in range(128, 256, 1)]).to(self.device)
                                        desc45_q = torch.tensor([int(d0q) for d0q in lined0.split(',')[4:8]]).to(self.device)

                                        # print([int(d45q) for d45q in lined45.split(',')[:4]] + [orient])
                                        # exit()
                                        message = torch.cat((pt, desc0_f, desc45_f, desc0_q, desc45_q), dim=-1).unsqueeze(0)
                                        if message_all is None:
                                            message_all = copy.deepcopy(message)
                                        else:
                                            message_all = torch.cat((message_all, message), dim=0)
                                        orient_all.append(orient)
                                        # pt_str = linep.replace('\n', '').split('PRINT_KEYPOINT,X,Y,ORI,')[-1]
                                        # pts_out_f.write('id:' + str(pts_count) + '\n')
                                        # pts_out_f.write(self.get_pts_str(pt_str) + '\n')  
                                        # pts_count += 1
 
                                    if message_all is None:
                                        pts_f.close()
                                        pts_desc_f.close()
                                        continue

                                    desc_save_path = self.pts_desc_outdir + '_'.join(n1.split('/')[-2:] + [name.split('_')[0]]) + '.csv'
                                    df = pd.DataFrame(message_all.detach().cpu().numpy().astype(np.int64))
                                    df.insert(loc=message_all.shape[1], column='orient', value=orient_all)
                                    df.to_csv(desc_save_path, header=None)     

                                    pts_f.close()
                                pts_desc_f.close()
                            # continue
                        else:
                            continue
            
            # 输出sift point.txt
            with open(self.siftp_path, mode='w') as pts_out_f:
                # print(self.pts_desc_outdir)
                pts_all = os.listdir(self.pts_desc_outdir)  
                pts_all.sort()  
                for pc in pts_all:
                    pc_path = self.pts_desc_outdir + pc
                    pts_ori = self.get_pts(pc_path)     # Nx3 x y ori
                    img_path = self.pts_desc_outdir.replace('pnt_desc_data', '').replace('6193Test', '6193/process9800').replace('_9800', '') + '/'.join(pc.replace('.csv', '').split('_')) + '_extend.bmp'
                    img_path = img_path.replace('//', '/')
                    pts_out_f.write(img_path + '\n')
                    pts_out_f.write('n=' + str(pts_ori.shape[0]) + '\n') 
                    for i in range(pts_ori.shape[0]):
                        pts_str = ','.join([str(int(c.item())) for c in pts_ori[i, :]])
                        pts_out_f.write('id:' + str(i) + '\n')
                        pts_out_f.write(self.get_pts_str(pts_str) + '\n')  

                pts_out_f.write('/\n')
                pts_out_f.close()  
     
            pass


        # 从大图随机裁出小模板
        if self.generate_small_templet_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            print(imgall_dir)
            for n1, _, n3 in os.walk(str(imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace(self.category, 'img_ori_data'), exist_ok=True)
                    # os.makedirs(n1.replace(self.category, 'img_extend_data'), exist_ok=True)
                    # os.makedirs(n1.replace(self.category, 'pnt_img_data'), exist_ok=True)
                    for name in n3:
                        if '_' not in name:
                            img_big_template = load_as_float(n1 + '/' + name)
                            img_big_template_torch = torch.tensor(img_big_template)
                            # 6185和93存在镜像关系
                            img_big_template_torch = torch.flip(img_big_template_torch, dims=[1])

                            H, W = img_big_template_torch.shape[-2], img_big_template_torch.shape[-1]
                            expand_sizeH, expand_sizeW = 750, 750
                            expand_size = ((expand_sizeW - W) // 2, (expand_sizeW - W) // 2,
                                            (expand_sizeH - H) // 2, (expand_sizeH - H) // 2)  
                            img_big_template_expand = F.pad(img_big_template_torch, expand_size, "constant", 1).unsqueeze(0).unsqueeze(0) # 114 x 132 -> 500 x 500
                            
                            downsampe_ratio = 8
                            x = torch.linspace(0, W-1, W // downsampe_ratio) + downsampe_ratio // 2     
                            y = torch.linspace(0, H-1, H // downsampe_ratio) + downsampe_ratio // 2
                            mesh_points = torch.stack(torch.meshgrid([y, x])).view(2, -1).t()[:, [1, 0]].to(img_big_template_expand.device)
                            mesh_points[:, 0] += expand_size[0]     # 500 x 500
                            mesh_points[:, 1] += expand_size[2]
                            
                            # rand_angle_enhance = torch.zeros(mesh_points.shape[0], device=img_big_template_expand.device)
                            rand_angle_enhance = torch.rand(mesh_points.shape[0], device=img_big_template_expand.device) * 360 - 180 # [-180, 180)

                            def homography_centorcrop(homography, Hdev_top, Wdev_left):
                                homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32).to(homography.device)
                                scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32).to(homography.device)
                                homography = (homography - homography_dev) @ scale
                                return homography

                            lib_gallery = [
                                '6193_DK7_normal_8mul_merge',        # 39281
                                '6193_DK7_partialPress_8mul_merge',  # 13905
                                '6193_DK7_powder_8mul',              # 21266
                                '6193_DK7_random_merge_total',       # 25848      
                                '6193_DK7_rotate_8mul_merge',        # 19847
                                '6193_DK7_suppress',                 # 7172
                                '6193_DK7_wet_8mul_merge',           # 27633
                                # '6193_DK7_normal_second_part',
                                # '6193-DK7-scene_normal',
                                # '6193-DK4-wet',                      # 16614
                                # '6193-DK4-130-purer-supPress_merge', # 26036
                                # '6193-DK4-150-mul8-base',            # 22326
                            ]   
                            libary_index = random.choice(range(len(lib_gallery))) 
                            libary_prefix = '/hdd/file-input/linwc/Descriptor/data/6193/process9800'
                            trans_samples_name = libary_prefix + '/' + lib_gallery[libary_index] + '/' + 'trans_compare_frr.txt'
                            with open(trans_samples_name, mode='r') as trans_samples:
                                lines = trans_samples.readlines()
                                num_lines = len(lines)

                                trans_all_torch = None
                                for _ in range(mesh_points.shape[0]):
                                    single_trans_list = [int(h) / 256 for h in lines[random.choice(range(num_lines))].split(',Trans:')[1].split(',')[:6]] + [0, 0, 1]
                                    single_trans_torch = torch.tensor(single_trans_list).to(img_big_template_expand.device).view(-1, 3, 3)
                                    single_trans_modify = homography_centorcrop(single_trans_torch, 2, 2).unsqueeze(0)  # 122 x 36 -> 118 x 32
                                    if trans_all_torch is None:
                                        trans_all_torch = single_trans_modify
                                    else:
                                        trans_all_torch = torch.cat((trans_all_torch, single_trans_modify), dim=0)   
                                trans_samples.close()                           

                            small_templet_size = (118, 32)

                            def warp_points_batch(points, homographies, device='cpu'):
                                """
                                Warp a list of points with the given homography.

                                Arguments:
                                    points: list of N points, shape (N, 2(x, y))).
                                    homography: batched or not (shapes (B, 3, 3) and (...) respectively).

                                Returns: a Tensor of shape (N, 2) or (B, N, 2(x, y)) (depending on whether the homography
                                        is batched) containing the new coordinates of the warped points.

                                """
                                # expand points len to (x, y, 1)
                                no_batches = len(homographies.shape) == 2
                                homographies = homographies.unsqueeze(0) if no_batches else homographies
                                # homographies = homographies.unsqueeze(0) if len(homographies.shape) == 2 else homographies
                                batch_size = homographies.shape[0]
                                points = torch.cat((points.double(), torch.ones((points.shape[0], 1)).to(device)), dim=1)
                                points = points.to(device)
                                homographies = homographies.view(batch_size*3,3)
                                # warped_points = homographies*points
                                # points = points.double()
                                homographies = homographies.to(points.device)

                                warped_points = homographies@points.transpose(0,1)
                                # warped_points = np.tensordot(homographies, points.transpose(), axes=([2], [0]))
                                # normalize the points
                                warped_points = warped_points.view([batch_size, 3, -1])
                                warped_points = warped_points.transpose(2, 1)
                                warped_points = warped_points[:, :, :2] / warped_points[:, :, 2:]
                                return warped_points[0,:,:] if no_batches else warped_points

                            def get_rotation_matrix(theta):
                                batchsize = len(theta)
                                theta_r = theta*3.14159265/180
                                rotate_maxtrix = torch.zeros((batchsize, 3,3))
                                rotate_maxtrix[:,0,0] = torch.cos(theta_r)
                                rotate_maxtrix[:,0,1] = torch.sin(theta_r)
                                rotate_maxtrix[:,0,2] = 0
                                rotate_maxtrix[:,1,0] = -torch.sin(theta_r)
                                rotate_maxtrix[:,1,1] = torch.cos(theta_r)
                                rotate_maxtrix[:,1,2] = 0
                                rotate_maxtrix[:,2,0] = 0
                                rotate_maxtrix[:,2,1] = 0
                                rotate_maxtrix[:,2,2] = 1

                                return rotate_maxtrix

                            def inv_warp_patch_batch_rec(img, points, theta, sample_trans=None, patch_size=(32,8), sample_factor = 1, mode='bilinear'):
                                '''
                                Inverse warp images in batch

                                :param img:
                                    batch of images
                                    tensor [batch_size, 1, H, W]
                                :param points:
                                    batch of points
                                    tensor [batch_size, N, 2]
                                :param theta:
                                    batch of orientation [-90 +90]
                                    tensor [batch_size, N]
                                :param device:
                                    GPU device or CPU
                                :return:
                                    batch of warped images
                                    tensor [batch_size, 1, patch_size, patch_size]
                                '''
                                batch_size, _, H, W = img.shape
                                # points = points_batch.view(-1,2)
                                # theta = theta_batch.view(-1)

                                # compute inverse warped points
                                if len(img.shape) == 2 or len(img.shape) == 3:
                                    img = img.view(1,1,img.shape[0], img.shape[1])

                                if sample_trans is None:
                                    mat_homo_inv = get_rotation_matrix(theta)
                                    if len(mat_homo_inv.shape) == 2:
                                        mat_homo_inv = mat_homo_inv.view(1,3,3)
                                else:
                                    mat_homo_inv = sample_trans

                                device = img.device
                                _, channel, H, W = img.shape
                                Batch = len(points)
                                points_num = Batch // batch_size

                                patch_y = patch_size[0]*sample_factor / 2
                                patch_x = patch_size[1]*sample_factor / 2

                                coor_cells = torch.stack(torch.meshgrid(torch.linspace(-patch_x, patch_x, patch_size[1]), torch.linspace(-patch_y, patch_y, patch_size[0])), dim=2)  # 产生两个网格
                                coor_cells = coor_cells.transpose(1,0)
                                coor_cells = coor_cells.to(device)
                                coor_cells = coor_cells.contiguous()

                                src_pixel_coords = warp_points_batch(coor_cells.view([-1, 2]).double(), mat_homo_inv.double(), device)
                                src_pixel_coords = src_pixel_coords.view([Batch, patch_size[0], patch_size[1], 2])
                                src_pixel_coords = src_pixel_coords.float() + points.unsqueeze(1).unsqueeze(1).repeat(1,patch_size[0],patch_size[1],1)


                                src_pixel_coords_ofs = torch.floor(src_pixel_coords)
                                src_pixel_coords_ofs_Q11 = src_pixel_coords_ofs.view([Batch, -1, 2])

                                batch_image_coords_correct = torch.linspace(0, (batch_size-1)*H*W, batch_size).long().to(device)

                                src_pixel_coords_ofs_Q11 = (src_pixel_coords_ofs_Q11[:,:,0] + src_pixel_coords_ofs_Q11[:,:,1]*W).long()
                                src_pixel_coords_ofs_Q21 = src_pixel_coords_ofs_Q11 + 1
                                src_pixel_coords_ofs_Q12 = src_pixel_coords_ofs_Q11 + W
                                src_pixel_coords_ofs_Q22 = src_pixel_coords_ofs_Q11 + W + 1

                                warp_weight = (src_pixel_coords - src_pixel_coords_ofs).view([Batch, -1, 2])

                                alpha = warp_weight[:,:,0]
                                beta = warp_weight[:,:,1]
                                
                                # src_Q11 = img.take(src_pixel_coords_ofs_Q11).view(-1, patch_size*patch_size)
                                # src_Q21 = img.take(src_pixel_coords_ofs_Q21).view(-1, patch_size*patch_size)
                                # src_Q12 = img.take(src_pixel_coords_ofs_Q12).view(-1, patch_size*patch_size)
                                # src_Q22 = img.take(src_pixel_coords_ofs_Q22).view(-1, patch_size*patch_size)
                                src_Q11 = img.take(src_pixel_coords_ofs_Q11.view(batch_size, points_num, -1) + batch_image_coords_correct[:,None,None]).view(Batch, -1)
                                src_Q21 = img.take(src_pixel_coords_ofs_Q21.view(batch_size, points_num, -1) + batch_image_coords_correct[:,None,None]).view(Batch, -1)
                                src_Q12 = img.take(src_pixel_coords_ofs_Q12.view(batch_size, points_num, -1) + batch_image_coords_correct[:,None,None]).view(Batch, -1)
                                src_Q22 = img.take(src_pixel_coords_ofs_Q22.view(batch_size, points_num, -1) + batch_image_coords_correct[:,None,None]).view(Batch, -1)

                                warped_img = src_Q11*(1 - alpha)*(1 - beta) + src_Q21*alpha*(1 - beta) + \
                                    src_Q12*(1 - alpha)*beta + src_Q22*alpha*beta
                                warped_img = warped_img.view([Batch, patch_size[0],patch_size[1]])
                                return warped_img

                            # small_templet_img_all = inv_warp_patch_batch_rec(img_big_template_expand, mesh_points, rand_angle_enhance, None, small_templet_size, 1, 'bilinear')

                            small_templet_img_all = inv_warp_patch_batch_rec(img_big_template_expand, mesh_points, rand_angle_enhance, trans_all_torch, small_templet_size, 1, 'bilinear')

                            print(small_templet_img_all.shape)
                            
                            for b_i in range(small_templet_img_all.shape[0]):
                                small_templet_img = Image.fromarray((small_templet_img_all[b_i].squeeze().cpu().numpy() * 255).astype(np.uint8), mode='L')
                                # small_templet_img_path = n1.replace(self.category, 'img_ori_data') + '/' + name.replace('.bmp', '_' + str(b_i) + '.bmp')
                                small_templet_img_path = n1.replace(self.category, 'img_ori_data') + '/' + str(b_i).rjust(4, '0') + '.bmp'
                                small_templet_img.save(small_templet_img_path)
                            
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_ori_data') + '/')  
                            continue
                        else:
                            continue
            
            begin_index93 = 250
            noprocess_imgall_dir = imgall_dir.replace(self.category, 'noprocess/6193')
            for n1, _, n3 in os.walk(str(noprocess_imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace('noprocess/6193', 'img_ori_data'), exist_ok=True)
                    # os.makedirs(n1.replace(self.category, 'img_extend_data'), exist_ok=True)
                    # os.makedirs(n1.replace(self.category, 'pnt_img_data'), exist_ok=True)
                    for name in n3:
                        if '_' not in name:
                            name_index = int(name.replace('.bmp', '')) + begin_index93
                            shutil.copyfile(n1 + '/' + name, n1.replace('noprocess/6193', 'img_ori_data') + '/' + name.replace(name.replace('.bmp', ''), str(name_index).rjust(4 ,'0'))) 
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_extend_data') + '/') 
                            # continue

        # 找到basecase中的图像对的trans(基于trans顺序和图像下标索引一致)
        if self.get_basecase_match_pairByzsn:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            want_trans_index_all = []
            for n1, _, n3 in os.walk(str(imgall_dir)):
                # os.makedirs(n1.replace(self.category, 'pnt_img_data'), exist_ok=True)
                for name in n3:
                    if '.bmp' in name and 'exp' not in name:
                        want_trans_index = int(name.split('.')[0])
                        want_trans_index_all.append(want_trans_index)
            want_trans_index_all.sort()
            want_trans_index_all = [211, 439, 604, 983, 1489, 1519, 1855, 2048, 2295, 2340, 2521, 3030, 3351, 3420, 3560, 3645, 4289, 
                                    4355, 4879, 4957, 5062, 6143, 6544, 6985, 7136, 7433, 7499, 7594, 7705, 7801, 8907]
            print(want_trans_index_all)

            badcase_trans_txt = self.trans_head_badcase.replace('.txt', '_bad.txt')
            with open(self.trans_head_badcase, mode='r') as trans_f:
                with open(badcase_trans_txt, mode="w", encoding='utf-8') as badcase_trans_f: 
                    count = 1
                    for line in trans_f:
                        if count in want_trans_index_all:
                            trans_part = torch.tensor([float(coeff) for coeff in line.split('trans:')[-1].split(' ')] + [0, 0, 1]).reshape(3, 3)
                            trans_part_inv = torch.inverse(trans_part)[:2, :].contiguous().view(-1).tolist()

                            message ={
                                    'Trans:': ','.join([str(int(float(coeff) * 256)) for coeff in trans_part_inv]),
                                    'verify: ': int(line.split('temp:')[1].split(',samp:')[0].split('/')[-1].replace('.bmp', '')),
                                    'ENROLL: ': int(line.split('samp:')[1].split(',trans:')[0].split('/')[-1].replace('.bmp', '')),
                                    # 'ENROLL: ': int(line.split('temp:')[1].split(',samp:')[0].split('/')[-1].replace('.bmp', '')),
                                    # 'verify: ': int(line.split('samp:')[1].split(',trans:')[0].split('/')[-1].replace('.bmp', '')),
                                    'path: ': line.split('temp:')[1].split(',samp:')[0].replace('/home/zhangsn/match/000_data/mergeimg/' + self.category.replace('_a', '') + '/', imgall_dir.replace(self.category + '/', '')).replace(self.category.replace('_a', '_9800'), self.category.replace('_a', '_9800') + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '').replace('_9800', ''),
                                    'score=': '0',
                                    'up=': '1'
                                }
                            badcase_trans_f.write(self.get_trans_str(message) + '\n')  
                        count = count + 1    
                    trans_f.close()
                badcase_trans_f.close()

        # 数据集格式转换 from zsn to lwc
        if self.get_merge_match_pair:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            merge_trans_txt = self.trans_head_merge1_txt.replace('.txt', '_merge2.txt')
            with open(self.trans_head_merge2_txt, mode='r') as trans_f:
                with open(merge_trans_txt, mode="w", encoding='utf-8') as merge_trans_f: 
                    for line in trans_f:
                        trans_part = torch.tensor([float(coeff) for coeff in line.split('trans:')[-1].split(' ')] + [0, 0, 1]).reshape(3, 3)
                        # trans_part_inv = torch.inverse(trans_part)[:2, :].contiguous().view(-1).tolist()
                        trans_part = trans_part[:2, :].contiguous().view(-1).tolist()

                        message ={
                                'Trans:': ','.join([str(int(float(coeff) * 256)) for coeff in trans_part]),
                                # 'verify: ': int(line.split('temp:')[1].split(',samp:')[0].split('/')[-1].replace('.bmp', '')),
                                # 'ENROLL: ': int(line.split('samp:')[1].split(',trans:')[0].split('/')[-1].replace('.bmp', '')),
                                # 'path: ': line.split('temp:')[1].split(',samp:')[0].replace('/home/zhangsn/match/000_data/mergeimg/' + self.category.replace('_a', '').replace('_part', '') + '/', imgall_dir.replace(self.category + '/', '')).replace(self.category.replace('_a', '_9800'), self.category.replace('_a', '_9800') + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '').replace('_9800', ''),
                                'ENROLL: ': int(line.split('temp:')[1].split(',samp:')[0].split('/')[-1].replace('.bmp', '')),
                                'verify: ': int(line.split('samp:')[1].split(',trans:')[0].split('/')[-1].replace('.bmp', '')),
                                'path: ': line.split('samp:')[1].split(',trans:')[0].replace('/home/zhangsn/match/000_data/mergeimg/' + self.category.replace('_a', '').replace('_part', '') + '/', imgall_dir.replace(self.category + '/', '')).replace(self.category.replace('_a', '_9800'), self.category.replace('_a', '_9800') + '/img_extend_data').replace('.bmp', '_extend.bmp').replace('\n', '').replace('_9800', ''),
                                'score=': '0',
                                'up=': '1'
                            }
                        
                        person = int(message['path: '].split('/')[-3])
                        # 判断图像质量和湿手指mask（是否能产生点和描述）
                        pnt_desc_path_ver = imgall_dir[:-1].replace(self.category, '') + 'pnt_desc_data/' + '_'.join(message['path: '].split('/')[-3:]).replace('_extend', '').replace('.bmp', '.csv')
                        pnt_desc_path_enr = imgall_dir[:-1].replace(self.category, '') + 'pnt_desc_data/' + '_'.join(message['path: '].split('/')[-3:-1] + [str(message['ENROLL: ']).rjust(4, '0')]) + '.csv'
                        
                        if not os.path.exists(pnt_desc_path_ver) or not os.path.exists(pnt_desc_path_enr):
                            continue

                        # 模板数和人数卡控
                        # if message['ENROLL: '] >= self.temp_num or message['verify: '] < self.temp_num or person >= self.p_thr: # or message['verify: '] >= 120:
                        if person >= self.p_thr:
                        # # if message['ENROLL: '] >= self.temp_num:
                            continue
                        merge_trans_f.write(self.get_trans_str(message) + '\n')    
                    merge_trans_f.close() 
                trans_f.close()

        # 拷贝描述的enhance到工具文件下
        if self.copy_enhance_flag:
            os.makedirs(self.tool_path , exist_ok=True)
            for n1, _, n3 in os.walk(str(self.pts_desc_outdir.replace('pnt_desc_data', 'img_extend_data'))):
                if 'L' in n1 or 'R' in n1:
                    for name in n3:
                        if '_extend' in name:
                            source_path = n1 + '/' + name
                            shutil.copyfile(n1 + '/' + name, self.tool_path + '_'.join(source_path.split('/')[-3:]).replace('_extend', '')) 
            pass
        
        # 拷贝点的enhance到工具文件下
        if self.copy_pnt_flag:
            os.makedirs(self.tool_pnt_path , exist_ok=True)
            for n1, _, n3 in os.walk(str(self.pts_desc_outdir.replace('pnt_desc_data', 'img_pnt_data_540'))):   # 'img_pnt_data_540'
                if 'L' in n1 or 'R' in n1:
                    for name in n3:
                        if '_kpt' in name:
                            source_path = n1 + '/' + name
                            shutil.copyfile(n1 + '/' + name, self.tool_pnt_path + '_'.join(source_path.split('/')[-3:]).replace('_kpt', '')) 
            pass

        # 拷贝部分按压mask
        if self.copy_partial_mask_flag:
            # print(self.pts_desc_outdir.replace('pnt_desc_data', '').replace('6193Test', '6193mask/MASKNet_GT').replace('linwc/Descriptor', 'qint'))
            for n1, _, n3 in os.walk(str(self.pts_desc_outdir.replace('pnt_desc_data/', '').replace('6193Test', '6193mask/MASKNet_GT').replace('linwc/Descriptor', 'qint'))):
                if 'L' in n1 or 'R' in n1:
                    out_mask_path = n1.replace('6193mask/MASKNet_GT', '6193Test').replace('qint', 'linwc/Descriptor').replace(self.category.replace('_a', ''), self.category.replace('_a', '') +'/img_mask_data')
                    os.makedirs(out_mask_path, exist_ok=True)
                    for name in n3:
                        if '__msk1' in name:
                            source_path = n1 + '/' + name
                            shutil.copyfile(n1 + '/' + name,  out_mask_path + '/' + name.replace('__msk1', '_mask')) 
            pass

        # 分离原图和部分按压图
        if self.classify_ori_partial_mask_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            for n1, _, n3 in os.walk(str(imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace(self.category, 'img_mask_data'), exist_ok=True)
                    for name in n3:
                        if 'mask' in name:
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_mask_data') + '/')  
                            shutil.copyfile(n1 + '/' + name, n1.replace(self.category, 'img_mask_data') + '/' + '_'.join(name.split('_')[:2]) + '.bmp') 
                            # continue

        # 分离原图和点图
        if self.classify_ori_pnt_img_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            for n1, _, n3 in os.walk(str(imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace(self.category, 'img_pnt_data'), exist_ok=True)
                    for name in n3:
                        if 'kpt_base' in name:
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_mask_data') + '/')  
                            shutil.copyfile(n1 + '/' + name, n1.replace(self.category, 'img_pnt_data') + '/' + name.replace('mskEnhance.bmp_kpt_base', 'kpt')) 
                            # continue

        # 解析点和描述子
        if self.pts_desc_flag:
            with open(self.pts_desc_head, mode='r') as pts_desc_f:
                img_name_already = ['0000_L1_0000']
                message_all = None
                message = None
                flag = False
                for line in pts_desc_f:
                    if 'PRINT_KEYPOINT,X,Y,ORI' in line:
                        line.replace('\n', '')
                        pt = torch.tensor([int(p) for p in line.split(',')[4:6]]).to(self.device)
                        desc0_ori = torch.tensor([int(d) for d in line.split(',')[11:139]]).to(self.device)
                        desc0_quant_list = [self.HexStr2int32(hexstr) for hexstr in line.split(',')[7:11]]
                        orient = int(line.split(',')[6])
                        flag = True
                        continue
                    
                    if flag:
                        desc45_ori = torch.tensor([int(d) for d in line.split(',')[4:132]]).to(self.device)
                        desc45_quant_list = [self.HexStr2int32(hexstr) for hexstr in line.split(',')[0:4]]
                        desc0_quant_list.extend(desc45_quant_list)
                        desc0_quant_list.append(orient)
                        desc_quant = torch.tensor(desc0_quant_list).to(self.device)
                        message = torch.cat((pt, desc0_ori, desc45_ori, desc_quant), dim=-1).unsqueeze(0)
                        if message_all is None:
                            message_all = copy.deepcopy(message)
                        else:
                            message_all = torch.cat((message_all, message), dim=0)
                        flag = False


                    # 图像名
                    # if 'E:'in line and 'PRINT_KEYPOINT' in line:
                    if ('E:' in line and '=' not in line) or ('image =E:' in line):
                        img_name = '_'.join(line.split('/')[-3:]).replace('.bmp', '')[:-1]   # 0000_L1_0000
                        # print(img_name, img_name in img_name_already)
                        if not img_name in img_name_already:
                            desc_save_path = self.pts_desc_outdir + img_name_already[-1] + '.csv'
                            df = pd.DataFrame(message_all.detach().cpu().numpy())
                            df.to_csv(desc_save_path, header=None)
                            message_all = None
                            img_name_already.append(img_name)

                
                # 最后一张图
                desc_save_path = self.pts_desc_outdir + img_name_already[-1] + '.csv'
                df = pd.DataFrame(message_all.detach().cpu().numpy())
                df.to_csv(desc_save_path, header=None)        
                pts_desc_f.close()

        # 解析succ对的trans
        if self.trans_flag:
            trans_message_all = []
            count = 0
            ban_orientation = None
            if os.path.exists(self.orientation_poor_path):
                ban_orientation = np.load(self.orientation_poor_path, allow_pickle=True)
                
            with open(self.trans_head, mode='r') as trans_f:
                for line in trans_f:
                    if 'Trans:' in line:
                        # trans_message = [int(t) for t in line.split(':')[1].split(',')[:-1]]
                        # verify_path = line.split(':')[-1].split(',')[0]
                        # enrollverify = 'ENROLL: ' + str(int(line.split('.bmp')[0][-4:])) + ' verify: ' + str(int(line.split('.bmp')[1][-4:]))
                        # trans_message.extend([0, 0, 0, enrollverify, verify_path, 0, 0])
                        # trans_message_all.append(trans_message)
                        if (not os.path.exists(self.orientation_poor_path)) or ban_orientation[count] == 0:
                            trans_message = [int(t) for t in line.split('Trans:')[1].split('score')[0].split(',')[:-1]]
                            enrollverify = 'ENROLL: ' + line.split('ENROLL: ')[-1].split(',')[0] + ' verify: ' + line.split('verify: ')[-1].split(',')[0]
                            verify_path = line.split('path: ')[-1].split('Trans:')[0].split(',')[0]
                            verify_id = int(line.split('verify: ')[-1].split(',')[0])
                            person = int(verify_path.split('/')[-3])
                            if person < self.p_thr and verify_id < self.verify_num:                               
                                trans_message.extend([0, 0, 0, enrollverify, verify_path, 0, 0])
                                trans_message_all.append(trans_message) 
                        count += 1

   
                df_trans = pd.DataFrame(np.array(trans_message_all))
                df_trans.to_csv(self.succ_trans_outpath, header=None)        
                trans_f.close()
        
        # 解析succ对的trans成多个人的trans
        if self.trans_morefinger_flag:
            trans_message_all = []
            count = 0
            ban_orientation = None
            if os.path.exists(self.orientation_poor_path):
                ban_orientation = np.load(self.orientation_poor_path, allow_pickle=True)

            last_person = -1   
            with open(self.trans_head, mode='r') as trans_f:
                for line in trans_f:
                    if 'Trans:' in line:
                        # trans_message = [int(t) for t in line.split(':')[1].split(',')[:-1]]
                        # verify_path = line.split(':')[-1].split(',')[0]
                        # enrollverify = 'ENROLL: ' + str(int(line.split('.bmp')[0][-4:])) + ' verify: ' + str(int(line.split('.bmp')[1][-4:]))
                        # trans_message.extend([0, 0, 0, enrollverify, verify_path, 0, 0])
                        # trans_message_all.append(trans_message)
                        if (not os.path.exists(self.orientation_poor_path)) or ban_orientation[count] == 0:
                            trans_message = [int(t) for t in line.split('Trans:')[1].split('score')[0].split(',')[:-1]]
                            enrollverify = 'ENROLL: ' + line.split('ENROLL: ')[-1].split(',')[0] + ' verify: ' + line.split('verify: ')[-1].split(',')[0]
                            verify_path = line.split('path: ')[-1].split('Trans:')[0].split(',')[0]
                            verify_id = int(line.split('verify: ')[-1].split(',')[0])
                            person = int(verify_path.split('/')[-3])

                            if last_person == -1:
                                last_person = person

                            if last_person != person:
                                df_trans = pd.DataFrame(np.array(trans_message_all))
                                df_trans.to_csv(self.succ_trans_outpath.replace('.csv', str(last_person).rjust(4, '0') + '.csv'), header=None)  
                                
                                trans_message_all = []
                                last_person = person

                            if person < self.p_thr and verify_id < self.verify_num:                               
                                trans_message.extend([0, 0, 0, enrollverify, verify_path, 0, 0])
                                trans_message_all.append(trans_message) 
                                                        
                        count += 1

                # 最后一个人
                df_trans = pd.DataFrame(np.array(trans_message_all))
                df_trans.to_csv(self.succ_trans_outpath.replace('.csv', str(last_person).rjust(4, '0') + '.csv'), header=None)        
                trans_f.close()
                
        if self.pnt_img_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            for n1, _, n3 in os.walk(str(imgall_dir)):
                os.makedirs(n1.replace(self.category, 'img_pnt_data'), exist_ok=True)
                os.makedirs(n1.replace(self.category, 'pnt_desc_data_X'), exist_ok=True)
                for name in n3:  
                    print(n1 + name)          
                    if '.bmp' in name:
                        person_fp_name = '/'.join(name.split('_')[:2])
                        img_suffix_name = '/'.join(name.split('_')).replace('.bmp', '_kpt.bmp')
                        os.makedirs(n1.replace(self.category, 'img_pnt_data') + person_fp_name, exist_ok=True)
                        shutil.copyfile(n1 + name, n1.replace(self.category, 'img_pnt_data') + img_suffix_name) 
                    elif '.csv' in name:
                        label_name = ['sort', 'x', 'y']
                        df = pd.read_csv(n1 + name, names=label_name)
                        # df_ori_data = pd.read_csv(self.pts_desc_outdir + name, header=None)
                        message_all = None
                        if len(df['x'][1:].tolist()) > 0:
                            for index, (x, y) in enumerate(zip(df['x'][1:].tolist(), df['y'][1:].tolist())):
                                pntxy = torch.tensor([int(x), int(y)]).to(self.device)
                                desc0_f = torch.tensor([int(d0f) for d0f in range(128)]).to(self.device)
                                desc45_f = torch.tensor([int(d45f) for d45f in range(128, 256, 1)]).to(self.device)
                                desc0_q = torch.zeros(4).to(self.device)
                                desc45_q = torch.zeros(5).to(self.device)
                                message = torch.cat((pntxy, desc0_f, desc45_f, desc0_q, desc45_q), dim=-1).unsqueeze(0)
                                if message_all is None:
                                    message_all = copy.deepcopy(message)
                                else:
                                    message_all = torch.cat((message_all, message), dim=0)
                            df_ori_data = pd.DataFrame(message_all.detach().cpu().numpy().astype(np.int64))
                            df_ori_data.to_csv(n1.replace(self.category, 'pnt_desc_data_X') + name, header=None)     
                    else:
                        continue        
        
        pass   


class Get_Pts_Descriptor_From_HeadFA(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.classify_ori_enhance_flag = False
        self.category       = '6193_DK7_merge_test1' + '_a'
        self.siftp_path     = img_path + 'point.txt'

        self.copy_partial_mask_flag = False

        self.copy_enhance_flag = False
        self.tool_path      = '/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6_7024/' # '/hdd/file-input/linwc/Descriptor/Test_Tool/v_1_0/enhance/db_811_p10s200/images/'

        self.pts_desc_flag  = False
        self.pts_desc_head  = img_path + '91.desc.h' # img_path + 'desc.add45.h' # img_path + 'des.h'
        self.pts_desc_outdir = img_path + 'pnt_desc_data/'

        self.trans_flag     = True
        self.mode           = 'FR'          # FR FA
        self.model          = 'Net'        # Net SIFT
        self.NetName        = 'Net61000/' # 'Net26800/' ''
        self.trans_head     = img_path  + self.category.replace('_a', '') + '/' + self.model + '_Trans/' + self.mode + '/' + self.NetName + self.mode + '_trans_' + self.model + '.txt' # 'log.trans.0811.h'
        self.succ_trans_outpath = self.trans_head.replace(self.mode + '_trans_' + self.model + '.txt', self.model + '_tranSucc' + self.mode + '.csv')
        self.orientation_poor_path = img_path + self.category.replace('_a', '') + '/' + 'orientation_poor.npy' 
        self.p_thr          = 30
        self.verify_num     = 500

    def HexStr2int32(self, hexstr):
        # print(hexstr, len(hexstr))
        assert len(hexstr) == 8
        base = 16
        res = 0
        count = 0
        greater10 = {'A': 10, 'B': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15}
        for c in hexstr:
            assert (c >= '0' and c <='9') or (c >= 'A' and c <= 'F')
            if c >= '0' and c <='9':
                midv = int(c)
            else:
                midv = greater10[c]
            if count == 0:
                res = midv
            else:
                res = res * base + midv                
            count += 1
        return res

    def get_pts_str(self, ori_str):
        name = ['x:', ',y:', ',ori:']
        out_str = ''
        for c, n in zip(ori_str.split(','), name):
            out_str += n + c
        return out_str

    def get_pts(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor)    # [-90, 90]
        pts_ori = torch.stack((pnts_x, pnts_y, ori), dim=1)   # (x, y)

        return pts_ori

    def test_process(self, FPDT):
        # 分离原图和增强图
        if self.classify_ori_enhance_flag:
            imgall_dir = self.pts_desc_outdir.replace('pnt_desc_data', self.category)
            for n1, _, n3 in os.walk(str(imgall_dir)):
                if 'L' in n1 or 'R' in n1:
                    os.makedirs(n1.replace(self.category, 'img_ori_data'), exist_ok=True)
                    os.makedirs(n1.replace(self.category, 'img_extend_data'), exist_ok=True)
                    # os.makedirs(n1.replace(self.category, 'pnt_img_data'), exist_ok=True)
                    for name in n3:
                        if '_' not in name:
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_ori_data') + '/')  
                            continue
                        elif '_desc' in name:
                            # shutil.copyfile(n1 + '/' + name, n1.replace(self.category, 'img_extend_data') + '/' + name.replace('_desc', '_extend')) 
                            # shutil.move(n1 + '/' + name, n1.replace(self.category, 'img_extend_data') + '/') 
                            continue
                        elif '_kpt' in name:
                            # shutil.copyfile(n1 + '/' + name, n1.replace('6191_DK7-140_Mul_8_rot_p6_a', 'pnt_img_data') + '/' + name) 
                            continue
                        elif 'des.txt' in name:
                            message_all = None
                            print(n1 + '/' + name)
                            with open(n1 + '/' + name, mode='r') as pts_desc_f:
                                with open(n1 + '/' + name.replace('des', 'des45'), mode='r') as pts_desc_f45:
                                    with open(n1 + '/' + name.replace('des','kpt'), mode='r') as pts_f: 
                                        # img_path = (n1 + '/' + name).replace('mskEnhance.bmp.des.txt', 'extend.bmp').replace('6193Test/', '6193/process/').replace(self.category, '')
                                        # pts_out_f.write(img_path + '\n')
                                        # pts_out_f.write('n=' + str(130) + '\n') 
                                        # pts_count = 0
                                        for linep, lined0, lined45 in zip(pts_f, pts_desc_f, pts_desc_f45):
                                            # if 'PRINT_KEYPOINT,X,Y,ORI' in linep:
                                            pt = torch.tensor([int(p) for p in linep.split(',')[4:6]]).to(self.device)
                                            orient = int(int(linep.split(',')[6]) / (4096 * 1.5707963273) * 90)
                                            desc0_f = torch.tensor([int(d0f) for d0f in range(128)]).to(self.device)
                                            desc0_q = torch.tensor([int(d0q) for d0q in lined0.split(',')[:4]]).to(self.device)
                                            desc45_f = torch.tensor([int(d45f) for d45f in range(128, 256, 1)]).to(self.device)
                                            desc45_q = torch.tensor([int(d45q) for d45q in lined45.split(',')[:4]] + [orient]).to(self.device)
                                            message = torch.cat((pt, desc0_f, desc45_f, desc0_q, desc45_q), dim=-1).unsqueeze(0)
                                            if message_all is None:
                                                message_all = copy.deepcopy(message)
                                            else:
                                                message_all = torch.cat((message_all, message), dim=0)
                                            
                                            # pt_str = linep.replace('\n', '').split('PRINT_KEYPOINT,X,Y,ORI,')[-1]
                                            # pts_out_f.write('id:' + str(pts_count) + '\n')
                                            # pts_out_f.write(self.get_pts_str(pt_str) + '\n')  
                                            # pts_count += 1

                                        desc_save_path = self.pts_desc_outdir + '_'.join(n1.split('/')[-2:] + [name.split('_')[0]]) + '.csv'
                                        df = pd.DataFrame(message_all.detach().cpu().numpy())
                                        df.to_csv(desc_save_path, header=None)      

                                        pts_f.close()
                                    pts_desc_f45.close()
                                pts_desc_f.close()
                            # continue
                        else:
                            continue

            with open(self.siftp_path, mode='w') as pts_out_f:
                # print(self.pts_desc_outdir)
                pts_all = os.listdir(self.pts_desc_outdir)  
                pts_all.sort()  
                for pc in pts_all:
                    pc_path = self.pts_desc_outdir + pc
                    pts_ori = self.get_pts(pc_path)     # Nx3 x y ori
                    img_path = self.pts_desc_outdir.replace('pnt_desc_data', '').replace('6193Test', '6193/process') + '/'.join(pc.replace('.csv', '').split('_')) + '_extend.bmp'
                    img_path = img_path.replace('//', '/')
                    pts_out_f.write(img_path + '\n')
                    pts_out_f.write('n=' + str(pts_ori.shape[0]) + '\n') 
                    for i in range(pts_ori.shape[0]):
                        pts_str = ','.join([str(int(c.item())) for c in pts_ori[i, :]])
                        pts_out_f.write('id:' + str(i) + '\n')
                        pts_out_f.write(self.get_pts_str(pts_str) + '\n')  

                pts_out_f.write('/\n')
                pts_out_f.close()  
     
            pass

        # 拷贝enhance到工具文件下
        if self.copy_enhance_flag:
            for n1, _, n3 in os.walk(str(self.pts_desc_outdir.replace('pnt_desc_data', 'img_enhance_data'))):
                if 'L' in n1 or 'R' in n1:
                    for name in n3:
                        if 'mskEnhance' in name:
                            source_path = n1 + '/' + name
                            shutil.copyfile(n1 + '/' + name, self.tool_path + '_'.join(source_path.split('/')[-3:]).replace('_mskEnhance', '')) 
            pass

        # 拷贝部分按压mask
        if self.copy_partial_mask_flag:
            # print(self.pts_desc_outdir.replace('pnt_desc_data', '').replace('6193Test', '6193mask/MASKNet_GT').replace('linwc/Descriptor', 'qint'))
            for n1, _, n3 in os.walk(str(self.pts_desc_outdir.replace('pnt_desc_data/', '').replace('6193Test', '6193mask/MASKNet_GT').replace('linwc/Descriptor', 'qint'))):
                if 'L' in n1 or 'R' in n1:
                    out_mask_path = n1.replace('6193mask/MASKNet_GT', '6193Test').replace('qint', 'linwc/Descriptor').replace(self.category.replace('_a', ''), self.category.replace('_a', '') +'/img_mask_data')
                    os.makedirs(out_mask_path, exist_ok=True)
                    for name in n3:
                        if '__msk1' in name:
                            source_path = n1 + '/' + name
                            shutil.copyfile(n1 + '/' + name,  out_mask_path + '/' + name.replace('__msk1', '_mask')) 
            pass

        # 解析点和描述子
        if self.pts_desc_flag:
            with open(self.pts_desc_head, mode='r') as pts_desc_f:
                img_name_already = ['0000_L1_0000']
                message_all = None
                message = None
                flag = False
                for line in pts_desc_f:
                    if 'PRINT_KEYPOINT,X,Y,ORI' in line:
                        line.replace('\n', '')
                        pt = torch.tensor([int(p) for p in line.split(',')[4:6]]).to(self.device)
                        desc0_ori = torch.tensor([int(d) for d in line.split(',')[11:139]]).to(self.device)
                        desc0_quant_list = [self.HexStr2int32(hexstr) for hexstr in line.split(',')[7:11]]
                        orient = int(line.split(',')[6])
                        flag = True
                        continue
                    
                    if flag:
                        desc45_ori = torch.tensor([int(d) for d in line.split(',')[4:132]]).to(self.device)
                        desc45_quant_list = [self.HexStr2int32(hexstr) for hexstr in line.split(',')[0:4]]
                        desc0_quant_list.extend(desc45_quant_list)
                        desc0_quant_list.append(orient)
                        desc_quant = torch.tensor(desc0_quant_list).to(self.device)
                        message = torch.cat((pt, desc0_ori, desc45_ori, desc_quant), dim=-1).unsqueeze(0)
                        if message_all is None:
                            message_all = copy.deepcopy(message)
                        else:
                            message_all = torch.cat((message_all, message), dim=0)
                        flag = False


                    # 图像名
                    # if 'E:'in line and 'PRINT_KEYPOINT' in line:
                    if ('E:' in line and '=' not in line) or ('image =E:' in line):
                        img_name = '_'.join(line.split('/')[-3:]).replace('.bmp', '')[:-1]   # 0000_L1_0000
                        # print(img_name, img_name in img_name_already)
                        if not img_name in img_name_already:
                            desc_save_path = self.pts_desc_outdir + img_name_already[-1] + '.csv'
                            df = pd.DataFrame(message_all.detach().cpu().numpy())
                            df.to_csv(desc_save_path, header=None)
                            message_all = None
                            img_name_already.append(img_name)

                
                # 最后一张图
                desc_save_path = self.pts_desc_outdir + img_name_already[-1] + '.csv'
                df = pd.DataFrame(message_all.detach().cpu().numpy())
                df.to_csv(desc_save_path, header=None)        
                pts_desc_f.close()

        # 解析succ对的trans
        if self.trans_flag:
            trans_message_all = []
            count = 0
            # ban_orientation = np.load(self.orientation_poor_path, allow_pickle=True)
            with open(self.trans_head, mode='r') as trans_f:
                for line in trans_f:
                    if 'Trans:' in line:
                        # trans_message = [int(t) for t in line.split(':')[1].split(',')[:-1]]
                        # verify_path = line.split(':')[-1].split(',')[0]
                        # enrollverify = 'ENROLL: ' + str(int(line.split('.bmp')[0][-4:])) + ' verify: ' + str(int(line.split('.bmp')[1][-4:]))
                        # trans_message.extend([0, 0, 0, enrollverify, verify_path, 0, 0])
                        # trans_message_all.append(trans_message)
                        # if ban_orientation[count] == 0:
                        trans_message = [int(t) for t in line.split('Trans:')[1].split('score')[0].split(',')[:-1]]
                        enrollverify = 'ENROLL: ' + line.split('ENROLL: ')[-1].split(' verify:')[0].replace(',', '') + ' verify: ' + line.split('verify: ')[-1].split(',')[0]
                        verify_path = line.split('path: ')[-1].split('Trans:')[0].split(',')[0]
                        verify_id = int(line.split('verify: ')[-1].split(',')[0])
                        score = int(line.split('score=')[-1].split(',')[0])
                        up = int(line.split('up=')[-1].replace('\n', ''))
                        person = int(verify_path.split('/')[-3])
                        if person < self.p_thr and verify_id < self.verify_num:                               
                            trans_message.extend([0, 0, 0, enrollverify, verify_path, score, up])
                            trans_message_all.append(trans_message) 
                        count += 1

   
                df_trans = pd.DataFrame(np.array(trans_message_all))
                df_trans.to_csv(self.succ_trans_outpath, header=None)        
                trans_f.close()
                
        pass   



class Csv_Add_Orientation(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):
        self.device         = device
        self.config         = config
        self.output_dir     = config['output_dir']
        self.point_path     = '/hdd/file-input/linwc/Descriptor/data/6191/process/6191_DK7-140_Mul_8_rot_p6/point.txt'
        self.ori_csv_root   = '/hdd/file-input/linwc/Descriptor/data/6191_DK7-140_Mul_8_rot_p6/pnt_desc_data/'

    def write_csv(self, path, data_row):
        with open(path,"r") as f_ori:
            csv_read = csv.reader(f_ori)
            count = 0
            outpath = path.replace('pnt_desc_data', 'pnt_desc_data_new')
            with open(outpath,'a+') as f_out:
                for line in csv_read:
                    csv_write = csv.writer(f_out)
                    line[-1] = data_row[count]
                    csv_write.writerow(line)
                    count += 1

    def test_process(self, FPDT):
        # ------ images points read ------
        coor = []
        coor_group = []
        name = ''
        # img_6159_path = []
        df_p = pd.read_csv(self.point_path, header=None, encoding="gb2312", sep='\t')    # 'gb2312'

        for line in zip(df_p[0]):  # line: [index, .]
            if (line[0][0] == '/') and (len(coor) != 0):
                oripath = self.ori_csv_root + name
                self.write_csv(oripath, coor)
                coor = []
                
            if line[0][0:4] == '/hdd':
                '''换成npy格式(DOG数据)'''
                # tmp = line[0]
                # tmp = tmp.replace('bmp', 'npy')
                # tmp = tmp.replace('6159_cd_p11s400', '6159_cd_p11s400_DOG')
                # img_6159_path.append(tmp[1:])
                name = '_'.join(line[0][0:].replace('_enhance.bmp', '.csv').split('/')[-3:])
                # img_6159_path.append(line[0][0:])

            if line[0][0] == 'x':
                ls = re.split('[,:]', line[0])
                # coor.append(ls[1])    # 提取x,y, ori
                # coor.append(ls[3])
                coor.append(ls[5])
                                              
        pass        


class Plot_Spoof_Match_Img(object):
    def __init__(self, img_path=None, info_path=None, device="cpu", **config):

        self.device         = device
        self.top_k          = config['top_k']
        self.top_k_net      = 130
        self.sizer          = config['resize']
        self.output_ratio   = config['output_ratio']
        self.output_images  = config['output_images']
        self.conf_thresh    = 0.01 # config['detec_thre']
        self.nms            = 1 # config['nms']
        self.bord           = 2

        self.isDilation     = config['isDilation']  # True: 扩充36->40  False: 裁剪36->32

        self.output_dir     = Path(config['output_dir']) / 'val'
        self.sift_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        # self.net_index     = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        self.net_index      = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()      
        self.thr_mode       = True
        self.th1            = 60
        self.th2            = 110
        self.isrot          = True
        self.isrot91        = True

        self.isdense        = False # False
        self.has45          = False # True

        self.partial_filter = True
        self.use_netori     = False

        self.is_rec         = False
        self.cat_sift       = False
        self.test_sift128   = False

        self.slope          = 5000
        self.remove_border_w = 0
        self.remove_border_h = 0
        self.test_netp_flag  = True
        self.test_netdesc_flag = False # True
        self.siftq           = True

        self.sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193-DK4-130-purple-tickle-fix_9800/SIFT_Trans/SIFT_transSucc_TN.csv')
        # self.test_netp_flag = False
        os.makedirs(self.output_dir, exist_ok=True)

        self.transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((self.sizer[0], self.sizer[1])),
        ])

        # print("load dataset_files from: ", img_path)
        
        # 6193_DK7_merge_test1_9800
        # 6193_DK7_XA_rot_test_9800
        # 6193_DK7_merge_test2_9800
        # 6193-DK7-160-8-normal-nocontrol_test_9800
        
        if not self.isrot:
            # ------ matched images ------
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/repeat/6159_p21s600_data/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv(sift_succ_path, 'SIFT_Point_6159_p21s600')
        elif self.isrot91:
            # sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6193Test/6193-DK4-110-tarnish-normal-fix-SNR80_9800/SIFT_Trans/SIFT_transSucc_TN.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(self.sift_succ_path, 'pnt_desc_data')
        else:
            sift_succ_path = Path('/hdd/file-input/linwc/Descriptor/data/6159_rot2_p10s300/SIFT_Trans/SIFT_transSucc.csv')
            self.sift_succ_dict = self.Decode_Succ_Fail_Csv_rot(sift_succ_path, 'pnt_desc_data')

        pass

    def Decode_Succ_Fail_Csv(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / '6159_p21s600'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['_'.join(e.split('/')[-3:]).replace('bmp', 'csv') for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('_' + str(e_v[1]), '_' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.csv', '_enhance.csv')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name,
            'enroll_name': tmp_enroll_name,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):
            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def Decode_Succ_Fail_Csv_rot(self, path, pts_desc_dir):
        # pts_desc_dir = 'NET_Point_6159_p21s600' if (file_name == 'Net_Succ' or file_name == 'Net_Fail') else 'SIFT_Point_6159_p21s600'
        img_path = Path('/'.join(str(path).split('/')[:-2])) / 'img_extend_data'
        pts_path = Path('/'.join(str(path).split('/')[:-2])) / str(pts_desc_dir)

        namelist = ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'score','deformation0', 'deformation1', 'enrollverify', 'path', 'score_flag', 'up']
        df = pd.read_csv(path, names=namelist)

        H0, H1, H2 = df['h0'].to_list(), df['h1'].to_list(), df['h2'].to_list()
        H3, H4, H5 = df['h3'].to_list(), df['h4'].to_list(), df['h5'].to_list()
        homos = [[H0[i], H1[i], H2[i], H3[i], H4[i], H5[i]] for i in range(len(H0))]

        enr_vefy = df['enrollverify'].to_list()
        # for e in enr_vefy:
        #     print(e.split(' '))

        enr_vefy = [[e.split(' ')[1].rjust(4, '0'), e.split(' ')[3].rjust(4, '0')] for e in enr_vefy]   # e.g. '3'->'0003'  ['0003', '0015']
        # enroll, verify = enr_vefy[:, 0], enr_vefy[:, 1]

        tmp_path = df['path'].to_list()
        tmp_verify_name = ['/'.join(e.replace('_extend', '').split('/')[-3:]) for e in tmp_path]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name = [vef_n.replace('/' + str(e_v[1]), '/' + str(e_v[0])) for (e_v, vef_n) in zip(enr_vefy, tmp_verify_name)]
        # tmp_img_name = []
        img_verify_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_verify_name]
        img_enroll_file = [Path(img_path, e.replace('.bmp', '_extend.bmp')) for e in tmp_enroll_name]
        pts_verify_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_verify_name]
        pts_enroll_file = [Path(pts_path, e.replace('/', '_').replace('.bmp', '.csv')) for e in tmp_enroll_name]

        tmp_n = re.split('[/.]', str(path))[-2]     # re
        NorS, SorF = [tmp_n.split('_')[0]], [tmp_n.split('_')[-1][-4:]]
        NorS, SorF = list(NorS) * len(homos), list(SorF) * len(homos)       # flag: Net/Sift, Succ/Fail 
        tmp_verify_name_short = [e.replace('/', '_') for e in tmp_verify_name]  # 0002/L1/0013.bmp -> 0002_L1_0013.csv
        tmp_enroll_name_short = [e.replace('/', '_') for e in tmp_enroll_name]
        files = {
            'img_verify': img_verify_file,
            'img_enroll': img_enroll_file,
            'pts_verify': pts_verify_file,
            'pts_enroll': pts_enroll_file,
            'verify_name': tmp_verify_name_short,
            'enroll_name': tmp_enroll_name_short,
            'homos': homos,
            'NorS': NorS,
            'SorF': SorF
        }

        sequence_set = []
        for (img_vf, img_en, pts_vf, pts_en, name_vf, name_en, homo, NorS, SorF) in zip(
            files['img_verify'],
            files['img_enroll'],
            files['pts_verify'],    # including (x, y) & desc in the file
            files['pts_enroll'],
            files['verify_name'],
            files['enroll_name'],
            files['homos'],         # size (2, 3)
            files['NorS'],
            files['SorF']
            ):

            # 判断是否被湿手指mask和图像质量卡控：
            if os.path.exists(pts_vf) == False or os.path.exists(pts_en) == False:
                continue

            sample = {'img_vf': img_vf, 'img_en': img_en, 'pts_vf': pts_vf, 'pts_en': pts_en, 'name_vf': name_vf, 'name_en': name_en, 'homo': homo, 'NorS': NorS, 'SorF': SorF}
            sequence_set.append(sample)

        return sequence_set

    def get_pts_desc_rot(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        # desc_back   = torch.tensor(mat_info[:, 131:]).type(torch.FloatTensor) / 1e4

        return pts, desc_front
    
    def get_pts_desc_rot91(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        desc_front  = torch.tensor(mat_info[:, 3:131]).type(torch.FloatTensor) 
        desc_back   = torch.tensor(mat_info[:, 131:259]).type(torch.FloatTensor)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori

    def get_pts_desc_rot91_siftq(self, path):
        # name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
        df = pd.read_csv(path, header=None)
        mat_info = df.to_numpy()
        pnts_x = torch.tensor(mat_info[:, 1]).type(torch.FloatTensor)
        pnts_y = torch.tensor(mat_info[:, 2]).type(torch.FloatTensor)
        pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)

        # desc_front  = torch.tensor(mat_info[:, 259:263]) 
        # desc_back   = torch.tensor(mat_info[:, 263:267])
        
        desc1, desc5 = mat_info[:, -9].astype(np.int64), mat_info[:, -5].astype(np.int64)
        desc2, desc6 = mat_info[:, -8].astype(np.int64), mat_info[:, -4].astype(np.int64)
        desc3, desc7 = mat_info[:, -7].astype(np.int64), mat_info[:, -3].astype(np.int64)
        desc4, desc8 = mat_info[:, -6].astype(np.int64), mat_info[:, -2].astype(np.int64)

        desc1 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc1]).type(torch.FloatTensor)
        desc2 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc2]).type(torch.FloatTensor)
        desc3 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc3]).type(torch.FloatTensor)
        desc4 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc4]).type(torch.FloatTensor)
        desc5 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc5]).type(torch.FloatTensor)
        desc6 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc6]).type(torch.FloatTensor)
        desc7 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc7]).type(torch.FloatTensor)
        desc8 = torch.tensor([[(e >> n) & 0x1 for n in (range(32))] for e in desc8]).type(torch.FloatTensor)
        
        desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
        desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

        ori = torch.tensor(mat_info[:, -1]).type(torch.FloatTensor) - 90    # [-90, 90]

        return pts, desc_front, desc_back, ori


    def Extract_Csv(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        def pts_desc_decode(path):
            name = ['sort', 'x', 'y', 'desc1', 'desc2', 'desc3', 'desc4', 'desc5', 'desc6', 'desc7', 'desc8']
            df = pd.read_csv(path, names=name)
            pnts_x = torch.tensor(df['x'].to_list()).type(torch.FloatTensor)
            pnts_y = torch.tensor(df['y'].to_list()).type(torch.FloatTensor)
            pts = torch.stack((pnts_x, pnts_y), dim=1)   # (x, y)
            mask_pts = (pts[:, 0] >= 2) * (pts[:, 0] <= 33)
            pts = pts[mask_pts]     # 点在136*36范围
            # if self.isDilation:
            #     pts[:, 0] += 2    # 36->40  add 2 pixel
            # else:
            #     pts[:, 0] -= 2    # 36->32  cut 2 pixel
            desc1, desc5 = df['desc1'].to_list(), df['desc5'].to_list()
            desc2, desc6 = df['desc2'].to_list(), df['desc6'].to_list()
            desc3, desc7 = df['desc3'].to_list(), df['desc7'].to_list()
            desc4, desc8 = df['desc4'].to_list(), df['desc8'].to_list()
            desc1 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc1]).type(torch.FloatTensor)
            desc2 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc2]).type(torch.FloatTensor)
            desc3 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc3]).type(torch.FloatTensor)
            desc4 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc4]).type(torch.FloatTensor)
            desc5 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc5]).type(torch.FloatTensor)
            desc6 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc6]).type(torch.FloatTensor)
            desc7 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc7]).type(torch.FloatTensor)
            desc8 = torch.tensor([[(e >> n) & 0x1 for n in reversed(range(32))] for e in desc8]).type(torch.FloatTensor)

            desc_front  = torch.cat((desc1, desc2, desc3, desc4), dim=-1)
            desc_back   = torch.cat((desc5, desc6, desc7, desc8), dim=-1)

            return pts, desc_front[mask_pts], desc_back[mask_pts]

        '''#### img A(verify) & B(enroll) ####'''
        img = pd.read_csv(samples[index]['img_vf'], header=None).to_numpy()
        img_o_A = img.astype(np.float32) / 255
        img = pd.read_csv(samples[index]['img_en'], header=None).to_numpy()
        img_o_B = img.astype(np.float32) / 255
        img_o_A, img_o_B = img_o_A[:, :36], img_o_B[:, :36]

        img_o_A = self.transforms(img_o_A)
        img_o_B = self.transforms(img_o_B)
        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = imgA.shape[1], imgA.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf = pts_desc_decode(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en = pts_desc_decode(samples[index]['pts_en'])
        
        '''#### homo ####'''
        homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
        homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
        vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
        homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)

        '''#### imgA warped ####'''
        imgA_warped = inv_warp_image(imgA.squeeze(), homo)
        imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
        imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo)

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'homo': homo})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask, 'imgA_o_warped': imgA_o_warped})        # 非模板按照csv中trans进行的旋转
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = F.pad(img_o_A, (2, 2), 'constant')     # 36 -> 40    constant/reflect
            imgB = F.pad(img_o_B, (2, 2), 'constant')     # 36 -> 40
        else:
            imgA = img_o_A[:, 12:-12, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, 12:-12, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf = self.get_pts_desc_rot(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en = self.get_pts_desc_rot(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_front_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_front_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0)     # 144x52 -> 136x40
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgA, 'imgB': imgB, 'img_o_A': img_o_A, 'img_o_B': img_o_B})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_vf, 'pts_en': pts_en})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_vf, 'desc_back_vf': desc_back_vf})
        input.update({'desc_front_en': desc_front_en, 'desc_back_en': desc_back_en})
        input.update({'ori_vf': ori_vf, 'ori_en': ori_en})
        input.update({'homo': homo, 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_vf'], 'name_en': samples[index]['name_en']})

        return input

    def Extract_Succ_Csv_rot91_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 4:-4, 6:-6].unsqueeze(0)
            imgB = img_o_B[0, 4:-4, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 
        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_pnt(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_pnt_ori = load_as_float(str(samples[index]['img_vf']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))
        img_pnt_o_A = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_pnt_ori = load_as_float(str(samples[index]['img_en']).replace('img_extend_data', 'img_pnt_data').replace('extend', 'kpt'))   
        img_pnt_o_B = torch.tensor(img_pnt_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (2, 2, 3, 3)     
            imgA_pnt = F.pad(img_pnt_o_A, pad_size, "constant", 0)    # 122 x 36 -> 128 x 40 
            imgB_pnt = F.pad(img_pnt_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_pnt': imgB_pnt, 'imgB_pnt': imgA_pnt, 'img_pnt_o_A': img_pnt_o_B, 'img_pnt_o_B': img_pnt_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input

    def Extract_Succ_Csv_rot91_siftq_t2s_base(self, index, samples):
        # from Model_component import sample_homography_cv
        # from Model_component import imgPhotometric
        

        '''#### img A(verify) & B(enroll) ####'''
        img_ori = load_as_float(samples[index]['img_vf'])
        img_o_A = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)
        '''#### img A(verify) & B(enroll) ####'''
        img_base_ori = load_as_float(str(samples[index]['img_vf']).replace('img_extend_data', 'img_ori_data').replace('_extend', ''))
        img_base_o_A = torch.tensor(img_base_ori, dtype=torch.float32).unsqueeze(0)

        '''传统增强扩边处理-逆：36->32'''  
        # if self.isDilation:
        #     import torch.nn.functional as F
        #     img_aug = F.pad(img_ori, (2, 2), 'constant')     # 36 -> 40
        # else:
        #     img_aug = img_ori[:, 2:-2]      # [1, 136, 32]

        img_ori = load_as_float(samples[index]['img_en'])   
        img_o_B = torch.tensor(img_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        img_base_ori = load_as_float(str(samples[index]['img_en']).replace('img_extend_data', 'img_ori_data').replace('_extend', ''))   
        img_base_o_B = torch.tensor(img_base_ori, dtype=torch.float32).unsqueeze(0)       # 128 x 52

        if self.isDilation:
            import torch.nn.functional as F
            imgA = img_o_A[0, 3:-3, 6:-6].unsqueeze(0)      # 128 x 52 -> 122 x 40
            imgB = img_o_B[0, 3:-3, 6:-6].unsqueeze(0) 

            pad_size = (0, 0, 0, 0)     
            imgA_base = F.pad(img_base_o_A, pad_size, "constant", 0)    # 118 x 32 -> 118 x 32
            imgB_base = F.pad(img_base_o_B, pad_size, "constant", 0)

        else:
            imgA = img_o_A[:, :, 2:-2]      # [1, 136, 32] 传统增强扩边处理-逆：36->32
            imgB = img_o_B[:, :, 2:-2]      # [1, 136, 32]
        H, W = img_o_A.shape[1], img_o_A.shape[2]

        # load mask
        imgA_mask_path = str(samples[index]['img_vf']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgB_mask_path = str(samples[index]['img_en']).replace('_9704', '').replace('img_extend_data', 'img_mask_data').replace('_extend', '_mask')
        imgA_mask, imgB_mask = None, None
        if os.path.exists(imgA_mask_path) and os.path.exists(imgB_mask_path):
            imgA_mask = load_as_float(imgA_mask_path)
            imgA_mask = torch.tensor(imgA_mask, dtype=torch.float32).unsqueeze(0)
            imgB_mask = load_as_float(imgB_mask_path) 
            imgB_mask = torch.tensor(imgB_mask, dtype=torch.float32).unsqueeze(0)

        '''#### verify/enroll pts & desc ####'''
        pts_vf, desc_front_vf, desc_back_vf, ori_vf = self.get_pts_desc_rot91_siftq(samples[index]['pts_vf'])     # (x, y), [n, 128], [n, 128]
        pts_en, desc_front_en, desc_back_en, ori_en = self.get_pts_desc_rot91_siftq(samples[index]['pts_en'])
        
        '''#### homo ####'''
        if samples[index]['homo'] is not None:
            homo = torch.tensor(samples[index]['homo']).type(torch.FloatTensor)
            homo = 2 * homo / 512.  # csv中的数据需要乘上2再除以512
            vec_one = torch.tensor([[0, 0, 1]], dtype=torch.float32)
            homo = torch.cat((homo.reshape(2, 3), vec_one), dim=0)
            # if self.isDilation:
            #     homo_fixed = None
            #     # homo_fixed = homograghy_transform(homo, 0, 2) # 尺寸修改后对应的trans也要变
            # else:
            #     homo_fixed = None
                # homo_fixed = homograghy_transform(homo, 0, -2)

            # '''#### imgA warped 裁剪(扩充)图和原图####'''
            # imgA_warped = inv_warp_image(imgA.squeeze(), homo_fixed)
            # imgA_warped_mask = compute_valid_mask(torch.tensor([H, W]), inv_homography=homo_fixed)
            # imgA_o_warped = inv_warp_image(img_o_A.squeeze(), homo)
            # imgA_o_warped_mask = compute_valid_mask(torch.tensor([img_o_A.shape[1], img_o_A.shape[2]]), inv_homography=homo)
        else:
            homo = None
        homo_fixed = None
        imgA_warped = None
        imgA_warped_mask = None
        imgA_o_warped = None
        imgA_o_warped_mask = None

        input  = {}
        input.update({'imgA': imgB, 'imgB': imgA, 'img_o_A': img_o_B, 'img_o_B': img_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'imgA_base': imgB_base, 'imgB_base': imgA_base, 'img_base_o_A': img_base_o_B, 'img_base_o_B': img_base_o_A})  # imgA: 非原图尺寸  img_o_A: 原图尺寸136*36
        input.update({'pts_vf': pts_en, 'pts_en': pts_vf})
        input.update({'pts_ori_vf': None, 'pts_ori_en': None})  # 原始坐标
        input.update({'desc_front_vf': desc_front_en, 'desc_back_vf': desc_back_en})
        input.update({'desc_front_en': desc_front_vf, 'desc_back_en': desc_back_vf})
        input.update({'ori_vf': ori_en, 'ori_en': ori_vf})
        input.update({'homo': torch.inverse(homo), 'homo_fixed': homo_fixed})
        input.update({'imgA_warped': imgA_warped, 'imgA_warped_mask': imgA_warped_mask})            # 非模板按照csv中trans进行的旋转
        input.update({'imgA_o_warped': imgA_o_warped, 'imgA_o_warped_mask': imgA_o_warped_mask})        
        input.update({'name_vf': samples[index]['name_en'], 'name_en': samples[index]['name_vf']})
        input.update({'imgA_mask': imgB_mask, 'imgB_mask': imgA_mask})

        return input


    def get_des_hanmingdis_wht(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))
        hanming_dist = wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis(self, des_a, des_b):
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        hanming_dist = des_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist

    def get_des_hanmingdis_permute(self, des_a, des_b, th1=60, th2=110):
        # # 固定thr1后计算thr2
        desc_binary_a = torch.where(des_a > 0, torch.ones_like(des_a), torch.zeros_like(des_a))    # 256维
        desc_binary_b = torch.where(des_b > 0, torch.ones_like(des_b), torch.zeros_like(des_b))
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        # index = (((torch.linspace(32,1,32)**8).unsqueeze(0) @ hadamard(32)) == ((torch.linspace(1,32,32)**8).unsqueeze(0) @ hadamard(32))).long().squeeze(0).unsqueeze(1).repeat(1,4).view(-1).bool()
        desc_binary_a_begin = torch.cat((desc_binary_a[:, :128][:, self.sift_index], desc_binary_a[:, 128:][:, self.sift_index]), dim=-1)  # 128维
        desc_binary_b_begin = torch.cat((desc_binary_b[:, :128][:, self.sift_index], desc_binary_b[:, 128:][:, self.sift_index]), dim=-1)
        desc_binary_a_end = torch.cat((desc_binary_a[:, :128][:, self.sift_index==False], desc_binary_a[:, 128:][:, self.sift_index==False]), dim=-1)  # 128维
        desc_binary_b_end = torch.cat((desc_binary_b[:, :128][:, self.sift_index==False], desc_binary_b[:, 128:][:, self.sift_index==False]), dim=-1)
        hanming_dist_begin = des_a.shape[1] // 2 - desc_binary_a_begin @ desc_binary_b_begin.t() - (1 - desc_binary_a_begin) @ (1 - desc_binary_b_begin.t())   # 0-128
        hanming_dist_end = des_a.shape[1] // 2 - desc_binary_a_end @ desc_binary_b_end.t() - (1 - desc_binary_a_end) @ (1 - desc_binary_b_end.t())
        hanming_dist_end_min = torch.where(hanming_dist_end >= 64, 128 - hanming_dist_end, hanming_dist_end)
        hanming_dist = hanming_dist_begin + hanming_dist_end_min
        hanming_dist_nothr = copy.deepcopy(hanming_dist)
        mask1 = hanming_dist_begin < th1
        mask2 = hanming_dist < th2
        hanming_dist[mask1 == False] = des_a.shape[-1] + 1    # 不满足阈值的对距离赋值为257
        hanming_dist[mask2 == False] = des_a.shape[-1] + 1
        return hanming_dist, hanming_dist_nothr

    def get_des_hanmingdis_wht_permute(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end_min
    
    def get_des_hanmingdis_wht_permute_256(self, des_a, des_b):
        hadama_trans = torch.tensor(hadamard(des_a.shape[1]), device=des_a.device).float()
        wht_desc_a, wht_desc_b = des_a @ hadama_trans, des_b @ hadama_trans
        half_dim =  wht_desc_a.shape[1] // 2 
        desc_binary_a = torch.where(wht_desc_a > 0, torch.ones_like(wht_desc_a), torch.zeros_like(wht_desc_a))  # Nxdim
        desc_binary_b = torch.where(wht_desc_b > 0, torch.ones_like(wht_desc_b), torch.zeros_like(wht_desc_b))  # Mxdim
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.net_index] @ desc_binary_b[:, self.net_index].t() - (1 - desc_binary_a[:, self.net_index]) @ (1 - desc_binary_b[:, self.net_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.net_index==False] @ desc_binary_b[:, self.net_index==False].t() - (1 - desc_binary_a[:, self.net_index==False]) @ (1 - desc_binary_b[:, self.net_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end

    def get_des_hanmingdis_wht_permute_rec_sift_256(self, des_a, des_b):
        half_dim =  des_a.shape[1] // 2 
        desc_binary_a = des_a  # Nxdim
        desc_binary_b = des_b
        # index = (((torch.linspace(16,1,16)**4).unsqueeze(0) @ hadamard(16)) == ((torch.linspace(1,16,16)**4).unsqueeze(0) @ hadamard(16))).long().squeeze(0).unsqueeze(1).repeat(1,8).view(-1).bool()
        hanming_dist_part_begin = half_dim - desc_binary_a[:, self.sift_index] @ desc_binary_b[:, self.sift_index].t() - (1 - desc_binary_a[:, self.sift_index]) @ (1 - desc_binary_b[:, self.sift_index].t())       
        hanming_dist_part_end = half_dim - desc_binary_a[:, self.sift_index==False] @ desc_binary_b[:, self.sift_index==False].t() - (1 - desc_binary_a[:, self.sift_index==False]) @ (1 - desc_binary_b[:, self.sift_index==False].t())    
        # hanming_dist_part_end_min = torch.where(hanming_dist_part_end >= half_dim // 2, half_dim - hanming_dist_part_end, hanming_dist_part_end)        # 0-32
        # hanming_dist = torch.ones((wht_desc_a.shape[0], wht_desc_b.shape[0]), device=self.device) * wht_desc_a.shape[1] - desc_binary_a @ desc_binary_b.t() - (1 - desc_binary_a) @ (1 - desc_binary_b.t())
        return hanming_dist_part_begin, hanming_dist_part_end


    def get_point_pair_ori(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[ch, a2b_min_id[ch]] = dis[ch, a2b_min_id[ch]]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] == 1
            ch = ch * (ch_norepeat_A == 1)
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def get_match_nomatch_dis(self, pA_norm, pB_norm, desA, desB, repeat_mask):
        pred_descA = sample_descriptor(desA, pA_norm.unsqueeze(0), bilinear_interp=True)[0]
        pred_descB = sample_descriptor(desB, pB_norm.unsqueeze(0), bilinear_interp=True)[0]
        hanmingdist_AtoB = self.get_des_hanmingdis(pred_descA, pred_descB)
        # hanmingdist_AtoB = self.get_des_hanmingdis_wht(pred_descA, pred_descB)
        match_indicesA, match_indicesB, _, match_mask = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=pred_descA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler(self, desA, desB, repeat_mask):
        hanmingdist_AtoB = self.get_des_hanmingdis(desA, desB)
        match_indicesA, match_indicesB, _, _ = self.get_match_point_pair(hanmingdist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hanmingdist_AtoB[repeat_mask == 1], hanmingdist_AtoB[repeat_mask == 0]

    def thresholding_desc(self, descs):
        norm = torch.sqrt(torch.sum(descs * descs, dim = 1)) * 0.2
        norm = norm.int().unsqueeze(-1).expand_as(descs).float()
        descs = torch.where(descs < norm, torch.sqrt(descs), torch.sqrt(norm)).int().float()
        return descs
    
    def quantize_desc_hadama(self, desc):
        hadama_trans = torch.tensor(hadamard(desc.shape[-1]), device=desc.device).float()
        wht_desc = torch.einsum('nd,de->ne', desc, hadama_trans).int()
        desc_binary = torch.where(wht_desc > 0, torch.ones_like(wht_desc), torch.zeros_like(wht_desc)).int()
        return desc_binary

    def get_match_nomatch_dis_nosampler_hadama(self, desA, desB, repeat_mask):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hadamadist_AtoB = self.get_des_hanmingdis_wht(desA_thr, desB_thr)
        match_indicesA, match_indicesB, _ = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)      
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute(self, desA, desB, repeat_mask, th1=30, th2=55):
        desA_thr, desB_thr = self.thresholding_desc(torch.round(desA * self.slope) + 5000), self.thresholding_desc(torch.round(desB * self.slope) + 5000)
        hanming_distAtoB_part_begin, hanming_distAtoB_part_end_min = self.get_des_hanmingdis_wht_permute(desA_thr, desB_thr)
        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        # if min_dis.shape[0] >= 30:
        #     # Top30
        #     top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        # else:
        #     top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA, match_indicesB, hadamadist_AtoB[repeat_mask == 1], hadamadist_AtoB[repeat_mask == 0]

    def get_match_nomatch_dis_nosampler_hadama_permute_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = self.thresholding_desc(torch.round(desA[:, 128:] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, 128:] * self.slope) + 5000)
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr

    def get_match_nomatch_dis_nosampler_hadama_permute_rec_sift_256(self, desA, desB, repeat_mask, th1=60, th2=110):
        '''
            desA, desB: 0和45度的256维描述子
        '''
        desA_0_thr, desB_0_thr = self.thresholding_desc(torch.round(desA[:, :128] * self.slope) + 5000), self.thresholding_desc(torch.round(desB[:, :128] * self.slope) + 5000)
        desA_45_thr, desB_45_thr = desA[:, 128:].float(), desB[:, 128:].float()
        hanming_distAtoB_part_begin_0, hanming_distAtoB_part_end_0 = self.get_des_hanmingdis_wht_permute_256(desA_0_thr, desB_0_thr)
        hanming_distAtoB_part_begin_45, hanming_distAtoB_part_end_45 = self.get_des_hanmingdis_wht_permute_rec_sift_256(desA_45_thr, desB_45_thr)
        hanming_distAtoB_part_begin = hanming_distAtoB_part_begin_0 + hanming_distAtoB_part_begin_45
        hanming_distAtoB_part_end = hanming_distAtoB_part_end_0 + hanming_distAtoB_part_end_45
        hanming_distAtoB_part_end_min = torch.where(hanming_distAtoB_part_end >= desA.shape[-1] // 4, desA.shape[-1] // 2 - hanming_distAtoB_part_end, hanming_distAtoB_part_end)        # 0-32

        hadamadist_AtoB = hanming_distAtoB_part_begin + hanming_distAtoB_part_end_min
        hadamadist_AtoB_nothr = copy.deepcopy(hadamadist_AtoB)
        hadamadist_AtoB_m_nothr,  hadamadist_AtoB_nm_nothr = hadamadist_AtoB_nothr[repeat_mask == 1], hadamadist_AtoB_nothr[repeat_mask == 0]

        mask1 = hanming_distAtoB_part_begin < th1
        mask2 = hadamadist_AtoB < th2
        hadamadist_AtoB[mask1 == False] = desA.shape[-1] + 1    # 不满足阈值的对距离赋值为129
        hadamadist_AtoB[mask2 == False] = desA.shape[-1] + 1
        # 单向最近邻
        match_indicesA, match_indicesB, min_dis = self.get_point_pair(hadamadist_AtoB, dis_thre=desA.shape[-1] + 1)
        if min_dis.shape[0] >= 30:
            # Top30
            top30_mask = torch.topk(min_dis, 30, largest=False).indices  
        else:
            top30_mask = torch.ones_like(match_indicesA).to(self.device).bool()    
        return match_indicesA[top30_mask], match_indicesB[top30_mask], hadamadist_AtoB_m_nothr, hadamadist_AtoB_nm_nothr, hadamadist_AtoB_nothr


    def get_dis(self, p_a, p_b):
        eps = 1e-12
        x = torch.unsqueeze(p_a[:, 0], 1) - torch.unsqueeze(p_b[:, 0], 0)  # N 2 -> NA 1 - 1 NB -> NA NB
        y = torch.unsqueeze(p_a[:, 1], 1) - torch.unsqueeze(p_b[:, 1], 0)
        dis = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2) + eps)
        return dis

    def get_ori_diff(self, ori_a, ori_b, trans_angle):
        # ori_a, ori_b: [-90, 90)，斜切角对trans角有影响
        # 通过trans重新计算A图的主方向更合理
        ori_diff = torch.unsqueeze(ori_a, 1) - torch.unsqueeze(ori_b, 0)  # [-180, 180]
        ori_a_after = ori_a - trans_angle + 180         # [-270, 270]
        ori_a_after[ori_a_after < 0] += 360   # [-270, 0] -> [90, 360]; [0, 270] -> [0, 270]
        cond = torch.logical_and(ori_a_after > 90, ori_a_after < 270)
        ori_diff -= (trans_angle - 180)                 # [-360, 360]
        flag_cal = 1 if trans_angle > 180 else -1
        # if trans_angle < 180 + dev and trans_angle > 180 - dev:
        #     flag_cal = 0
        ori_diff[cond, :] +=  flag_cal * 180
        ori_diff[ori_diff < -90] += 180
        ori_diff[ori_diff > 90] -= 180
        ori_diff = torch.abs(ori_diff)
        return ori_diff

    def get_point_pair(self, dis, dis_thre=-1, test_hanming = True):  # 获得匹配点
        # a2b_min_id = torch.argmin(dis, dim=1)
        # len_p = len(a2b_min_id)
        # ch = dis[list(range(len_p)), a2b_min_id] < dis_thre

        # idx_x = a2b_min_id[ch]
        # dis_pair = dis[ch, a2b_min_id[ch]]
        
        # return dis_pair
        correspond = self.correspond if dis_thre == -1 else dis_thre
            

        a2b_min_id = torch.argmin(dis, dim=1)  # M X 1
        len_p = len(a2b_min_id)
        ch = dis[list(range(len_p)), a2b_min_id] < correspond
        reshape_as = torch.tensor(list(range(len_p)), device=self.device)
        reshape_as_rep = torch.tensor(list(range(dis.shape[1])), device=self.device).repeat(dis.shape[0], 1)
        mask2D = reshape_as_rep == a2b_min_id.unsqueeze(1).repeat(1, dis.shape[1])
        # reshape_bs = b_s

        max_thr = 257 if self.has45 else 129
        if test_hanming:
            # 最近邻次近邻比值卡控 < 0.99
            a2b_values_nnn, _ = torch.topk(dis, dim=1, k=2, largest=False)
            nnn_mask = (a2b_values_nnn[:, 0] / a2b_values_nnn[:, 1]) < 0.99     # 保留mask
            ch = ch * nnn_mask
            nnn_mask2D = torch.zeros_like(dis).to(dis.device)
            nnn_mask2D[ch, :] = 1
            nnn_mask2D = (nnn_mask2D == 1) * mask2D
            # print(torch.sum(nnn_mask2D == 1))

            # 去重（滤除多对一情况， 保留列方向最小，和双向最近邻有区别）
            mid_dis = torch.ones_like(dis).to(dis.device) * max_thr
            mid_dis[nnn_mask2D] = dis[nnn_mask2D]
            a2b_norepeat_id = torch.argmin(mid_dis, dim=0)
            ch_norepeat_B = mid_dis[a2b_norepeat_id, list(range(len(a2b_norepeat_id)))] < max_thr
            ch_norepeat_A = torch.zeros_like(ch).to(dis.device)
            ch_norepeat_A[a2b_norepeat_id[ch_norepeat_B]] = 1
            # print(torch.sum(ch_norepeat_A == 1))
            ch = ch * (ch_norepeat_A == 1)
            # print(torch.sum(ch))
         
        a_s = reshape_as[ch]
        b_s = a2b_min_id[ch]
        d_k = dis[ch, a2b_min_id[ch]]

        return a_s, b_s, d_k

    def cal_max_min_mean(self, dis_all, phase):
        dis_max = torch.max(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_min = torch.min(dis_all, dim=-1).values.detach().cpu().numpy()
        dis_mean = torch.mean(dis_all, dim=-1).detach().cpu().numpy()
        dis_std = torch.std(dis_all, unbiased=False).detach().cpu().numpy()

        print(phase + ' Hanming Distance Distribution:')
        print('Max: ', dis_max)
        print('Min: ', dis_min)
        print('Mean: ', dis_mean)
        print('Std: ', dis_std)
        return dis_max, dis_min, dis_mean, dis_std

    def ransac_new(self, matched_left, matched_right, dist_list_return):
        # ransac
        H = np.zeros((3,3))

        if len(matched_right) > 3:
            # FundamentalMatrixTransform EuclideanTransform ， warpPerspective, ProjectiveTransform
            from other_tools.SLransac import slransac
            model, inliers = slransac((matched_left,matched_right),
                                    AffineTransform, min_samples=3,
                                    residual_threshold=2, max_trials=1000, random_state=2000, weight_pairs=None)
            # model, inliers = ransac((matched_left,matched_right),
            #                         AffineTransform, min_samples=3,
            #                         residual_threshold=2, max_trials=1000, random_state=2000)

            if model is None:
                return None, None
            else:
                H[0,0] = model.params[0,0]
                H[0,1] = model.params[0,1]
                H[0,2] = model.params[0,2]
                H[1,0] = model.params[1,0]
                H[1,1] = model.params[1,1]
                H[1,2] = model.params[1,2]
                H[2,0] = 0
                H[2,1] = 0
                H[2,2] = 1

                return H, inliers
        else:
            return None, None

    def remove_border(self, pts, bordW, bordH, W, H):
        toremoveW = torch.logical_or(pts[:, 0] < bordW, pts[:, 0] >= (W - bordW))
        toremoveH = torch.logical_or(pts[:, 1] < bordH, pts[:, 1] >= (H - bordH))
        toremove = torch.logical_or(toremoveW, toremoveH)
        pts = pts[~toremove, :]
        return pts, ~toremove

    def homography_centorcrop(self, homography, Hdev_top, Wdev_left):
        homography_dev = torch.tensor([[0, 0, Wdev_left], [0, 0, Hdev_top], [0, 0, 0]], dtype=torch.float32)
        scale = torch.tensor([[1, 0, Wdev_left], [0, 1, Hdev_top], [0, 0, 1]], dtype=torch.float32)
        homography = (homography - homography_dev) @ scale
        return homography

    # def remove_border(self, pts, bord, W, H):
    #     toremoveW = torch.logical_or(pts[:, 0] < bord, pts[:, 0] >= (W - bord))
    #     toremoveH = torch.logical_or(pts[:, 1] < bord, pts[:, 1] >= (H - bord))
    #     toremove = torch.logical_or(toremoveW, toremoveH)
    #     pts = pts[~toremove, :]
    #     return pts, ~toremove

    def ptq(self, cal_iter, FPDT):
        print("Calibrating...")
        total_count = len(self.sift_succ_dict)
        # KLdiv need record pre-minmax
        FPDT.detector_net.descriptor_net.model_open_calibrate()
        # for idx in range(total_count):
        #     image1_PIL = Image.open(filename_list1[i]).convert('L')
        #     image1_np = np.array(image1_PIL)
        #     h, w = image1_np.shape
        #     keypoints1 = FPRDL.detector_points(image1_PIL, 0.5, 2)
        #     descriptor1 = FPRDL.hardnet_descriptor(image1_PIL, keypoints1)
            
        #     image2_PIL = Image.open(filename_list2[i]).convert('L')
        #     image2_np = np.array(image2_PIL)
        #     keypoints2 = FPRDL.detector_points(image2_PIL, 0.5, 2)
        #     descriptor2 = FPRDL.hardnet_descriptor(image2_PIL, keypoints2)

        #     matched_left, matched_right, dist_list_return = FPRDL.match(descriptor1, descriptor2, keypoints1, keypoints2, 0.05, H=h, W=w)  
    
        # FPRDL.model_open_hist_mode()      
        for idx in range(total_count):
            # count += 1
            # print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            
            imgA, imgB = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0)
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W

            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Tradition'''
            pts_A, _ = FPDT.run_pts_desc_dkdwithhard(imgA.to(self.device), self.conf_thresh, self.nms)
            pts_B, _ = FPDT.run_pts_desc_dkdwithhard(imgB.to(self.device), self.conf_thresh, self.nms)
            
            pointsA = torch.tensor(pts_A.transpose(1, 0)[:, [0, 1]], device=self.device)        # [w, h]
            pointsB = torch.tensor(pts_B.transpose(1, 0)[:, [0, 1]], device=self.device)

            pointsA_o = copy.deepcopy(pointsA)
            pointsB_o = copy.deepcopy(pointsB)

            '''后续计算均还原到原图136*36上'''
            if not self.isDilation:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o += 4            # 32->36
                pointsA_o[:, 0] += 2
                pointsB_o[:, 0] += 2
                    
            else:
                imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)
                W_o -= 4    # 40->36
                mask_ptsA = (pointsA_o[:, 0] >= 2) * (pointsA_o[:, 0] <= 37)
                pointsA_o = pointsA_o[mask_ptsA, :]
                mask_ptsB = (pointsB_o[:, 0] >= 2) * (pointsB_o[:, 0] <= 37)
                pointsB_o = pointsB_o[mask_ptsB, :]
                pointsA_o[:, 0] -= 2
                pointsB_o[:, 0] -= 2
            # desc_A = FPDT.detector_net.cut_patch_unfold_interpolation(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
            remove_border_r = 2
            if remove_border_r > 0:
                pointsA_o, _ = self.remove_border(pointsA_o, remove_border_r, remove_border_r, W_o, H_o)
                pointsB_o, _ = self.remove_border(pointsB_o, remove_border_r, remove_border_r, W_o, H_o)  # [136, 36]的中间（132，32）块
            
            '''截断150'''
            if self.top_k_net:
                if pointsA_o.shape[0] > self.top_k_net:
                    pointsA_o = pointsA_o[:self.top_k_net, :]
                if pointsB_o.shape[0] > self.top_k_net:
                    pointsB_o = pointsB_o[:self.top_k_net, :]

            if self.isrot and not self.isrot91:
                pointsA_o[:, 1] += 12    # [136,36]->[160,36]
                pointsB_o[:, 1] += 12
                H_o += 24    # 136->160 

            if self.isdense:
                # desc_A = FPDT.detector_net.cut_patch_from_featuremap(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                # desc_B = FPDT.detector_net.cut_patch_from_featuremap(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)

                desc_A = FPDT.detector_net.cut_patch_from_featuremap_pad(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_from_featuremap_pad(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            else:
                desc_A = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgA.to(self.device), pointsA_o.unsqueeze(0), False, False)         # 136x36
                if idx == cal_iter - 1:
                    print('Last calibration:')
                    FPDT.detector_net.descriptor_net.model_open_last_calibrate()
                desc_B = FPDT.detector_net.cut_patch_unfold_patch_map_interpolation_aligned_batch(imgB.to(self.device), pointsB_o.unsqueeze(0), False, False)
            if idx == cal_iter - 1:
                break
        print('Calibration finished!')
        FPDT.detector_net.descriptor_net.model_close_calibrate()
        FPDT.detector_net.descriptor_net.model_quant()


    def test_process(self, FPDT):

        # PTQ
        PTQ = False
        if PTQ:
            cal_iter = 1000
            # cal_csv = "/hdd/file-input/linwc/match/datasets/LoFTR/20210713_7015s9常低温-raw-tradition/7015_test.csv" # 取测试集前cal_iter对校正
            # quant_param_path = '/hdd/file-input/linwc/Descriptor/code/Test_sys/logs/param/quant_param.h'
            self.ptq(cal_iter, FPDT)
            #FPDT.detector_net.detector_net.get_parameters(quant_param_path, begin_flag=True, end_flag=False)
            #FPDT.detector_net.desctiptor_net.get_parameters(quant_param_path, begin_flag=False, end_flag=False)

            # FPDT.detector_net.matcher_net.get_parameters(quant_param_path, begin_flag=True, end_flag=True)

        count = 0

        # net_match_hanming_dis_all = []
        # net_nomatch_hanming_dis_all = []
        # sift_match_hanming_dis_all = []
        # sift_nomatch_hanming_dis_all = []
        # siftnet_match_hanming_dis_all = []
        # siftnet_nomatch_hanming_dis_all = []
        total_count = len(self.sift_succ_dict) # 200 
        repeat_dis = 2
        net_require_image = False
        sift_require_image = True
        remove_border_r = 2
        content_hit = []
        for idx in range(total_count):
            count += 1
            print("progress:{0}%".format(round((count + 1) * 100 / total_count)), end="\r")
            # print("it: ", count)
            if self.isrot:
                if self.isrot91:
                    sample = self.Extract_Succ_Csv_rot91_siftq_t2s_base(idx, self.sift_succ_dict)
                else:
                    sample = self.Extract_Succ_Csv_rot(idx, self.sift_succ_dict)
            else:
                sample = self.Extract_Csv(idx, self.sift_succ_dict)
            imgA, imgB, mat_H = sample["imgA"].unsqueeze(0), sample["imgB"].unsqueeze(0), sample['homo']
            imgA_base, imgB_base = sample["imgA_base"].unsqueeze(0), sample["imgB_base"].unsqueeze(0)
            imgA_partial_mask, imgB_partial_mask = sample['imgA_mask'], sample['imgB_mask']
            H, W = imgA.shape[2], imgB.shape[3]
            H_o, W_o = H, W     # 122 x 40
            imgA_warped_mask = sample['imgA_warped_mask']
            imgA_m, imgB_m = copy.deepcopy(imgA), copy.deepcopy(imgB)
            nameA = sample['name_vf'][:-4]
            nameB = sample['name_en'][:-4]
            nameAB = str(nameA) + '_' + str(nameB)
            # logging.info(f"nameA: {nameA} <-> nameB: {nameB}")

            '''NET'''
            # # pass through network
            # pts_A, desc_A = FPDT.run_pts_desc(imgA.to(self.device), self.conf_thresh, self.nms)
            # pts_B, desc_B = FPDT.run_pts_desc(imgB.to(self.device), self.conf_thresh, self.nms)

            '''Trans angle'''
            trans_obj = AffineTransform(matrix=mat_H.cpu().numpy())         # 122 x 36
            trans_angle = trans_obj.rotation * 180 / math.pi + 180  # [0, 360]

            # sift点
            sift_ptsA, sift_ptsB = sample['pts_vf'].squeeze(), sample['pts_en'].squeeze()   # 122 x 36中心的118 x 32
            sift_ptsA = torch.tensor(sift_ptsA, device=self.device).float()
            sift_ptsB = torch.tensor(sift_ptsB, device=self.device).float()

            if self.isDilation:
                W_o -= 4        # 36

            if self.isrot and not self.isrot91:
                if self.remove_border_w > 0 or self.remove_border_h > 0:
                    sift_ptsA, borderA = self.remove_border(sift_ptsA, self.remove_border_w, self.remove_border_h, W_o, H_o)        #中心块132x32
                    sift_ptsB, borderB = self.remove_border(sift_ptsB, self.remove_border_w, self.remove_border_h, W_o, H_o)
            else:
                sift_ptsA, borderA = self.remove_border(sift_ptsA, remove_border_r, remove_border_r, W_o, H_o)
                sift_ptsB, borderB = self.remove_border(sift_ptsB, remove_border_r, remove_border_r, W_o, H_o)

            sift_oriA = sample['ori_vf'][borderA]
            sift_oriB = sample['ori_en'][borderB]

            '''截断150'''
            if self.top_k:
                if sift_ptsA.shape[0] > self.top_k:
                    sift_ptsA = sift_ptsA[:self.top_k, :]
                    sift_oriA = sift_oriA[:self.top_k] 
                if sift_ptsB.shape[0] > self.top_k:
                    sift_ptsB = sift_ptsB[:self.top_k, :]
                    sift_oriB = sift_oriB[:self.top_k]
            
            # warped_sift_pts_A = warp_points(sift_ptsA, mat_H.to(self.device).squeeze(), device=self.device)  # 利用变换矩阵变换坐标点
            # sift_point_disAB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2]) # N x M

            # warped_sift_pts_A, sift_mask_points = filter_points(warped_sift_pts_A, torch.tensor([W_o, H_o], device=self.device), return_mask=True)

            # sift_ori_diff = self.get_ori_diff(sift_oriA, sift_oriB, trans_angle)

            # sift_key_disAtoB = self.get_dis(warped_sift_pts_A[:, :2], sift_ptsB[:, :2])
            # sift_pos_repeatA_mask, sift_pos_repeatB_mask, _ = self.get_point_pair(sift_key_disAtoB, dis_thre=repeat_dis, test_hanming=False)  # p -> k
            # # sift_match_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            # if sift_pos_repeatA_mask.shape[0] > 0:
            #     sift_A_all_indexes = torch.tensor(list(range(sift_ptsA.shape[0])), device=self.device)
            #     sift_pos_repeatA_indices = sift_A_all_indexes[sift_mask_points][sift_pos_repeatA_mask]
            #     # sift_match_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1
            #     sift_nn_mask = torch.zeros((sift_ptsA.shape[0], sift_ptsB.shape[0]), device=self.device) 
            #     sift_nn_mask[sift_pos_repeatA_indices, sift_pos_repeatB_mask] = 1

            # # sift描述
            # sift_descA = torch.cat((sample['desc_front_vf'], sample['desc_back_vf']), dim=-1).to(self.device)   # N x 128
            # sift_descB = torch.cat((sample['desc_front_en'], sample['desc_back_en']), dim=-1).to(self.device)
            # if self.isrot and not self.isrot91: 
            #     if self.remove_border_w > 0 or self.remove_border_h > 0: 
            #         sift_descA = sift_descA[borderA, :]
            #         sift_descB = sift_descB[borderB, :]
            # else:
            #     if remove_border_r > 0:
            #         sift_descA = sift_descA[borderA, :]
            #         sift_descB = sift_descB[borderB, :]
            # if self.top_k:
            #     if sift_descA.shape[0] > self.top_k:
            #         sift_descA = sift_descA[:self.top_k, :]
            #     if sift_descB.shape[0] > self.top_k:
            #         sift_descB = sift_descB[:self.top_k, :]
            
            imgA, imgB = sample['img_o_A'].unsqueeze(0), sample['img_o_B'].unsqueeze(0)     # 128 x 52
            
            pai_coef = 3.14159265
            ## output images for visualization labels
            if sift_require_image:
                pred, img_pair, img_base_pair = {}, {}, {}
                pred.update({
                    "pts": sift_ptsA.detach().cpu().numpy(), 
                    "pts_H": sift_ptsB.detach().cpu().numpy(),
                    "lab": None, #pts_A_lable,
                    "lab_H": None, #pts_B_lable,
                    "pts_TH": None, # warped_pts_A.detach().cpu().numpy(),
                    "pts_repeatA": None, #pos_repeatA.detach().cpu().numpy(),
                    "pts_repeatB": None, #pos_repeatB.detach().cpu().numpy(),
                    "pts_nncandA": None, #pos_nncandA.detach().cpu().numpy(),
                    "pts_nncandB": None, #pos_nncandB.detach().cpu().numpy(),
                    "pts_nnA": None, # pos_nnA.detach().cpu().numpy(),
                    "pts_nnB": None, # pos_nnB.detach().cpu().numpy(),
                    'pts_degree': sift_oriA.squeeze().detach().cpu().numpy() * pai_coef / 180,
                    'pts_H_degree': sift_oriB.squeeze().detach().cpu().numpy() * pai_coef / 180, 
                    'pts_degree_label': sift_oriA.squeeze().detach().cpu().numpy() * pai_coef / 180,
                    'pts_H_degree_label': sift_oriB.squeeze().detach().cpu().numpy() * pai_coef / 180,
                    })
                '''NET'''
                img_2D_A = imgA.numpy().squeeze()[3:-3, 8:-8]       # 122 x 36
                img_2D_B = imgB.numpy().squeeze()[3:-3, 8:-8]
                img_pair.update({'img': img_2D_A, 'img_H': img_2D_B})
                img_base_pair.update({'img': imgA_base.numpy().squeeze(), 'img_H': imgB_base.numpy().squeeze()})
                img_desc_match = draw_match_pair_degree_match(img_pair, pred, color=(255, 0, 0), radius=3, s=3, Htrans=mat_H)
                f = str(self.sift_succ_path).replace('SIFT_Trans/SIFT_transSucc_TN.csv', 'registration/sift/') + (nameAB + "_desc_match.bmp")
                cv2.imwrite(str(f), img_desc_match)

                mat_H_modify = self.homography_centorcrop(mat_H, 2, 2) # 122x36 -> 118x32
                img_base_match = draw_match_pair_degree_match(img_base_pair, pred, color=(255, 0, 0), radius=3, s=3, Htrans=mat_H_modify, is_base=True)
                fb = str(self.sift_succ_path).replace('SIFT_Trans/SIFT_transSucc_TN.csv', 'registration/sift/') + (nameAB + "_base_match.bmp")
                cv2.imwrite(str(fb), img_base_match)

